term: gradient descent
aliases:
- steepest descent
- batch gradient descent
categories:
- Foundations
roles:
- data_science
- engineering
- product
part_of_speech: process
short_def: Iterative optimization algorithm that updates model parameters in the direction
  of the negative gradient to minimize a loss function.
long_def: Gradient descent is the workhorse optimization routine behind many machine
  learning models. The algorithm measures how the loss function changes with respect
  to each parameter, then nudges those parameters in the direction that most quickly
  reduces the loss. Step size is controlled by the learning rate, and the procedure
  repeats until convergence or an early stopping rule triggers. Product teams see
  the effects of gradient descent in training curves and quality improvements, engineers
  focus on stability and runtime characteristics, and data scientists tune batch size,
  momentum, and learning rate schedules to balance accuracy with compute budget. Understanding
  gradient descent is critical when diagnosing training instability, bias amplification,
  or regressions after feature changes because it exposes how the model is navigating
  its optimization landscape.
audiences:
  exec: Gradient descent is the routine that steadily adjusts model knobs until errors
    shrink.
  engineer: Compute parameter updates using grad_theta L(theta); tune learning rate,
    batching, and momentum to ensure stable convergence.
examples:
  do:
  - Monitor training and validation loss together to catch divergence early.
  - Scale learning rates with batch size when moving between GPU configurations.
  dont:
  - Ignore exploding gradients without adding clipping or schedule adjustments.
  - Assume a single learning rate works across every feature or dataset refresh.
governance:
  nist_rmf_tags:
  - validity
  - reliability
  risk_notes: Poorly tuned optimization can encode bias or destabilize production
    models, so training controls and reviews are essential.
relationships:
  related:
  - loss function
  - regularization
  - fine-tuning
citations:
- source: Google ML Glossary
  url: https://developers.google.com/machine-learning/glossary
- source: DeepLearning.AI Resources
  url: https://www.deeplearning.ai/resources/
- source: Wikipedia â€“ Gradient Descent
  url: https://en.wikipedia.org/wiki/Gradient_descent
license: CC BY-SA 4.0
status: approved
last_reviewed: '2025-10-10'
