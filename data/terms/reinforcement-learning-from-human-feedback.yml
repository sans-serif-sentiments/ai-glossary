term: reinforcement learning from human feedback
aliases:
- rlhf
- preference optimization
categories:
- LLM Core
roles:
- engineering
- data_science
- product
- policy
part_of_speech: process
short_def: Training approach that tunes a model using reward signals learned from
  human preference data.
long_def: Reinforcement learning from human feedback (RLHF) trains a reward model
  on human preference comparisons, then uses that reward model to fine-tune a base
  language model with reinforcement learning. The process typically involves collecting
  prompts and multiple responses, asking human labelers which response better follows
  policy, training a reward model on those rankings, and running a reinforcement learning
  algorithm (often PPO) to optimize the model’s behavior. RLHF aligns outputs with
  human expectations when explicit rules are difficult to encode. Product teams define
  the guidelines labelers follow, policy teams ensure safety requirements are reflected,
  and engineers monitor for reward hacking or regressions. RLHF requires continuous
  refreshes as policies evolve; stale preference data can drift from current standards.
audiences:
  exec: Budget for ongoing RLHF cycles so the model keeps pace with policy and product
    shifts.
  engineer: Instrument reward signals, evaluation sets, and safety metrics to catch
    regressions early.
examples:
  do:
  - Collect diverse preference data that captures edge cases and sensitive topics.
  - Audit reward model calibration to prevent exploiting annotation quirks.
  dont:
  - Assume one RLHF pass will keep a model aligned indefinitely.
  - Let annotator instructions diverge from published safety or product policies.
governance:
  nist_rmf_tags:
  - risk_management
  - monitoring
  - transparency
  risk_notes: Poorly curated feedback can encode bias or create reward hacking that
    violates policy.
relationships:
  broader:
  - fine-tuning
  related:
  - reward model
  - alignment
  - robust prompting
citations:
- source: OpenAI – Learning from Human Preferences
  url: https://arxiv.org/abs/1706.03741
- source: 'InstructGPT: Training Language Models to Follow Instructions'
  url: https://arxiv.org/abs/2203.02155
- source: Hugging Face – RLHF Guide
  url: https://huggingface.co/blog/rlhf
license: CC BY-SA 4.0
status: approved
last_reviewed: '2025-10-09'
