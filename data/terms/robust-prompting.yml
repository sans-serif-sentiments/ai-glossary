term: robust prompting
aliases:
- defensive prompting
- resilient prompting
categories:
- LLM Core
roles:
- engineering
- product
- security
part_of_speech: process
short_def: Prompt design techniques that harden models against injections, ambiguity,
  and unsafe outputs.
long_def: Robust prompting combines structured instructions, guard clauses, and response
  checks to make language models more resilient to adversarial or ambiguous inputs.
  Teams use it to constrain topics, refuse unsafe requests, and preserve critical
  context even when users try to override instructions. Techniques include layered
  system messages, explicit refusal criteria, verification prompts, and output formatting
  templates that make policy violations easier to detect. Engineers pair robust prompts
  with automated tests, while product and security stakeholders review language to
  ensure safety requirements and user empathy co-exist. Without defensive prompts,
  even well-governed systems can be steered into disallowed actions by crafted jailbreaks
  or accidental misuse.
audiences:
  exec: Invest in robust prompts to reduce incident volume before adding expensive
    human moderation.
  engineer: Version prompts, test for bypasses, and log refusals so you can tune protections
    proactively.
examples:
  do:
  - Include explicit refusal language and escalation guidance when requests break
    policy.
  - Pair prompts with automated prompt-injection tests in CI.
  dont:
  - Rely on a single system message to enforce all safety requirements.
  - Ship prompt changes without regression testing for jailbreak coverage.
governance:
  nist_rmf_tags:
  - risk_management
  - safety
  - monitoring
  risk_notes: Weak prompts increase exposure to jailbreaks, data leakage, and brand-damaging
    outputs.
relationships:
  broader:
  - prompt engineering
  related:
  - prompt injection
  - guardrail policy
  - self-critique loop
citations:
- source: Prompting Guide – Defensive Prompting
  url: https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/content-filter-prompt-shields
- source: 'arXiv – InjecGuard: Benchmarking and Mitigating Over-defense in Prompt
    Injection Guardrail Models'
  url: https://arxiv.org/abs/2410.22770
license: CC BY-SA 4.0
status: approved
last_reviewed: '2025-10-10'
