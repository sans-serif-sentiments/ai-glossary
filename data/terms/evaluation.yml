term: evaluation
aliases:
- model evaluation
- AI evaluation
categories:
- Operations & Monitoring
- Governance & Risk
roles:
- communications
- engineering
- legal
- policy
- product
- security
part_of_speech: process
short_def: Systematic measurement of model performance, safety, and reliability using
  defined tests.
long_def: 'Evaluation is the disciplined practice of testing AI systems against quantitative
  and qualitative criteria before and after deployment. It extends beyond accuracy
  metrics to encompass robustness, bias detection, factual correctness, latency, and
  safety stress tests such as red teaming or jailbreak attempts. Teams build eval
  suites that blend automated metrics—like BLEU, accuracy@k, or toxicity scores—with
  human review checklists tailored to critical user journeys. Continuous evaluation
  supports regression detection when prompts, datasets, or infrastructure change.
  Governance frameworks treat evaluations as audit artifacts: they document assumptions,
  thresholds, and sign-offs required before promoting models to production. Mature
  programs integrate evaluation pipelines into CI/CD, enabling reproducibility and
  traceability. Without rigorous evaluation, organizations cannot credibly claim their
  models meet compliance obligations or user expectations.'
audiences:
  exec: Evaluation is our quality gate—it proves the AI delivers safe, reliable outcomes
    before we launch.
  engineer: Automated and human-in-the-loop test harnesses measuring task metrics,
    robustness, bias, and safety across model releases.
examples:
  do:
  - Run targeted red-team scenarios alongside quantitative metrics before shipping
    new prompts or fine-tuned models.
  dont:
  - Rely on a single aggregate score without examining subgroup performance or qualitative
    feedback.
governance:
  nist_rmf_tags:
  - validity
  - accountability
  risk_notes: Skipping or weakening evaluations increases the likelihood of undetected
    harmful behaviors in production.
relationships:
  broader:
  - model governance
  narrower:
  - safety evaluation
  - capability evaluation
  related:
  - guardrails
  - alignment
  - red teaming
citations:
- source: NIST AI RMF Glossary
  url: https://www.nist.gov/itl/ai-risk-management-framework
- source: Google ML Glossary
  url: https://developers.google.com/machine-learning/glossary
- source: Stanford HAI Brief Definitions
  url: https://hai.stanford.edu/news/brief-definitions
license: CC BY-SA 4.0
status: approved
last_reviewed: '2025-10-09'
