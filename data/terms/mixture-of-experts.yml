term: mixture of experts
aliases:
- moe
- expert gating
categories:
- Optimization & Efficiency
roles:
- engineering
- data_science
- product
part_of_speech: concept
short_def: Neural architecture that routes tokens to specialized submodels to scale
  capacity efficiently.
long_def: Mixture-of-experts (MoE) models split a network into many expert submodels
  and use a gating function to decide which experts process each token. This allows
  models to increase parameter counts without running every computation for every
  input, improving efficiency and enabling specialization (e.g., code vs. dialogue
  experts). Engineers must manage load balancing, routing stability, and inference
  infrastructure that can handle sparse activation. Product teams care about how specialization
  affects consistency across use cases, and governance teams assess whether routing
  introduces bias or explainability challenges. Operationally, MoE systems demand
  robust logging to trace which experts contributed to an output for debugging and
  accountability.
audiences:
  exec: MoE architectures stretch compute budgets while boosting quality, but require
    investment in routing controls.
  engineer: Monitor expert utilization, balance routing, and record which experts
    fired for auditability.
examples:
  do:
  - Set guardrails for experts that handle sensitive domains like legal or medical
    content.
  - Instrument per-expert performance metrics to catch degradation early.
  dont:
  - Assume routing is fair without analyzing demographic or domain skew.
  - Deploy MoE models without redundancy plans for underperforming experts.
governance:
  nist_rmf_tags:
  - monitoring
  - risk_management
  - transparency
  risk_notes: Opaque routing can hide failures or bias; sparse activation complicates
    reproducibility.
relationships:
  broader:
  - fine-tuning
  related:
  - knowledge distillation
  - model interpretability
  - evaluation
citations:
- source: Google Research – Switch Transformers
  url: https://arxiv.org/abs/2101.03961
- source: Google Brain – Outrageously Large Neural Networks
  url: https://arxiv.org/abs/1701.06538
license: CC BY-SA 4.0
status: approved
last_reviewed: '2025-10-09'
