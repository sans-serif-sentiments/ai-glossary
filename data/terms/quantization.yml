term: quantization
aliases:
- model quantization
- weight quantization
categories:
- Optimization & Efficiency
roles:
- data_science
- engineering
part_of_speech: process
short_def: Technique that compresses model weights into lower-precision formats to
  shrink size and speed inference.
long_def: 'Quantization converts neural network parameters and activations from high-precision
  floating point representations (such as FP32) into lower bit-width formats (such
  as INT8 or INT4) to reduce memory footprint and accelerate inference. By mapping
  continuous values into a discrete set, quantization enables models to run on cost-sensitive
  hardware, deliver faster responses, and consume less energy, which is critical when
  deploying large language models at scale or on edge devices. Engineers choose between
  post-training quantization, which calibrates a frozen model on representative data,
  and quantization-aware training, which simulates low-precision behavior during fine-tuning
  to preserve accuracy. Careful evaluation is required to understand the trade-offs:
  aggressive quantization can introduce numerical instability, harm latency determinism,
  or amplify bias if calibration data under-represents certain groups. Successful
  programs pair quantization with monitoring, backstops such as higher-precision fallbacks,
  and documentation that makes these trade-offs explicit to stakeholders.'
audiences:
  exec: 'A cost-control lever: shrink model footprints so you can serve more traffic
    on existing hardware.'
  engineer: Apply per-tensor or per-channel scaling, choose symmetric/asymmetric schemes,
    and validate perplexity and latency post-quantization.
examples:
  do:
  - Benchmark accuracy and latency before and after quantization to document the trade-offs.
  - Use representative calibration datasets that include edge cases and demographic
    variation.
  dont:
  - Quantize safety-critical models without fallback paths or runtime monitoring.
  - Assume INT4 settings will work across architectures without profiling.
governance:
  nist_rmf_tags:
  - efficiency
  - robustness
  - documentation
  risk_notes: Quantization can reduce accuracy or shift error distribution; record
    evaluations and obtain stakeholder sign-off.
relationships:
  broader:
  - model optimization
  narrower:
  - post-training quantization
  - quantization-aware training
  related:
  - compression
  - distillation
  - hardware acceleration
citations:
- source: Google ML Glossary
  url: https://developers.google.com/machine-learning/glossary
- source: Hugging Face Glossary
  url: https://huggingface.co/docs/transformers/en/glossary
- source: Stanford HAI Brief Definitions
  url: https://hai.stanford.edu/news/brief-definitions
license: CC BY-SA 4.0
status: approved
last_reviewed: '2025-10-09'
