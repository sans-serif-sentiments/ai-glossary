term: guardrails
aliases:
- safety guardrails
- policy guardrails
categories:
- Governance & Risk
roles:
- communications
- legal
- policy
- product
part_of_speech: noun
short_def: Controls that constrain model behavior to comply with safety, legal, or
  brand requirements.
long_def: Guardrails combine policy, technical, and operational measures designed
  to keep AI systems within acceptable behavior. They can include pre- and post-processing
  filters, policy-informed prompts, classifier ensembles, or moderation APIs that
  block disallowed content before it reaches end users. Effective guardrail programs
  coordinate with legal and risk teams to encode organizational standards and regulatory
  obligations. Engineers integrate guardrails into the request pipeline, monitor their
  performance, and log interventions for audit trails. Product managers review guardrail
  coverage to understand trade-offs between user experience and safety friction. Governance
  stakeholders treat guardrails as living controls that require change management,
  testing, and documentation to demonstrate due diligence. When guardrails fail or
  drift, the resulting incidents can expose organizations to legal liability, reputational
  damage, or regulatory penalties.
audiences:
  exec: Guardrails are the checks that keep the AI from saying or doing things that
    would put the company at risk.
  engineer: Policy-aligned filters, prompts, and classifiers embedded in the inference
    stack to block or reshape unsafe outputs.
examples:
  do:
  - Test guardrail coverage with red-team prompts whenever the base model or system
    prompt changes.
  dont:
  - Rely on a single moderation classifier without monitoring precision and recall
    across scenarios.
governance:
  nist_rmf_tags:
  - accountability
  - privacy
  - robustness
  risk_notes: Outdated guardrails can miss harmful outputs or inadvertently censor
    legitimate content, creating compliance gaps.
relationships:
  broader:
  - responsible AI
  narrower:
  - output filtering
  - safety prompt
  related:
  - system prompt
  - temperature
  - red teaming
citations:
- source: NIST AI RMF Glossary
  url: https://www.nist.gov/itl/ai-risk-management-framework
- source: Salesforce â€“ Building AI Guardrails
  url: https://www.ibm.com/think/topics/ai-guardrails
- source: Stanford HAI Brief Definitions
  url: https://hai.stanford.edu/news/brief-definitions
license: CC BY-SA 4.0
status: approved
last_reviewed: '2025-10-09'
