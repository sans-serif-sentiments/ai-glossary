term: token
aliases:
- subword token
- tokenized unit
categories:
- LLM Core
roles:
- data_science
- engineering
- product
part_of_speech: noun
short_def: Smallest unit of text a model processes after tokenization, such as a word
  fragment or character.
long_def: In language models a token is the discrete unit produced by a tokenizer
  before feeding data into the network. Depending on the tokenizer, a token might
  represent a whole word, a meaningful subword chunk, or even individual characters
  and punctuation. Models operate on token sequences rather than raw text, which makes
  token boundaries central to context window sizing, cost estimation, and latency
  planning. Tokens also drive how prompts and completions are billed in commercial
  APIs, where limits are expressed in tokens instead of characters. Engineers track
  token counts to avoid truncating prompts or spilling past context limits, while
  product teams translate token budgets into supported use cases and pricing. Understanding
  tokenization quirks helps explain why uncommon spellings, multilingual inputs, or
  code snippets can explode in length compared with natural language, affecting both
  accuracy and economics. Governance stakeholders rely on the deterministic nature
  of tokenization when auditing prompts and ensuring reproducibility across deployments.
audiences:
  exec: Think of tokens as the LEGO bricks that determine how long prompts can be
    and what they cost.
  engineer: Tokenizer output units that set sequence length, influence embedding lookup,
    and bound context windows for inference/training.
examples:
  do:
  - Estimate prompt costs by counting tokens instead of characters before rolling
    out a pricing plan.
  dont:
  - Assume character length equals token length when enforcing guardrails or cost
    controls.
governance:
  nist_rmf_tags:
  - transparency
  - validity
  risk_notes: Incorrect token accounting can break context constraints, leading to
    truncated outputs and compliance failures.
relationships:
  broader:
  - tokenization
  related:
  - context window
  - embedding
  - prompt engineering
citations:
- source: Wikipedia – Tokenization
  url: https://en.wikipedia.org/wiki/Tokenization_(lexical_analysis)
- source: OpenAI API Reference
  url: https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/prompt-engineering
- source: Hugging Face – Tokenizers
  url: https://huggingface.co/docs/tokenizers/en/quicktour
license: CC BY-SA 4.0
status: approved
last_reviewed: '2025-10-10'
