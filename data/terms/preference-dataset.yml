term: preference dataset
aliases:
- preference data
- human feedback dataset
categories:
- LLM Core
roles:
- data_science
- policy
- product
- engineering
part_of_speech: noun_phrase
short_def: Labeled comparisons of model outputs that capture which responses humans
  prefer.
long_def: A preference dataset contains prompts, model outputs, and human annotations
  indicating which output better satisfies instructions or policies. Teams use it
  to train reward models or perform direct preference optimization. Gathering preference
  data requires clear labeling rubrics, diverse annotators, and safeguards for sensitive
  content. Policy teams define decision criteria, product teams ensure tone and UX
  requirements are reflected, and engineers manage secure tooling for annotation.
  Poorly managed preference data can leak personal information or encode unintended
  bias that later influences RLHF or reranking systems.
audiences:
  exec: Protect preference data like any user research asset—it shapes model behavior
    and compliance.
  engineer: Track lineage from annotation tools to training pipelines and enforce
    access controls.
examples:
  do:
  - Capture rationale alongside preferences so reviewers understand annotations.
  - Audit samples regularly for bias or policy drift.
  dont:
  - Mix production user data into preference datasets without consent.
  - Allow annotators to work without updated safety guidelines.
governance:
  nist_rmf_tags:
  - accountability
  - risk_management
  - privacy
  risk_notes: Lax controls can leak sensitive prompts or entrench biased judgments
    into downstream models.
relationships:
  broader:
  - reinforcement learning from human feedback
  related:
  - reward model
  - instruction tuning
  - risk register
citations:
- source: OpenAI – InstructGPT
  url: https://arxiv.org/abs/2203.02155
- source: Microsoft – Responsible AI Principles
  url: https://www.microsoft.com/en-us/ai/principles-and-approach
license: CC BY-SA 4.0
status: approved
last_reviewed: '2025-10-09'
