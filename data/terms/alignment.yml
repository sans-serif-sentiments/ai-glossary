term: alignment
aliases:
- AI alignment
- value alignment
categories:
- Governance & Risk
roles:
- communications
- legal
- policy
- product
part_of_speech: concept
short_def: Making sure AI systems optimize for human values, policies, and intended
  outcomes.
long_def: Alignment is the multidisciplinary effort to design AI systems whose goals,
  behaviors, and outputs remain consistent with human intent and societal norms. It
  spans technical research—such as reward modeling, constitutional AI, interpretability,
  and adversarial training—and organizational governance, including policy frameworks,
  oversight committees, and escalation paths. Alignment work acknowledges that models
  learn from imperfect data and may pursue proxy objectives that conflict with human
  priorities. Product leaders use alignment roadmaps to decide which features require
  human-in-the-loop review, while engineers translate alignment goals into metrics,
  eval harnesses, and guardrails. Regulators and standards bodies, including NIST
  and ISO, emphasize alignment as part of trustworthy AI, requiring documentation
  of assumptions, residual risks, and impact mitigation strategies. Sustainable alignment
  programs treat it as an ongoing lifecycle activity rather than a one-time tuning
  exercise.
audiences:
  exec: Alignment is how we make sure the AI keeps serving our mission and values
    as it evolves.
  engineer: Research and governance toolkit ensuring loss functions, feedback loops,
    and guardrails drive behavior toward intended objectives.
examples:
  do:
  - Document alignment hypotheses and track eval metrics tied to specific risk scenarios.
  dont:
  - Assume alignment is solved after one fine-tuning pass without continuous monitoring.
governance:
  nist_rmf_tags:
  - accountability
  - transparency
  - validity
  risk_notes: Weak alignment programs allow models to pursue proxy goals that conflict
    with legal or ethical obligations.
relationships:
  broader:
  - responsible AI
  narrower:
  - constitutional AI
  - reinforcement learning from human feedback
  related:
  - guardrails
  - evaluation
  - red teaming
citations:
- source: Wikipedia – AI Alignment
  url: https://en.wikipedia.org/wiki/AI_alignment
- source: OpenAI – Learning from Human Preferences
  url: https://arxiv.org/abs/1706.03741
- source: Microsoft – Responsible AI Principles
  url: https://www.microsoft.com/en-us/ai/principles-and-approach
license: CC BY-SA 4.0
status: approved
last_reviewed: '2025-10-09'
