term: hallucination
aliases:
- AI hallucination
- confabulation
categories:
- LLM Core
- Governance & Risk
roles:
- communications
- data_science
- engineering
- legal
- policy
- product
part_of_speech: noun
short_def: When an AI model presents fabricated or unsupported information as fact.
long_def: Hallucination describes the tendency of generative models to deliver content
  that sounds plausible but is either factually incorrect, logically inconsistent,
  or entirely invented. The phenomenon stems from the probabilistic way large language
  models predict the next token based on training data patterns rather than grounded
  knowledge of the world. It can occur when prompts lack sufficient context, when
  the model has not seen relevant examples during training, or when decoding strategies
  over-index on fluency instead of accuracy. Product teams experience hallucination
  as broken user trust, while engineers may notice it during evaluation as high lexical
  overlap paired with low factual precision. Mitigations range from retrieval augmentation
  and prompt constraints to post-generation fact checking, human review, and model
  fine-tuning on verified corpora. Organizations must treat hallucination as both
  a quality and a risk management issue, particularly in regulated or safety-critical
  workflows.
audiences:
  exec: Signals that the model is making things up, which erodes user trust and can
    trigger compliance issues.
  engineer: Indicates the model sampled a high-probability sequence lacking factual
    grounding; investigate context, decoding, and eval signals.
examples:
  do:
  - Log hallucination incidents and route high-severity cases to human review for
    remediation.
  - Use retrieval augmentation or tool grounding to supply verifiable context before
    generation.
  dont:
  - Deploy long-form responses without monitoring factual accuracy or adding disclaimers.
  - Assume higher model size alone will eliminate hallucination without evaluation
    improvements.
governance:
  nist_rmf_tags:
  - accuracy
  - transparency
  - validity
  risk_notes: Unaddressed hallucinations can produce misleading outputs that violate
    accuracy commitments and create legal exposure.
relationships:
  broader:
  - generative AI
  narrower:
  - factual hallucination
  - formal hallucination
  related:
  - retrieval-augmented generation
  - guardrails
  - evaluation
citations:
- source: Hugging Face Glossary
  url: https://huggingface.co/docs/transformers/en/glossary
- source: NIST AI RMF Glossary
  url: https://www.nist.gov/itl/ai-risk-management-framework
- source: Google ML Glossary
  url: https://developers.google.com/machine-learning/glossary
license: CC BY-SA 4.0
status: approved
last_reviewed: '2025-10-09'
