{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI Glossary","text":"Shared language for AI builders and stewards <p>       Every term in the glossary is citation-backed, audience-aware, and stored as       structured YAML so it can power docs, APIs, and review workflows. Browse by       category, role, or metric to keep product, engineering, and governance teams in sync.     </p> Shailesh Rawat @sans_serif_sentiments \ud83d\udd0d Explore the interactive search \ud83d\udc65 Role starter packs \ud83e\udded Category explorer"},{"location":"#whats-new-sep-28-2025","title":"What's new (Sep 28 2025)","text":"<ul> <li>Lifecycle snapshot: Search now surfaces status counts and the latest review date so teams can see glossary health at a glance.</li> <li>100-term milestone: Added new entries spanning assurance cases, RLHF workflows, and safety controls to push the glossary past one hundred vetted terms.</li> <li>Sort for relevance: Reorder results by freshness or category to share curated views with stakeholders in seconds.</li> </ul> <p>     Submit feedback or new term ideas via the term request intake.   </p>"},{"location":"#project-focus","title":"Project focus","text":"<ul> <li>Shared language: harmonizes terminology across technical, product, and policy stakeholders.</li> <li>Structured delivery: each entry lives in YAML with examples, aliases, and governance context for downstream reuse.</li> <li>Traceable sources: every definition includes citations, NIST AI RMF tags, and lifecycle status for auditability.</li> </ul>"},{"location":"#current-features","title":"Current features","text":"<ul> <li>Seed coverage for foundational concepts spanning large language models, retrieval, MLOps, and governance.</li> <li>Automated validation that enforces schema rules, definition length limits, and audience-specific explanations before publication.</li> <li>Generated JSON datasets (<code>build/glossary.json</code>, <code>build/search-index.json</code>) and Markdown documentation (<code>site/docs/terms/</code>) derived from the same YAML source.</li> <li>Categorized navigation so visitors can browse by LLM internals, retrieval, optimization, operations, or governance topics.</li> <li>Role starter packs and a guided search experience so product, engineering, policy, legal, security, and communications teams can find relevant terms fast.</li> </ul>"},{"location":"#deep-dive-resources","title":"Deep-dive resources","text":"<ul> <li>External source catalog \u2014 official glossaries and standards to cite.</li> <li>Governance dashboard \u2014 metrics, NIST coverage, and intake guidance.</li> <li>Prompt engineering playbook \u2014 practical workflows for shaping model behaviour.</li> </ul>"},{"location":"#popular-categories","title":"Popular categoriesLLM CoreRetrieval &amp; RAGGovernance &amp; RiskOptimization &amp; Efficiency","text":"<p>Attention, decoding, prompting, and the building blocks behind language models.</p> Browse terms \u2192 <p>Grounding models with hybrid search, chunking, reranking, and retrieval pipelines.</p> Browse terms \u2192 <p>Responsible AI practices, documentation, privacy, and safety mitigation.</p> Browse terms \u2192 <p>Quantization, LoRA, distillation, and performance tuning for deployment.</p> Browse terms \u2192"},{"location":"#roadmap-highlights","title":"Roadmap highlights","text":"<ul> <li>Curate the next 150 terms with deeper coverage of safety tooling, security, and applied UX patterns.</li> <li>Launch a public design kit so teams can reuse the Shailesh Rawat (sans_serif_sentiments) brand components across docs and products.</li> <li>Publish live health dashboards that blend glossary freshness, evaluation signals, and intake response times.</li> </ul> <p>Contributors can review the Contribution guide to learn how to add or refine glossary entries.</p> <p>\u2728 Crafted and curated by Shailesh Rawat \u00b7 sans_serif_sentiments</p>"},{"location":"categories/","title":"Category Explorer","text":""},{"location":"categories/#category-explorer","title":"Category Explorer","text":"<p>Browse terms grouped by focus area. Categories align with navigation and search filters.</p>"},{"location":"categories/#foundations","title":"Foundations","text":"<p>Core machine learning concepts that underpin modern AI systems.</p> <ul> <li>bias-variance tradeoff \u2014 Data Science &amp; Research, Engineering &amp; Platform, Policy &amp; Risk</li> <li>clip \u2014 Engineering &amp; Platform, Data Science &amp; Research, Product &amp; Program Managers, Communications &amp; Enablement</li> <li>confusion matrix \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers, Policy &amp; Risk</li> <li>cross-validation \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>diffusion model \u2014 Engineering &amp; Platform, Data Science &amp; Research, Product &amp; Program Managers, Communications &amp; Enablement</li> <li>f1 score \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>feature engineering \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>generalization \u2014 Product &amp; Program Managers, Engineering &amp; Platform, Data Science &amp; Research</li> <li>generative ai \u2014 Communications &amp; Enablement, Policy &amp; Risk, Product &amp; Program Managers</li> <li>gradient descent \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>loss function \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers, Policy &amp; Risk</li> <li>overfitting \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>precision \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers, Policy &amp; Risk</li> <li>recall \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers, Policy &amp; Risk</li> <li>regularization \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>roc auc \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>target variable \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>test set \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>training data \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>validation set \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>voice cloning \u2014 Product &amp; Program Managers, Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Security &amp; Trust</li> </ul>"},{"location":"categories/#llm-core","title":"LLM Core","text":"<p>Mechanics of transformers, prompting, decoding, and language model internals.</p> <ul> <li>attention \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>beam search \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>chain-of-thought prompting \u2014 Engineering &amp; Platform, Data Science &amp; Research, Product &amp; Program Managers</li> <li>context window \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>decoding \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>direct preference optimization \u2014 Engineering &amp; Platform, Data Science &amp; Research, Product &amp; Program Managers</li> <li>greedy decoding \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>hallucination \u2014 Communications &amp; Enablement, Data Science &amp; Research, Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers</li> <li>kv cache \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>log probability \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>preference dataset \u2014 Data Science &amp; Research, Policy &amp; Risk, Product &amp; Program Managers, Engineering &amp; Platform</li> <li>prompt engineering \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>reinforcement learning from human feedback \u2014 Engineering &amp; Platform, Data Science &amp; Research, Product &amp; Program Managers, Policy &amp; Risk</li> <li>repetition penalty \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>reward model \u2014 Engineering &amp; Platform, Data Science &amp; Research, Policy &amp; Risk</li> <li>robust prompting \u2014 Engineering &amp; Platform, Product &amp; Program Managers, Security &amp; Trust</li> <li>self-consistency decoding \u2014 Engineering &amp; Platform, Data Science &amp; Research</li> <li>system prompt \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>temperature \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>token \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>top-k sampling \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>top-p sampling \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> </ul>"},{"location":"categories/#retrieval-rag","title":"Retrieval &amp; RAG","text":"<p>Tools for grounding models with external knowledge and search infrastructure.</p> <ul> <li>chunking \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>embedding \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>reranking \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>retrieval \u2014 Communications &amp; Enablement, Data Science &amp; Research, Product &amp; Program Managers, Security &amp; Trust, Engineering &amp; Platform, Policy &amp; Risk</li> <li>retrieval-augmented generation \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>vector store \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> </ul>"},{"location":"categories/#agents-tooling","title":"Agents &amp; Tooling","text":"<p>Agent patterns, tool invocation, and orchestration strategies.</p> <ul> <li>agent executor \u2014 Engineering &amp; Platform, Product &amp; Program Managers</li> <li>agentic ai \u2014 Engineering &amp; Platform, Product &amp; Program Managers</li> <li>function calling \u2014 Engineering &amp; Platform, Product &amp; Program Managers, Data Science &amp; Research</li> <li>human handoff \u2014 Product &amp; Program Managers, Communications &amp; Enablement, Engineering &amp; Platform, Policy &amp; Risk</li> <li>memory strategy \u2014 Engineering &amp; Platform, Product &amp; Program Managers</li> <li>self-critique loop \u2014 Engineering &amp; Platform, Product &amp; Program Managers, Policy &amp; Risk</li> <li>tool use \u2014 Engineering &amp; Platform, Product &amp; Program Managers</li> </ul>"},{"location":"categories/#optimization-efficiency","title":"Optimization &amp; Efficiency","text":"<p>Techniques for scaling inference and training effectively.</p> <ul> <li>fine-tuning \u2014 Data Science &amp; Research, Engineering &amp; Platform</li> <li>instruction tuning \u2014 Engineering &amp; Platform, Data Science &amp; Research, Product &amp; Program Managers, Policy &amp; Risk</li> <li>knowledge distillation \u2014 Data Science &amp; Research, Engineering &amp; Platform</li> <li>low-rank adaptation \u2014 Data Science &amp; Research, Engineering &amp; Platform</li> <li>mixture of experts \u2014 Engineering &amp; Platform, Data Science &amp; Research, Product &amp; Program Managers</li> <li>quantization \u2014 Data Science &amp; Research, Engineering &amp; Platform</li> </ul>"},{"location":"categories/#operations-monitoring","title":"Operations &amp; Monitoring","text":"<p>Operational playbooks for running AI in production.</p> <ul> <li>ai circuit breaker \u2014 Engineering &amp; Platform, Security &amp; Trust, Policy &amp; Risk, Product &amp; Program Managers</li> <li>ai incident response \u2014 Communications &amp; Enablement, Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust</li> <li>data lineage \u2014 Engineering &amp; Platform, Data Science &amp; Research, Policy &amp; Risk, Legal &amp; Compliance</li> <li>evaluation \u2014 Communications &amp; Enablement, Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust</li> <li>evaluation harness \u2014 Engineering &amp; Platform, Data Science &amp; Research, Product &amp; Program Managers</li> <li>incident taxonomy \u2014 Policy &amp; Risk, Security &amp; Trust, Product &amp; Program Managers, Engineering &amp; Platform</li> <li>ml observability \u2014 Engineering &amp; Platform, Policy &amp; Risk, Security &amp; Trust</li> <li>ml ops \u2014 Engineering &amp; Platform, Policy &amp; Risk, Security &amp; Trust</li> <li>model drift \u2014 Communications &amp; Enablement, Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust</li> <li>shadow deployment \u2014 Engineering &amp; Platform, Product &amp; Program Managers, Policy &amp; Risk, Security &amp; Trust</li> <li>synthetic data evaluation \u2014 Data Science &amp; Research, Engineering &amp; Platform, Policy &amp; Risk, Product &amp; Program Managers</li> </ul>"},{"location":"categories/#governance-risk","title":"Governance &amp; Risk","text":"<p>Policies, controls, and assessments that ensure responsible AI.</p> <ul> <li>ai assurance \u2014 Product &amp; Program Managers, Engineering &amp; Platform, Policy &amp; Risk, Legal &amp; Compliance, Security &amp; Trust</li> <li>algorithmic audit \u2014 Product &amp; Program Managers, Engineering &amp; Platform, Policy &amp; Risk, Legal &amp; Compliance, Communications &amp; Enablement</li> <li>algorithmic bias \u2014 Policy &amp; Risk, Legal &amp; Compliance, Product &amp; Program Managers, Communications &amp; Enablement, Security &amp; Trust</li> <li>algorithmic impact assessment \u2014 Policy &amp; Risk, Legal &amp; Compliance, Product &amp; Program Managers, Communications &amp; Enablement, Engineering &amp; Platform</li> <li>alignment \u2014 Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers</li> <li>assurance case \u2014 Policy &amp; Risk, Legal &amp; Compliance, Engineering &amp; Platform</li> <li>consent management \u2014 Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Engineering &amp; Platform</li> <li>constitutional ai \u2014 Product &amp; Program Managers, Engineering &amp; Platform, Policy &amp; Risk</li> <li>content moderation \u2014 Policy &amp; Risk, Communications &amp; Enablement, Product &amp; Program Managers, Security &amp; Trust, Engineering &amp; Platform</li> <li>data minimization \u2014 Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust</li> <li>data redaction \u2014 Engineering &amp; Platform, Policy &amp; Risk, Legal &amp; Compliance, Security &amp; Trust</li> <li>data retention \u2014 Legal &amp; Compliance, Policy &amp; Risk, Security &amp; Trust, Product &amp; Program Managers</li> <li>dataset card \u2014 Data Science &amp; Research, Policy &amp; Risk, Legal &amp; Compliance, Product &amp; Program Managers</li> <li>differential privacy \u2014 Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Security &amp; Trust</li> <li>escalation policy \u2014 Product &amp; Program Managers, Policy &amp; Risk, Security &amp; Trust, Engineering &amp; Platform</li> <li>fairness metrics \u2014 Policy &amp; Risk, Legal &amp; Compliance, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>guardrail policy \u2014 Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust, Engineering &amp; Platform</li> <li>guardrails \u2014 Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers</li> <li>impact mitigation plan \u2014 Policy &amp; Risk, Product &amp; Program Managers, Legal &amp; Compliance, Engineering &amp; Platform</li> <li>jailbreak prompt \u2014 Security &amp; Trust, Product &amp; Program Managers, Engineering &amp; Platform</li> <li>model card \u2014 Policy &amp; Risk, Legal &amp; Compliance, Product &amp; Program Managers, Engineering &amp; Platform, Communications &amp; Enablement</li> <li>model governance \u2014 Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers</li> <li>model interpretability \u2014 Engineering &amp; Platform, Policy &amp; Risk, Legal &amp; Compliance, Product &amp; Program Managers</li> <li>privacy \u2014 Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers</li> <li>privacy budget \u2014 Data Science &amp; Research, Policy &amp; Risk, Legal &amp; Compliance, Engineering &amp; Platform</li> <li>privacy impact assessment \u2014 Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust</li> <li>prompt injection \u2014 Security &amp; Trust, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>red teaming \u2014 Communications &amp; Enablement, Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust</li> <li>responsible ai \u2014 Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers</li> <li>risk register \u2014 Policy &amp; Risk, Product &amp; Program Managers, Legal &amp; Compliance, Engineering &amp; Platform</li> <li>safety classifier \u2014 Security &amp; Trust, Policy &amp; Risk, Product &amp; Program Managers, Engineering &amp; Platform</li> <li>safety evaluation \u2014 Engineering &amp; Platform, Policy &amp; Risk, Product &amp; Program Managers, Communications &amp; Enablement</li> <li>safety review board \u2014 Policy &amp; Risk, Legal &amp; Compliance, Security &amp; Trust, Product &amp; Program Managers</li> <li>safety spec \u2014 Product &amp; Program Managers, Engineering &amp; Platform, Policy &amp; Risk, Security &amp; Trust, Communications &amp; Enablement</li> <li>synthetic data \u2014 Data Science &amp; Research, Engineering &amp; Platform, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust</li> <li>transparency report \u2014 Policy &amp; Risk, Legal &amp; Compliance, Communications &amp; Enablement, Product &amp; Program Managers</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for helping expand the glossary. Follow these steps to get started:</p> <ol> <li>Fork the repository and create a feature branch.</li> <li>Install dependencies with <code>pip install -r requirements.txt</code>.</li> <li>Create or update a YAML file under <code>data/terms/</code>, covering categories, roles, citations, and examples.</li> <li>Run <code>make validate</code> to ensure schema and lint checks pass.</li> <li>Run <code>make build</code> followed by <code>cd site &amp;&amp; mkdocs build --strict</code> to confirm the docs compile cleanly.</li> <li>Open a pull request using the provided template.</li> </ol> <p>See the root-level <code>CONTRIBUTING.md</code> for detailed guidance, including formatting rules, review checklist, and citation requirements.</p>"},{"location":"data-model/","title":"Data Model","text":"<p>All glossary entries follow the schema defined in <code>schema/term.schema.json</code>. Each YAML file under <code>data/terms/</code> captures a single concept and includes metadata for cross-team alignment.</p> <pre><code>term: \"example\"\naliases:\n  - \"synonym\"\ncategories:\n  - \"Category name\"\nroles:\n  - \"product\"\npart_of_speech: \"noun\"\nshort_def: \"\u226440 word summary.\"\nlong_def: &gt;-\n  120\u2013200 word explanation that provides history, context, and operational\n  considerations.\naudiences:\n  exec: \"Business-ready framing.\"\n  engineer: \"Technical framing.\"\nexamples:\n  do:\n    - \"What good practice looks like.\"\n  dont:\n    - \"What to avoid.\"\ngovernance:\n  nist_rmf_tags:\n    - \"tag\"\n  risk_notes: \"Key compliance considerations.\"\nrelationships:\n  broader:\n    - \"parent term\"\n  narrower:\n    - \"child term\"\n  related:\n    - \"peer term\"\ncitations:\n  - source: \"Credible reference\"\n    url: \"https://example.com\"\nlicense: \"CC BY-SA 4.0\"\nstatus: \"draft\"\nlast_reviewed: \"2024-01-01\"\n</code></pre>"},{"location":"data-model/#validation-rules","title":"Validation rules","text":"<ul> <li><code>short_def</code> must be 40 words or fewer.</li> <li><code>long_def</code> must fall between 80 and 220 words.</li> <li>Executive and engineering variants are mandatory.</li> <li>Provide at least one example in each of the <code>do</code> and <code>dont</code> lists.</li> <li>Assign at least one category so entries can be grouped in navigation and search.</li> <li>Tag entries with at least one role (<code>product</code>, <code>engineering</code>, <code>data_science</code>, <code>policy</code>,   <code>legal</code>, <code>security</code>, or <code>communications</code>) for role-based guidance.</li> <li>Cite sources with URLs to reputable glossaries, standards, or academic   references.</li> </ul> <p>Run <code>make validate</code> locally to confirm entries match both the JSON Schema and custom rules.</p>"},{"location":"governance-dashboard/","title":"Governance Dashboard","text":"<p>Updated: 2024-11-26</p> <p>This page consolidates governance-oriented glossary content so compliance, policy, legal, and risk teams can quickly assess coverage, discover gaps, and coordinate actions.</p>"},{"location":"governance-dashboard/#snapshot-metrics-nov-26-2024","title":"Snapshot metrics (Nov\u00a026\u00a02024)","text":"<ul> <li>Total glossary entries: 65 (all currently in draft status)</li> <li>Top categories: Governance &amp; Risk (25), LLM Core (18), Operations &amp; Monitoring (13)</li> <li>Most represented roles: Product (57 terms), Engineering (54), Data Science (38)</li> <li>Newly added terms: algorithmic impact assessment, constitutional ai, function calling</li> </ul>"},{"location":"governance-dashboard/#nist-rmf-coverage-by-tag","title":"NIST RMF coverage by tag","text":"NIST tag Representative terms Action Accuracy hallucination, precision Ensure evaluation suites report precision/recall alongside hallucination incidents. Fairness algorithmic bias, fairness metrics Align fairness thresholds with policy requirements and document in model cards. Transparency model interpretability, model card Capture explanation tooling and documentation updates before launch. Privacy data minimization, privacy impact assessment, data retention Validate retention schedules and PIA sign-offs for new features. Risk Management content moderation, voice cloning Review incident response playbooks and escalation paths for emerging risks. Documentation model card, model governance Keep model cards and governance logs up to date with each release."},{"location":"governance-dashboard/#quick-tasks-for-governance-partners","title":"Quick tasks for governance partners","text":"<ol> <li>Review the Role Starter Pack for Policy &amp; Risk and ensure open tasks are assigned.</li> <li>For each upcoming launch, record fairness, privacy, and interpretability decisions in the appropriate glossary term relationships (e.g., link mitigation plans under \u201crelated\u201d).</li> <li>Schedule quarterly audits of the Category Explorer to confirm coverage is current and identify new terms to add.</li> <li>Use the interactive search with the status filter to prioritize review queues.</li> </ol>"},{"location":"governance-dashboard/#intake-backlog-signals","title":"Intake &amp; backlog signals","text":"<ul> <li>Incoming requests: Monitor submissions from the term intake form and tag them for governance review as needed.</li> <li>Backlog priorities: The keyword backlog highlights assurance, audit, and transparency terms awaiting owners.</li> <li>Review cadence: Add backlog review as a standing agenda item for the governance or risk council so high-impact drafts move toward approval.</li> </ul>"},{"location":"governance-dashboard/#useful-resources","title":"Useful resources","text":"<ul> <li>Prompt Engineering Playbook</li> <li>Category Explorer \u2013 Governance &amp; Risk</li> <li>Role Starter Packs \u2013 Legal &amp; Compliance</li> </ul>"},{"location":"keyword-backlog/","title":"Keyword Backlog","text":"<p>Purpose: keep our glossary focused on high-value, high-quality definitions while growing coverage methodically.</p>"},{"location":"keyword-backlog/#how-to-use-this-backlog","title":"How to use this backlog","text":"<ul> <li>Start with the highest-priority items in each cluster; they map to common employee pain points gathered from support, sales, and policy teams.</li> <li>Draft entries with the term schema so definitions stay consistent, cite at least two sources, and include audience guidance.</li> <li>Use <code>make new-term NAME=\"your term\"</code> to generate a scaffold that already meets schema requirements and reminds you to capture quality notes.</li> <li>After drafting, run <code>make check</code> to validate schema, tests, and docs before opening a PR.</li> <li>Pair with a reviewer from another function before publishing; readability and accessibility are part of the review.</li> <li>Update the \"Status\" column after every checkpoint (discovery, drafting, review, published) so everyone sees progress at a glance.</li> </ul>"},{"location":"keyword-backlog/#prioritization-criteria","title":"Prioritization criteria","text":"<ul> <li>Employee impact \u2013 does this term unblock onboarding, customer conversations, or compliance workflows?</li> <li>Source coverage \u2013 does it let us cite underused glossaries (OECD, AI Now, Partnership on AI, LangChain, OpenAI, Anthropic)?</li> <li>Clarity risk \u2013 will the average employee struggle without a carefully written explanation and examples?</li> <li>UX fit \u2013 can we explain it plainly in under 320 characters for the short definition without overwhelming readers?</li> </ul>"},{"location":"keyword-backlog/#cluster-overview","title":"Cluster overview","text":"<p>The clusters align to employee journeys so readers can scan by topic instead of alphabet soup.</p>"},{"location":"keyword-backlog/#pain-point-map","title":"Pain point map","text":"Journey Common friction Target terms Notes Launch governance Proving compliance, documenting mitigations AI assurance, algorithmic audit, transparency report Aligns with policy, legal, and audit stakeholders. Agent operations Safe orchestration of tools and automations agent executor, guardrail policy, human handoff Pair with LangChain/OpenAI docs for implementation recipes. Evaluation maturity Measuring safety and quality at scale evaluation harness, safety spec, robust prompting Ensure red team, QA, and DS share a single playbook. Data responsibility Handling sensitive data lifecycle consent management, impact mitigation plan, data minimization Link to privacy office guidelines and ISO references."},{"location":"keyword-backlog/#compliance-assurance","title":"Compliance &amp; Assurance","text":"<p>Learner moment: \"I have to prove this system meets policy and regulatory expectations.\"</p> Priority Term Employee need Seed sources Owner Status Notes 1 AI assurance Understand the controls and evidence needed to certify an AI system. OECD AI Glossary; Partnership on AI Glossary; AI Now Lexicon Policy &amp; Governance pod Discovery Pair with upcoming risk playbook. 1 algorithmic audit Clarify how independent audits test for bias, safety, and compliance. OECD AI Glossary; AI Now Lexicon Risk &amp; Compliance guild Discovery Differentiate from internal evaluations. 2 transparency report Summarize what disclosures regulators expect for AI deployments. OECD AI Glossary; Partnership on AI Glossary Policy &amp; Governance pod Published Published as <code>transparency report</code> (2025-09-28). 2 impact mitigation plan Show how mitigation tracking links to AIAs and incident response. AI Now Lexicon; Partnership on AI Glossary Product Operations Published Published as <code>impact mitigation plan</code> (2025-09-28). 3 assurance case Explain structured arguments used in safety-critical industries. OECD AI Glossary; Partnership on AI Glossary Safety Council Published Published as <code>assurance case</code> (2025-09-28)."},{"location":"keyword-backlog/#safety-alignment","title":"Safety &amp; Alignment","text":"<p>Learner moment: \"How do we keep models behaving safely as policies evolve?\"</p> Priority Term Employee need Seed sources Owner Status Notes 1 safety spec Give cross-functional teams a single reference for allowed/blocked behaviors. Anthropic / Claude Glossary; MIT Tech Review AI Dictionary Alignment Working Group Discovery Should link to guardrail templates. 1 self-critique loop Describe how models evaluate their own outputs before release. Anthropic / Claude Glossary; DeepLearning.AI Glossary Research Guild Published Published as <code>self-critique loop</code> (2025-09-28). 2 escalation policy Clarify when humans take over from automated systems. OECD AI Glossary; AI Now Lexicon Operations Published Published as <code>escalation policy</code> (2025-09-28). 2 robust prompting Help enablement teams teach defensive prompt design. MIT Tech Review AI Dictionary; Anthropic resources Enablement Chapter Published Published as <code>robust prompting</code> (2025-09-28). 3 consent management Explain how user consent is honored in AI experiences. OECD AI Glossary; Partnership on AI Glossary Privacy Office Published Published as <code>consent management</code> (2025-09-28)."},{"location":"keyword-backlog/#agent-orchestration-tooling","title":"Agent Orchestration &amp; Tooling","text":"<p>Learner moment: \"How do I design agent workflows without exposing users to risk?\"</p> Priority Term Employee need Seed sources Owner Status Notes 1 agent executor Show engineers how orchestration loops schedule tool calls. LangChain Glossary; OpenAI Glossary Agent Platform team Published Published as <code>agent executor</code> (2025-09-28). 1 memory strategy Teach when to use vector memory, summaries, or none. LangChain Glossary; Hugging Face Glossary Agent Platform team Published Published as <code>memory strategy</code> (2025-09-28). 2 evaluation harness Explain automated eval pipelines for agents and RAG flows. LangChain Glossary; OpenAI Glossary QA Guild Published Published as <code>evaluation harness</code> (2025-09-28). 2 guardrail policy Document how policy prompts map to enforcement levels. Anthropic / Claude Glossary; OpenAI Glossary Safety Engineering Published Published as <code>guardrail policy</code> (2025-09-28). 3 human handoff Outline UX patterns for escalating to people mid-session. LangChain Glossary; Partnership on AI Glossary Support Ops Published Published as <code>human handoff</code> (2025-09-28)."},{"location":"keyword-backlog/#seeding-progress-nov-26-2024","title":"Seeding progress (Nov\u00a026\u00a02024)","text":"Source Citations Google ML Glossary 44 NIST AI RMF Glossary 34 Stanford HAI Brief Definitions 30 Hugging Face Glossary 29 Wikipedia AI Glossary 26 UK POST AI Glossary 23 Anthropic / Claude Glossary 1 DeepLearning.AI AI Glossary 1 MIT Technology Review AI Dictionary 1 OECD AI Glossary 1 AI Now Institute Lexicon 1 Partnership on AI Glossary 1 Hugging Face Tokenizers 1 OpenAI Glossary 1 LangChain Documentation Glossary 1 <p>Focus upcoming seeding on the underused sources (Anthropic, OECD, AI Now, Partnership on AI, LangChain, OpenAI) so the glossary stays balanced across technical and governance perspectives.</p>"},{"location":"keyword-backlog/#drafting-checklist","title":"Drafting checklist","text":"<ul> <li>Lead with the employee problem, then deliver inclusive, plain-language definitions.</li> <li>Provide at least one example for the \"do\" and \"dont\" arrays that mirrors real workflows.</li> <li>Cite two or more sources, pulling from at least one underused glossary listed above when possible.</li> <li>Add accessibility by breaking long paragraphs, avoiding jargon, and front-loading the main idea.</li> <li>After draft review, run <code>mkdocs build --strict -f site/mkdocs.yml</code> to catch schema or formatting issues.</li> </ul>"},{"location":"keyword-backlog/#maintaining-readability","title":"Maintaining readability","text":"<ul> <li>Keep short definitions under 160 characters whenever possible so they scan well in mobile contexts.</li> <li>Use sentence case, avoid unexplained abbreviations, and include context in the first clause.</li> <li>When referencing regulations or frameworks, link to employee-friendly summaries instead of dense PDFs.</li> </ul> <p>Keep this backlog fresh: archive published items and add new requests from the intake form so contributors stay focused on what matters most to employees.</p>"},{"location":"prompting/","title":"Prompt Engineering Playbook","text":"<p>Last updated: 2025-09-29</p> <p>The glossary captures a full stack of prompt-related terminology. Use this page as a jumping-off point for crafting reliable instructions and understanding the knobs that affect behavior.</p> <p>Quick links</p> <ul> <li>Role starter pack: Product &amp; Program Managers</li> <li>Role starter pack: Engineering &amp; Platform</li> <li>Search prompts &amp; decoding terms</li> <li>Search governance touchpoints</li> <li>Safety classifier guide</li> <li>Robust prompting tips</li> </ul>"},{"location":"prompting/#core-concepts","title":"Core concepts","text":"<ul> <li>Prompt engineering \u2014 overall workflow for iterating and testing prompts.</li> <li>System prompt \u2014 immutable guardrail at the top of every conversation.</li> <li>Context window \u2014 token budget that constrains prompt size and retrieved context.</li> <li>Token \u2014 smallest unit models consume, essential for cost and context planning.</li> </ul>"},{"location":"prompting/#controlling-outputs","title":"Controlling outputs","text":"<ul> <li>Temperature \u2014 adjusts randomness in sampling; lower for consistency, higher for ideation.</li> <li>Top-k sampling \u2014 limits sampling to the top k candidates per step.</li> <li>Top-p sampling \u2014 chooses from the smallest probability mass that sums to p.</li> <li>Repetition penalty \u2014 discourages loops or repeated phrases.</li> <li>Beam search \u2014 deterministic multi-path decoding for structured responses.</li> <li>Self-consistency decoding \u2014 sample multiple reasoning chains and aggregate the dominant answer before finalizing.</li> <li>Chain-of-thought prompting \u2014 encourage the model to reason step by step before committing to an answer.</li> </ul>"},{"location":"prompting/#grounding-and-retrieval-aids","title":"Grounding and retrieval aids","text":"<ul> <li>Retrieval-augmented generation \u2014 combine prompts with retrieved context.</li> <li>Chunking and vector stores \u2014 structure knowledge bases for precise context windows.</li> <li>Reranking \u2014 surface the best supporting passages before they enter the prompt.</li> <li>Data lineage \u2014 keep track of where grounding data originated so prompt responses remain auditable.</li> <li>Shadow deployment \u2014 run new prompts in parallel to capture telemetry before rollout.</li> </ul>"},{"location":"prompting/#safety-and-governance-touchpoints","title":"Safety and governance touchpoints","text":"<ul> <li>Guardrails \u2014 policy-aligned controls before or after generation.</li> <li>Safety evaluation \u2014 ensure prompt changes don\u2019t undo previous approvals.</li> <li>Model card &amp; content moderation \u2014 document and monitor prompt behavior in production.</li> </ul>"},{"location":"prompting/#prompt-lifecycle-diagram","title":"Prompt lifecycle diagram","text":"<pre><code>flowchart LR\n    A[Define intent &amp; policy] --&gt; B[Draft system &amp; user prompts]\n    B --&gt; C[Test in sandbox]\n    C --&gt; D{Evaluation results}\n    D -- Fails --&gt; B\n    D -- Passes --&gt; E[Shadow deployment]\n    E --&gt; F{Production telemetry}\n    F -- Drift detected --&gt; B\n    F -- Stable --&gt; G[Launch &amp; monitor]\n</code></pre> <p>Use the diagram above as a checklist: always close the loop from telemetry back into prompt iteration so governance signals stay current.</p>"},{"location":"prompting/#quick-checklist-before-launch","title":"Quick checklist before launch","text":"<ol> <li>Version system prompts in source control and log every change.</li> <li>Benchmark prompts across accuracy, hallucination, fairness, jailbreak resilience, and latency metrics.</li> <li>Validate context window usage with representative journeys (long, multilingual, regulated).</li> <li>Document decoding settings (temperature, top-k/top-p, repetition penalties) in model cards.</li> <li>Run red-teaming and safety evaluations when prompts or grounding data change.</li> <li>Observe shadow deployments before launch, reviewing safety classifier scores and human handoff outcomes.</li> </ol> <p>For deeper exploration, use the interactive search with the <code>LLM Core</code> category or filter by your team\u2019s role.</p>"},{"location":"resources/","title":"External Glossary Sources","text":"<p>To seed the glossary we reference widely-used industry and academic vocabularies. The links below are grouped by the primary lens they bring to AI terminology.</p>"},{"location":"resources/#technical-foundations","title":"Technical Foundations","text":"<ul> <li>Google ML Glossary \u2014 classic machine learning terminology with engineer-focused explanations. https://developers.google.com/machine-learning/glossary</li> <li>Hugging Face Glossary \u2014 modern transformer and large language model vocabulary tied to code examples. https://huggingface.co/docs/transformers/main/en/glossary</li> <li>AWS Machine Learning Lens Glossary \u2014 cloud-focused terminology covering services, algorithms, and operational best practices. https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/glossary.html</li> <li>Deepchecks AI Glossary \u2014 practitioner-friendly explainers that balance depth and accessibility. https://cloud.google.com/docs/generative-ai/glossary</li> </ul>"},{"location":"resources/#standards-governance","title":"Standards &amp; Governance","text":"<ul> <li>NIST AI Risk Management Framework \u2014 trust, risk, bias, and governance concepts aligned to the NIST AI RMF. https://www.nist.gov/itl/ai-risk-management-framework</li> <li>ISO/IEC JTC 1/SC 42 AI Standards Glossary \u2014 formal standards vocabulary used across ISO and IEC publications. (PDF availability varies; summaries may be required).</li> <li>OECD AI Glossary \u2014 policy and ethics terminology used by member nations. https://www.oecd.ai/</li> </ul>"},{"location":"resources/#academic-public-reference","title":"Academic &amp; Public Reference","text":"<ul> <li>Wikipedia: Glossary of Artificial Intelligence \u2014 broad coverage updated continually by the community. https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence</li> <li>Stanford HAI Glossary \u2014 concise academic and policy definitions from Stanford's Institute for Human-Centered AI. https://hai.stanford.edu/education/ai-glossary</li> <li>AI Glossary &amp; Learning Hub (EHS) \u2014 journalistic framing of common AI buzzwords and emerging concepts. https://www.ehs.com/ai/ai-glossary-and-learning-hub/</li> </ul>"},{"location":"resources/#sector-specific-perspectives","title":"Sector-Specific Perspectives","text":"<ul> <li>ICO AI &amp; Data Protection Guidance \u2014 plain-language explanations oriented toward public sector stakeholders. https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/guidance-on-ai-and-data-protection/</li> <li>Microsoft OpenAI Responsible Use Guidance \u2014 sociotechnical and ethical terminology for global policy audiences. https://learn.microsoft.com/en-us/azure/ai-foundry/responsible-ai/openai/overview</li> <li>Partnership on AI Glossary \u2014 industry-policy bridge for responsible AI language. https://www.aifalabs.com/ai-glossary</li> </ul>"},{"location":"resources/#ecosystem-tooling","title":"Ecosystem &amp; Tooling","text":"<ul> <li>LangChain Concepts Overview \u2014 agentic system and retrieval-augmented generation terminology from the LangChain ecosystem. https://python.langchain.com/docs/how_to/chatbots_memory</li> <li>Prompt Engineering Overview (Microsoft) \u2014 concepts related to large language model APIs, usage patterns, and safety tools. https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/prompt-engineering</li> <li>Anthropic Tool Use Guide \u2014 alternative vocabulary for Claude models and safety practices. https://docs.anthropic.com/en/docs/claude/tool-use</li> </ul> <p>Use these sources when drafting new entries to keep definitions consistent, well-cited, and grounded in widely recognized language.</p>"},{"location":"resources/#deep-dive-documentation","title":"Deep-dive documentation","text":"<ul> <li>Azure OpenAI Safety Best Practices \u2014 operational guardrails, evaluations, and policy notes for production LLMs. https://learn.microsoft.com/en-us/azure/ai-foundry/responsible-ai/openai/overview</li> <li>AWS AI Glossary (Reference) \u2014 constitutional AI principles and enforcement examples. https://docs.aws.amazon.com/glossary/latest/reference/glos-chap.html</li> <li>Microsoft AI Red Team Guide \u2014 workflows for testing agents and RAG pipelines. https://learn.microsoft.com/en-us/security/ai-red-team/</li> <li>NIST AI RMF Playbook \u2014 implementation profiles and worksheets that complement glossary governance terms. https://www.nist.gov/itl/ai-risk-management-framework/ai-rmf-playbook</li> <li>OECD AI Policy Observatory \u2014 case studies and policy toolkits for responsible AI adoption. https://www.oecd.ai/policy</li> </ul>"},{"location":"roles/","title":"Role Starter Packs","text":""},{"location":"roles/#role-starter-packs","title":"Role Starter Packs","text":"<p>Guidance for common stakeholder groups. Each pack includes actionable steps and key focus areas so teams can operationalize insights immediately.</p>"},{"location":"roles/#product-program-managers","title":"Product &amp; Program Managers","text":"<p>Focus on user outcomes, feature scope, and launch readiness.</p> <p>Action plan - Bookmark the Glossary Search filtered to this role and review the top 5 unfamiliar terms. - Schedule a sync with partner roles listed under each term to clarify ownership and open questions. - Capture insights in your runbook or onboarding guide so future teammates ramp faster.</p>"},{"location":"roles/#guided-learning-path","title":"Guided learning path","text":"<ol> <li>Skim the Governance &amp; Risk category to learn which terms drive launch checklists.</li> <li>Bookmark three model or prompt concepts that influence roadmap trade-offs.</li> <li>Schedule a debrief with policy partners to align on escalation triggers.</li> </ol>"},{"location":"roles/#practice-checklist","title":"Practice checklist","text":"<ul> <li>Review the glossary search filtered to product + governance and log two takeaways in your launch checklist.</li> <li>Pair with engineering to confirm which guardrails or prompts need updates before feature freeze.</li> </ul>"},{"location":"roles/#focus-areas","title":"Focus areas","text":"<ul> <li>Governance &amp; Risk (38 terms)</li> <li>LLM Core (24 terms)</li> <li>Foundations (20 terms)</li> <li>Operations &amp; Monitoring (15 terms)</li> <li>Agents &amp; Tooling (7 terms)</li> <li>Retrieval &amp; RAG (7 terms)</li> <li>Optimization &amp; Efficiency (2 terms)</li> </ul>"},{"location":"roles/#recommended-terms","title":"Recommended terms","text":"<ul> <li>agent executor \u2014 Controller layer that schedules planning, tool calls, and stop conditions so an AI agent completes tasks safely.</li> <li>agentic ai \u2014 Systems that plan, act, and iterate with minimal human prompts by chaining model calls and tools.</li> <li>ai assurance \u2014 Discipline that produces evidence, controls, and attestations showing an AI system meets agreed safety, compliance, and performance thresholds.</li> <li>ai circuit breaker \u2014 Automated control that halts model responses or tool access when risk thresholds are exceeded.</li> <li>ai incident response \u2014 Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.</li> <li>algorithmic audit \u2014 Independent review of an AI system\u2019s data, design, and outcomes to verify compliance, fairness, and risk controls.</li> <li>algorithmic bias \u2014 Systematic unfairness in model outputs that disadvantages certain groups or outcomes.</li> <li>algorithmic impact assessment \u2014 Structured review that documents how an AI system may affect people, processes, and compliance obligations.</li> <li>alignment \u2014 Making sure AI systems optimize for human values, policies, and intended outcomes.</li> <li>attention \u2014 Technique enabling models to weight input tokens differently when producing each output.</li> <li>beam search \u2014 Deterministic decoding that keeps the top scoring sequences across multiple beams before selecting the final output.</li> <li>chain-of-thought prompting \u2014 Prompting technique that asks models to reason through intermediate steps before giving a final answer.</li> <li>chunking \u2014 Splitting source documents into manageable pieces before indexing or feeding them to models.</li> <li>clip \u2014 Multimodal model that embeds images and text into a shared space using contrastive learning.</li> <li>confusion matrix \u2014 Table that summarizes true/false positives and negatives to diagnose classification performance.</li> <li>consent management \u2014 Practices that capture, honor, and audit user permissions across AI features.</li> <li>constitutional ai \u2014 Alignment approach where models critique and revise their own outputs against a written set of principles.</li> <li>content moderation \u2014 Workflows and tools that review, filter, and act on user-generated content to enforce policy.</li> <li>context window \u2014 Maximum number of tokens a model can consider at once during prompting or inference.</li> <li>cross-validation \u2014 Evaluation technique that splits data into multiple folds to estimate model performance on unseen samples.</li> <li>data minimization \u2014 Principle of collecting and retaining only the data necessary for a defined purpose.</li> <li>data retention \u2014 Policies defining how long data is stored, where it lives, and how it is deleted.</li> <li>dataset card \u2014 Structured documentation describing a dataset\u2019s purpose, composition, risks, and usage constraints.</li> <li>decoding \u2014 Algorithms that turn model probability distributions into output tokens during generation.</li> <li>diffusion model \u2014 Generative model that iteratively denoises random noise to synthesize images, audio, or other data.</li> <li>direct preference optimization \u2014 Alignment technique that fine-tunes models directly on preference data without training a separate reward model.</li> <li>embedding \u2014 Dense numerical representation that captures semantic meaning of text, images, or other data.</li> <li>escalation policy \u2014 Playbook that defines when and how AI systems route control to human reviewers.</li> <li>evaluation \u2014 Systematic measurement of model performance, safety, and reliability using defined tests.</li> <li>evaluation harness \u2014 Automated pipeline that replays tasks, scores outputs, and reports regressions for AI systems.</li> <li>f1 score \u2014 Harmonic mean of precision and recall, balancing false positives and false negatives.</li> <li>fairness metrics \u2014 Quantitative measures that evaluate whether model performance is equitable across groups.</li> <li>feature engineering \u2014 Transforming raw data into model-ready features that improve signal, fairness, and maintainability.</li> <li>function calling \u2014 LLM capability that lets prompts invoke predefined functions and return structured arguments.</li> <li>generalization \u2014 Model's ability to sustain performance on unseen data rather than memorising the training set.</li> <li>generative ai \u2014 Family of models that produce new content\u2014text, images, code\u2014rather than only making predictions.</li> <li>gradient descent \u2014 Iterative optimization algorithm that updates model parameters in the direction of the negative gradient to minimize a loss function.</li> <li>greedy decoding \u2014 Strategy that selects the highest-probability token at each step, producing deterministic outputs.</li> <li>guardrail policy \u2014 Documented rules and prompts that define allowed, blocked, and escalated behaviors for AI systems.</li> <li>guardrails \u2014 Controls that constrain model behavior to comply with safety, legal, or brand requirements.</li> <li>hallucination \u2014 When an AI model presents fabricated or unsupported information as fact.</li> <li>human handoff \u2014 Moment when an AI workflow transfers control to a human for review or action.</li> <li>impact mitigation plan \u2014 Action plan that tracks risks, mitigations, owners, and timelines for an AI deployment.</li> <li>incident taxonomy \u2014 Standardized categories used to tag, analyze, and report AI incidents consistently.</li> <li>instruction tuning \u2014 Supervised training that teaches models to follow natural-language instructions using curated examples.</li> <li>jailbreak prompt \u2014 Crafted input that persuades a model to ignore safety instructions and produce disallowed responses.</li> <li>kv cache \u2014 Stored attention keys and values reused across decoding steps to speed sequential generation.</li> <li>log probability \u2014 Logarithm of a token\u2019s probability, used to inspect model confidence and guide decoding tweaks.</li> <li>loss function \u2014 Mathematical rule that scores how far model predictions deviate from desired targets.</li> <li>memory strategy \u2014 Deliberate approach for when an AI agent stores, retrieves, or forgets context across tasks.</li> <li>mixture of experts \u2014 Neural architecture that routes tokens to specialized submodels to scale capacity efficiently.</li> <li>model card \u2014 Standardized documentation describing a model\u2019s purpose, data, performance, and limitations.</li> <li>model drift \u2014 Gradual mismatch between model assumptions and real-world data that degrades performance over time.</li> <li>model governance \u2014 Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.</li> <li>model interpretability \u2014 Ability to explain how a model arrives at its predictions in ways stakeholders understand.</li> <li>overfitting \u2014 When a model memorizes training data patterns so closely that it performs poorly on new samples.</li> <li>precision \u2014 Share of predicted positives that are actually correct for a given classifier.</li> <li>preference dataset \u2014 Labeled comparisons of model outputs that capture which responses humans prefer.</li> <li>privacy \u2014 Principle of limiting data collection, use, and exposure to protect individuals\u2019 information.</li> <li>privacy impact assessment \u2014 Structured review that evaluates how a system collects, uses, and safeguards personal data.</li> <li>prompt engineering \u2014 Crafting and testing prompts to steer model behavior toward desired outcomes.</li> <li>prompt injection \u2014 Attack that inserts malicious instructions into model inputs to override original prompts or policies.</li> <li>recall \u2014 Share of actual positives a model successfully identifies.</li> <li>red teaming \u2014 Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.</li> <li>regularization \u2014 Techniques that add penalties or constraints during training to reduce overfitting and improve generalisation.</li> <li>reinforcement learning from human feedback \u2014 Training approach that tunes a model using reward signals learned from human preference data.</li> <li>repetition penalty \u2014 Decoding adjustment that down-weights tokens already generated to reduce loops and repeated phrases.</li> <li>reranking \u2014 Step that refines retrieval results using a more precise but slower scoring model.</li> <li>responsible ai \u2014 Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.</li> <li>retrieval \u2014 Process of selecting relevant documents or vectors from a corpus in response to a query.</li> <li>retrieval-augmented generation \u2014 Workflow that grounds a generative model with retrieved context before producing output.</li> <li>risk register \u2014 Central list of identified AI risks, their owners, mitigations, and review status.</li> <li>robust prompting \u2014 Prompt design techniques that harden models against injections, ambiguity, and unsafe outputs.</li> <li>roc auc \u2014 Metric summarizing binary classifier performance by measuring area under the ROC curve.</li> <li>safety classifier \u2014 Model that detects policy-violating or risky content before or after generation.</li> <li>safety evaluation \u2014 Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.</li> <li>safety review board \u2014 Cross-functional committee that approves high-risk AI launches and monitors mitigations.</li> <li>safety spec \u2014 Document that codifies allowed, disallowed, and escalated behaviours for an AI system so teams can enforce safety and policy expectations.</li> <li>self-critique loop \u2014 Pattern where a model reviews its own outputs, critiques them, and produces revisions before responding.</li> <li>shadow deployment \u2014 Deploying an AI system alongside the existing workflow without user impact to collect telemetry.</li> <li>synthetic data \u2014 Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.</li> <li>synthetic data evaluation \u2014 Process for measuring fidelity, utility, privacy, and bias of synthetic datasets before use.</li> <li>system prompt \u2014 Foundational instruction that sets role, tone, and guardrails for an AI assistant before user input.</li> <li>target variable \u2014 Outcome the model is trained to predict, providing the signal for calculating loss.</li> <li>temperature \u2014 Decoding parameter that controls how random or deterministic a model\u2019s outputs are.</li> <li>test set \u2014 Final evaluation split reserved for measuring real-world performance after all model tuning is finished.</li> <li>token \u2014 Smallest unit of text a model processes after tokenization, such as a word fragment or character.</li> <li>tool use \u2014 Pattern where a model selects external tools or functions to handle parts of a task.</li> <li>top-k sampling \u2014 Decoding method that samples from the k most probable next tokens to balance diversity and control.</li> <li>top-p sampling \u2014 Decoding strategy that samples from the smallest set of tokens whose probabilities sum to p.</li> <li>training data \u2014 Labeled examples the model learns from before it ever sees validation or test inputs.</li> <li>transparency report \u2014 Periodic disclosure that details how an AI system operates, what data it handles, and how risks are mitigated.</li> <li>validation set \u2014 Dataset slice used to tune hyperparameters and compare experiments without touching the test set.</li> <li>vector store \u2014 Database optimized to store embeddings and execute similarity search over vectors.</li> <li>voice cloning \u2014 Technique that replicates a person\u2019s voice using generative models trained on audio samples.</li> </ul>"},{"location":"roles/#engineering-platform","title":"Engineering &amp; Platform","text":"<p>Own model integration, infra, and technical debt.</p> <p>Action plan - Bookmark the Glossary Search filtered to this role and review the top 5 unfamiliar terms. - Schedule a sync with partner roles listed under each term to clarify ownership and open questions. - Capture insights in your runbook or onboarding guide so future teammates ramp faster.</p>"},{"location":"roles/#guided-learning-path_1","title":"Guided learning path","text":"<ol> <li>Start with LLM Core mechanics to understand knobs that affect reliability.</li> <li>Review Operations &amp; Monitoring entries and note which metrics to add to dashboards.</li> <li>Pair with policy leads on governance terms that require instrumentation support.</li> </ol>"},{"location":"roles/#practice-checklist_1","title":"Practice checklist","text":"<ul> <li>Use the search filters (engineering + operations) and capture metrics to wire into observability dashboards.</li> <li>Document deployment actions in your runbook using examples referenced in the glossary.</li> </ul>"},{"location":"roles/#focus-areas_1","title":"Focus areas","text":"<ul> <li>Governance &amp; Risk (29 terms)</li> <li>LLM Core (26 terms)</li> <li>Foundations (19 terms)</li> <li>Operations &amp; Monitoring (18 terms)</li> <li>Agents &amp; Tooling (7 terms)</li> <li>Retrieval &amp; RAG (7 terms)</li> <li>Optimization &amp; Efficiency (6 terms)</li> </ul>"},{"location":"roles/#recommended-terms_1","title":"Recommended terms","text":"<ul> <li>agent executor \u2014 Controller layer that schedules planning, tool calls, and stop conditions so an AI agent completes tasks safely.</li> <li>agentic ai \u2014 Systems that plan, act, and iterate with minimal human prompts by chaining model calls and tools.</li> <li>ai assurance \u2014 Discipline that produces evidence, controls, and attestations showing an AI system meets agreed safety, compliance, and performance thresholds.</li> <li>ai circuit breaker \u2014 Automated control that halts model responses or tool access when risk thresholds are exceeded.</li> <li>ai incident response \u2014 Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.</li> <li>algorithmic audit \u2014 Independent review of an AI system\u2019s data, design, and outcomes to verify compliance, fairness, and risk controls.</li> <li>algorithmic impact assessment \u2014 Structured review that documents how an AI system may affect people, processes, and compliance obligations.</li> <li>assurance case \u2014 Structured argument that proves an AI system meets safety and compliance expectations.</li> <li>attention \u2014 Technique enabling models to weight input tokens differently when producing each output.</li> <li>beam search \u2014 Deterministic decoding that keeps the top scoring sequences across multiple beams before selecting the final output.</li> <li>bias-variance tradeoff \u2014 Balance between underfitting and overfitting: low bias increases variance, while high bias lowers variance but misses patterns.</li> <li>chain-of-thought prompting \u2014 Prompting technique that asks models to reason through intermediate steps before giving a final answer.</li> <li>chunking \u2014 Splitting source documents into manageable pieces before indexing or feeding them to models.</li> <li>clip \u2014 Multimodal model that embeds images and text into a shared space using contrastive learning.</li> <li>confusion matrix \u2014 Table that summarizes true/false positives and negatives to diagnose classification performance.</li> <li>consent management \u2014 Practices that capture, honor, and audit user permissions across AI features.</li> <li>constitutional ai \u2014 Alignment approach where models critique and revise their own outputs against a written set of principles.</li> <li>content moderation \u2014 Workflows and tools that review, filter, and act on user-generated content to enforce policy.</li> <li>context window \u2014 Maximum number of tokens a model can consider at once during prompting or inference.</li> <li>cross-validation \u2014 Evaluation technique that splits data into multiple folds to estimate model performance on unseen samples.</li> <li>data lineage \u2014 Traceable record of how data moves, transforms, and is used across AI systems.</li> <li>data redaction \u2014 Removal or masking of sensitive fields before data is stored, shared, or used for model training.</li> <li>decoding \u2014 Algorithms that turn model probability distributions into output tokens during generation.</li> <li>differential privacy \u2014 Mathematical framework that limits how much any single record influences published data or model outputs.</li> <li>diffusion model \u2014 Generative model that iteratively denoises random noise to synthesize images, audio, or other data.</li> <li>direct preference optimization \u2014 Alignment technique that fine-tunes models directly on preference data without training a separate reward model.</li> <li>embedding \u2014 Dense numerical representation that captures semantic meaning of text, images, or other data.</li> <li>escalation policy \u2014 Playbook that defines when and how AI systems route control to human reviewers.</li> <li>evaluation \u2014 Systematic measurement of model performance, safety, and reliability using defined tests.</li> <li>evaluation harness \u2014 Automated pipeline that replays tasks, scores outputs, and reports regressions for AI systems.</li> <li>f1 score \u2014 Harmonic mean of precision and recall, balancing false positives and false negatives.</li> <li>fairness metrics \u2014 Quantitative measures that evaluate whether model performance is equitable across groups.</li> <li>feature engineering \u2014 Transforming raw data into model-ready features that improve signal, fairness, and maintainability.</li> <li>fine-tuning \u2014 Additional training that adapts a pretrained model to a specific task or domain.</li> <li>function calling \u2014 LLM capability that lets prompts invoke predefined functions and return structured arguments.</li> <li>generalization \u2014 Model's ability to sustain performance on unseen data rather than memorising the training set.</li> <li>gradient descent \u2014 Iterative optimization algorithm that updates model parameters in the direction of the negative gradient to minimize a loss function.</li> <li>greedy decoding \u2014 Strategy that selects the highest-probability token at each step, producing deterministic outputs.</li> <li>guardrail policy \u2014 Documented rules and prompts that define allowed, blocked, and escalated behaviors for AI systems.</li> <li>hallucination \u2014 When an AI model presents fabricated or unsupported information as fact.</li> <li>human handoff \u2014 Moment when an AI workflow transfers control to a human for review or action.</li> <li>impact mitigation plan \u2014 Action plan that tracks risks, mitigations, owners, and timelines for an AI deployment.</li> <li>incident taxonomy \u2014 Standardized categories used to tag, analyze, and report AI incidents consistently.</li> <li>instruction tuning \u2014 Supervised training that teaches models to follow natural-language instructions using curated examples.</li> <li>jailbreak prompt \u2014 Crafted input that persuades a model to ignore safety instructions and produce disallowed responses.</li> <li>knowledge distillation \u2014 Technique that trains a smaller student model to mimic a larger teacher model\u2019s behavior.</li> <li>kv cache \u2014 Stored attention keys and values reused across decoding steps to speed sequential generation.</li> <li>log probability \u2014 Logarithm of a token\u2019s probability, used to inspect model confidence and guide decoding tweaks.</li> <li>loss function \u2014 Mathematical rule that scores how far model predictions deviate from desired targets.</li> <li>low-rank adaptation \u2014 Parameter-efficient fine-tuning that injects low-rank update matrices into transformer weights.</li> <li>memory strategy \u2014 Deliberate approach for when an AI agent stores, retrieves, or forgets context across tasks.</li> <li>mixture of experts \u2014 Neural architecture that routes tokens to specialized submodels to scale capacity efficiently.</li> <li>ml observability \u2014 Practices and tooling that surface model health through metrics, traces, and alerts across the lifecycle.</li> <li>ml ops \u2014 Operational discipline that manages ML models from experimentation through deployment and monitoring.</li> <li>model card \u2014 Standardized documentation describing a model\u2019s purpose, data, performance, and limitations.</li> <li>model drift \u2014 Gradual mismatch between model assumptions and real-world data that degrades performance over time.</li> <li>model interpretability \u2014 Ability to explain how a model arrives at its predictions in ways stakeholders understand.</li> <li>overfitting \u2014 When a model memorizes training data patterns so closely that it performs poorly on new samples.</li> <li>precision \u2014 Share of predicted positives that are actually correct for a given classifier.</li> <li>preference dataset \u2014 Labeled comparisons of model outputs that capture which responses humans prefer.</li> <li>privacy budget \u2014 Quantitative limit on how much privacy loss is allowed when applying differential privacy.</li> <li>prompt engineering \u2014 Crafting and testing prompts to steer model behavior toward desired outcomes.</li> <li>prompt injection \u2014 Attack that inserts malicious instructions into model inputs to override original prompts or policies.</li> <li>quantization \u2014 Technique that compresses model weights into lower-precision formats to shrink size and speed inference.</li> <li>recall \u2014 Share of actual positives a model successfully identifies.</li> <li>red teaming \u2014 Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.</li> <li>regularization \u2014 Techniques that add penalties or constraints during training to reduce overfitting and improve generalisation.</li> <li>reinforcement learning from human feedback \u2014 Training approach that tunes a model using reward signals learned from human preference data.</li> <li>repetition penalty \u2014 Decoding adjustment that down-weights tokens already generated to reduce loops and repeated phrases.</li> <li>reranking \u2014 Step that refines retrieval results using a more precise but slower scoring model.</li> <li>retrieval \u2014 Process of selecting relevant documents or vectors from a corpus in response to a query.</li> <li>retrieval-augmented generation \u2014 Workflow that grounds a generative model with retrieved context before producing output.</li> <li>reward model \u2014 Model trained on human preferences that scores AI responses for alignment or quality.</li> <li>risk register \u2014 Central list of identified AI risks, their owners, mitigations, and review status.</li> <li>robust prompting \u2014 Prompt design techniques that harden models against injections, ambiguity, and unsafe outputs.</li> <li>roc auc \u2014 Metric summarizing binary classifier performance by measuring area under the ROC curve.</li> <li>safety classifier \u2014 Model that detects policy-violating or risky content before or after generation.</li> <li>safety evaluation \u2014 Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.</li> <li>safety spec \u2014 Document that codifies allowed, disallowed, and escalated behaviours for an AI system so teams can enforce safety and policy expectations.</li> <li>self-consistency decoding \u2014 Decoding strategy that samples multiple reasoning paths and aggregates the most consistent answer.</li> <li>self-critique loop \u2014 Pattern where a model reviews its own outputs, critiques them, and produces revisions before responding.</li> <li>shadow deployment \u2014 Deploying an AI system alongside the existing workflow without user impact to collect telemetry.</li> <li>synthetic data \u2014 Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.</li> <li>synthetic data evaluation \u2014 Process for measuring fidelity, utility, privacy, and bias of synthetic datasets before use.</li> <li>system prompt \u2014 Foundational instruction that sets role, tone, and guardrails for an AI assistant before user input.</li> <li>target variable \u2014 Outcome the model is trained to predict, providing the signal for calculating loss.</li> <li>temperature \u2014 Decoding parameter that controls how random or deterministic a model\u2019s outputs are.</li> <li>test set \u2014 Final evaluation split reserved for measuring real-world performance after all model tuning is finished.</li> <li>token \u2014 Smallest unit of text a model processes after tokenization, such as a word fragment or character.</li> <li>tool use \u2014 Pattern where a model selects external tools or functions to handle parts of a task.</li> <li>top-k sampling \u2014 Decoding method that samples from the k most probable next tokens to balance diversity and control.</li> <li>top-p sampling \u2014 Decoding strategy that samples from the smallest set of tokens whose probabilities sum to p.</li> <li>training data \u2014 Labeled examples the model learns from before it ever sees validation or test inputs.</li> <li>validation set \u2014 Dataset slice used to tune hyperparameters and compare experiments without touching the test set.</li> <li>vector store \u2014 Database optimized to store embeddings and execute similarity search over vectors.</li> </ul>"},{"location":"roles/#data-science-research","title":"Data Science &amp; Research","text":"<p>Drive experimentation, measurement, and model improvement.</p> <p>Action plan - Bookmark the Glossary Search filtered to this role and review the top 5 unfamiliar terms. - Schedule a sync with partner roles listed under each term to clarify ownership and open questions. - Capture insights in your runbook or onboarding guide so future teammates ramp faster.</p>"},{"location":"roles/#guided-learning-path_2","title":"Guided learning path","text":"<ol> <li>Refresh foundational metrics (precision, recall, ROC AUC) to ensure evaluation coverage.</li> <li>Study Optimization &amp; Efficiency techniques to plan future experiments.</li> <li>Document how governance-aligned metrics will be reported to stakeholders.</li> </ol>"},{"location":"roles/#practice-checklist_2","title":"Practice checklist","text":"<ul> <li>Select one evaluation metric and one mitigation technique from the glossary for your next experiment brief.</li> <li>Record baseline measurements tied to the definitions before shipping changes.</li> </ul>"},{"location":"roles/#focus-areas_2","title":"Focus areas","text":"<ul> <li>LLM Core (23 terms)</li> <li>Foundations (19 terms)</li> <li>Retrieval &amp; RAG (7 terms)</li> <li>Optimization &amp; Efficiency (6 terms)</li> <li>Governance &amp; Risk (5 terms)</li> <li>Operations &amp; Monitoring (4 terms)</li> <li>Agents &amp; Tooling (1 term)</li> </ul>"},{"location":"roles/#recommended-terms_2","title":"Recommended terms","text":"<ul> <li>attention \u2014 Technique enabling models to weight input tokens differently when producing each output.</li> <li>beam search \u2014 Deterministic decoding that keeps the top scoring sequences across multiple beams before selecting the final output.</li> <li>bias-variance tradeoff \u2014 Balance between underfitting and overfitting: low bias increases variance, while high bias lowers variance but misses patterns.</li> <li>chain-of-thought prompting \u2014 Prompting technique that asks models to reason through intermediate steps before giving a final answer.</li> <li>chunking \u2014 Splitting source documents into manageable pieces before indexing or feeding them to models.</li> <li>clip \u2014 Multimodal model that embeds images and text into a shared space using contrastive learning.</li> <li>confusion matrix \u2014 Table that summarizes true/false positives and negatives to diagnose classification performance.</li> <li>context window \u2014 Maximum number of tokens a model can consider at once during prompting or inference.</li> <li>cross-validation \u2014 Evaluation technique that splits data into multiple folds to estimate model performance on unseen samples.</li> <li>data lineage \u2014 Traceable record of how data moves, transforms, and is used across AI systems.</li> <li>dataset card \u2014 Structured documentation describing a dataset\u2019s purpose, composition, risks, and usage constraints.</li> <li>decoding \u2014 Algorithms that turn model probability distributions into output tokens during generation.</li> <li>diffusion model \u2014 Generative model that iteratively denoises random noise to synthesize images, audio, or other data.</li> <li>direct preference optimization \u2014 Alignment technique that fine-tunes models directly on preference data without training a separate reward model.</li> <li>embedding \u2014 Dense numerical representation that captures semantic meaning of text, images, or other data.</li> <li>evaluation harness \u2014 Automated pipeline that replays tasks, scores outputs, and reports regressions for AI systems.</li> <li>f1 score \u2014 Harmonic mean of precision and recall, balancing false positives and false negatives.</li> <li>feature engineering \u2014 Transforming raw data into model-ready features that improve signal, fairness, and maintainability.</li> <li>fine-tuning \u2014 Additional training that adapts a pretrained model to a specific task or domain.</li> <li>function calling \u2014 LLM capability that lets prompts invoke predefined functions and return structured arguments.</li> <li>generalization \u2014 Model's ability to sustain performance on unseen data rather than memorising the training set.</li> <li>gradient descent \u2014 Iterative optimization algorithm that updates model parameters in the direction of the negative gradient to minimize a loss function.</li> <li>greedy decoding \u2014 Strategy that selects the highest-probability token at each step, producing deterministic outputs.</li> <li>hallucination \u2014 When an AI model presents fabricated or unsupported information as fact.</li> <li>instruction tuning \u2014 Supervised training that teaches models to follow natural-language instructions using curated examples.</li> <li>knowledge distillation \u2014 Technique that trains a smaller student model to mimic a larger teacher model\u2019s behavior.</li> <li>kv cache \u2014 Stored attention keys and values reused across decoding steps to speed sequential generation.</li> <li>log probability \u2014 Logarithm of a token\u2019s probability, used to inspect model confidence and guide decoding tweaks.</li> <li>loss function \u2014 Mathematical rule that scores how far model predictions deviate from desired targets.</li> <li>low-rank adaptation \u2014 Parameter-efficient fine-tuning that injects low-rank update matrices into transformer weights.</li> <li>mixture of experts \u2014 Neural architecture that routes tokens to specialized submodels to scale capacity efficiently.</li> <li>overfitting \u2014 When a model memorizes training data patterns so closely that it performs poorly on new samples.</li> <li>precision \u2014 Share of predicted positives that are actually correct for a given classifier.</li> <li>preference dataset \u2014 Labeled comparisons of model outputs that capture which responses humans prefer.</li> <li>privacy budget \u2014 Quantitative limit on how much privacy loss is allowed when applying differential privacy.</li> <li>prompt engineering \u2014 Crafting and testing prompts to steer model behavior toward desired outcomes.</li> <li>quantization \u2014 Technique that compresses model weights into lower-precision formats to shrink size and speed inference.</li> <li>recall \u2014 Share of actual positives a model successfully identifies.</li> <li>regularization \u2014 Techniques that add penalties or constraints during training to reduce overfitting and improve generalisation.</li> <li>reinforcement learning from human feedback \u2014 Training approach that tunes a model using reward signals learned from human preference data.</li> <li>repetition penalty \u2014 Decoding adjustment that down-weights tokens already generated to reduce loops and repeated phrases.</li> <li>reranking \u2014 Step that refines retrieval results using a more precise but slower scoring model.</li> <li>retrieval \u2014 Process of selecting relevant documents or vectors from a corpus in response to a query.</li> <li>retrieval-augmented generation \u2014 Workflow that grounds a generative model with retrieved context before producing output.</li> <li>reward model \u2014 Model trained on human preferences that scores AI responses for alignment or quality.</li> <li>roc auc \u2014 Metric summarizing binary classifier performance by measuring area under the ROC curve.</li> <li>self-consistency decoding \u2014 Decoding strategy that samples multiple reasoning paths and aggregates the most consistent answer.</li> <li>synthetic data \u2014 Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.</li> <li>synthetic data evaluation \u2014 Process for measuring fidelity, utility, privacy, and bias of synthetic datasets before use.</li> <li>system prompt \u2014 Foundational instruction that sets role, tone, and guardrails for an AI assistant before user input.</li> <li>target variable \u2014 Outcome the model is trained to predict, providing the signal for calculating loss.</li> <li>temperature \u2014 Decoding parameter that controls how random or deterministic a model\u2019s outputs are.</li> <li>test set \u2014 Final evaluation split reserved for measuring real-world performance after all model tuning is finished.</li> <li>token \u2014 Smallest unit of text a model processes after tokenization, such as a word fragment or character.</li> <li>top-k sampling \u2014 Decoding method that samples from the k most probable next tokens to balance diversity and control.</li> <li>top-p sampling \u2014 Decoding strategy that samples from the smallest set of tokens whose probabilities sum to p.</li> <li>training data \u2014 Labeled examples the model learns from before it ever sees validation or test inputs.</li> <li>validation set \u2014 Dataset slice used to tune hyperparameters and compare experiments without touching the test set.</li> <li>vector store \u2014 Database optimized to store embeddings and execute similarity search over vectors.</li> </ul>"},{"location":"roles/#policy-risk","title":"Policy &amp; Risk","text":"<p>Ensure responsible AI controls align with governance frameworks.</p> <p>Action plan - Bookmark the Glossary Search filtered to this role and review the top 5 unfamiliar terms. - Schedule a sync with partner roles listed under each term to clarify ownership and open questions. - Capture insights in your runbook or onboarding guide so future teammates ramp faster.</p>"},{"location":"roles/#guided-learning-path_3","title":"Guided learning path","text":"<ol> <li>Read algorithmic governance terms to map glossary content to internal controls.</li> <li>Identify three technical concepts to discuss with engineering for upcoming reviews.</li> <li>Draft guidance for disclosure or transparency using relevant glossary examples.</li> </ol>"},{"location":"roles/#practice-checklist_3","title":"Practice checklist","text":"<ul> <li>Draft a review checklist referencing the top three governance terms surfaced in the backlog.</li> <li>Map required disclosures for the next launch memo using linked glossary examples.</li> </ul>"},{"location":"roles/#focus-areas_3","title":"Focus areas","text":"<ul> <li>Governance &amp; Risk (40 terms)</li> <li>Operations &amp; Monitoring (17 terms)</li> <li>Foundations (7 terms)</li> <li>LLM Core (6 terms)</li> <li>Agents &amp; Tooling (2 terms)</li> <li>Optimization &amp; Efficiency (1 term)</li> <li>Retrieval &amp; RAG (1 term)</li> </ul>"},{"location":"roles/#recommended-terms_3","title":"Recommended terms","text":"<ul> <li>ai assurance \u2014 Discipline that produces evidence, controls, and attestations showing an AI system meets agreed safety, compliance, and performance thresholds.</li> <li>ai circuit breaker \u2014 Automated control that halts model responses or tool access when risk thresholds are exceeded.</li> <li>ai incident response \u2014 Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.</li> <li>algorithmic audit \u2014 Independent review of an AI system\u2019s data, design, and outcomes to verify compliance, fairness, and risk controls.</li> <li>algorithmic bias \u2014 Systematic unfairness in model outputs that disadvantages certain groups or outcomes.</li> <li>algorithmic impact assessment \u2014 Structured review that documents how an AI system may affect people, processes, and compliance obligations.</li> <li>alignment \u2014 Making sure AI systems optimize for human values, policies, and intended outcomes.</li> <li>assurance case \u2014 Structured argument that proves an AI system meets safety and compliance expectations.</li> <li>bias-variance tradeoff \u2014 Balance between underfitting and overfitting: low bias increases variance, while high bias lowers variance but misses patterns.</li> <li>confusion matrix \u2014 Table that summarizes true/false positives and negatives to diagnose classification performance.</li> <li>consent management \u2014 Practices that capture, honor, and audit user permissions across AI features.</li> <li>constitutional ai \u2014 Alignment approach where models critique and revise their own outputs against a written set of principles.</li> <li>content moderation \u2014 Workflows and tools that review, filter, and act on user-generated content to enforce policy.</li> <li>data lineage \u2014 Traceable record of how data moves, transforms, and is used across AI systems.</li> <li>data minimization \u2014 Principle of collecting and retaining only the data necessary for a defined purpose.</li> <li>data redaction \u2014 Removal or masking of sensitive fields before data is stored, shared, or used for model training.</li> <li>data retention \u2014 Policies defining how long data is stored, where it lives, and how it is deleted.</li> <li>dataset card \u2014 Structured documentation describing a dataset\u2019s purpose, composition, risks, and usage constraints.</li> <li>differential privacy \u2014 Mathematical framework that limits how much any single record influences published data or model outputs.</li> <li>escalation policy \u2014 Playbook that defines when and how AI systems route control to human reviewers.</li> <li>evaluation \u2014 Systematic measurement of model performance, safety, and reliability using defined tests.</li> <li>fairness metrics \u2014 Quantitative measures that evaluate whether model performance is equitable across groups.</li> <li>generative ai \u2014 Family of models that produce new content\u2014text, images, code\u2014rather than only making predictions.</li> <li>guardrail policy \u2014 Documented rules and prompts that define allowed, blocked, and escalated behaviors for AI systems.</li> <li>guardrails \u2014 Controls that constrain model behavior to comply with safety, legal, or brand requirements.</li> <li>hallucination \u2014 When an AI model presents fabricated or unsupported information as fact.</li> <li>human handoff \u2014 Moment when an AI workflow transfers control to a human for review or action.</li> <li>impact mitigation plan \u2014 Action plan that tracks risks, mitigations, owners, and timelines for an AI deployment.</li> <li>incident taxonomy \u2014 Standardized categories used to tag, analyze, and report AI incidents consistently.</li> <li>instruction tuning \u2014 Supervised training that teaches models to follow natural-language instructions using curated examples.</li> <li>loss function \u2014 Mathematical rule that scores how far model predictions deviate from desired targets.</li> <li>ml observability \u2014 Practices and tooling that surface model health through metrics, traces, and alerts across the lifecycle.</li> <li>ml ops \u2014 Operational discipline that manages ML models from experimentation through deployment and monitoring.</li> <li>model card \u2014 Standardized documentation describing a model\u2019s purpose, data, performance, and limitations.</li> <li>model drift \u2014 Gradual mismatch between model assumptions and real-world data that degrades performance over time.</li> <li>model governance \u2014 Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.</li> <li>model interpretability \u2014 Ability to explain how a model arrives at its predictions in ways stakeholders understand.</li> <li>precision \u2014 Share of predicted positives that are actually correct for a given classifier.</li> <li>preference dataset \u2014 Labeled comparisons of model outputs that capture which responses humans prefer.</li> <li>privacy \u2014 Principle of limiting data collection, use, and exposure to protect individuals\u2019 information.</li> <li>privacy budget \u2014 Quantitative limit on how much privacy loss is allowed when applying differential privacy.</li> <li>privacy impact assessment \u2014 Structured review that evaluates how a system collects, uses, and safeguards personal data.</li> <li>recall \u2014 Share of actual positives a model successfully identifies.</li> <li>red teaming \u2014 Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.</li> <li>reinforcement learning from human feedback \u2014 Training approach that tunes a model using reward signals learned from human preference data.</li> <li>responsible ai \u2014 Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.</li> <li>retrieval \u2014 Process of selecting relevant documents or vectors from a corpus in response to a query.</li> <li>reward model \u2014 Model trained on human preferences that scores AI responses for alignment or quality.</li> <li>risk register \u2014 Central list of identified AI risks, their owners, mitigations, and review status.</li> <li>safety classifier \u2014 Model that detects policy-violating or risky content before or after generation.</li> <li>safety evaluation \u2014 Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.</li> <li>safety review board \u2014 Cross-functional committee that approves high-risk AI launches and monitors mitigations.</li> <li>safety spec \u2014 Document that codifies allowed, disallowed, and escalated behaviours for an AI system so teams can enforce safety and policy expectations.</li> <li>self-critique loop \u2014 Pattern where a model reviews its own outputs, critiques them, and produces revisions before responding.</li> <li>shadow deployment \u2014 Deploying an AI system alongside the existing workflow without user impact to collect telemetry.</li> <li>synthetic data \u2014 Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.</li> <li>synthetic data evaluation \u2014 Process for measuring fidelity, utility, privacy, and bias of synthetic datasets before use.</li> <li>transparency report \u2014 Periodic disclosure that details how an AI system operates, what data it handles, and how risks are mitigated.</li> <li>voice cloning \u2014 Technique that replicates a person\u2019s voice using generative models trained on audio samples.</li> </ul>"},{"location":"roles/#legal-compliance","title":"Legal &amp; Compliance","text":"<p>Evaluate regulatory exposure, contracts, and IP concerns.</p> <p>Action plan - Bookmark the Glossary Search filtered to this role and review the top 5 unfamiliar terms. - Schedule a sync with partner roles listed under each term to clarify ownership and open questions. - Capture insights in your runbook or onboarding guide so future teammates ramp faster.</p>"},{"location":"roles/#guided-learning-path_4","title":"Guided learning path","text":"<ol> <li>Focus on Responsible AI and compliance-related terms to spot regulatory hooks.</li> <li>Cross-reference privacy-focused entries with current policy language.</li> <li>Capture open questions for the next risk or contract review cycle.</li> </ol>"},{"location":"roles/#practice-checklist_4","title":"Practice checklist","text":"<ul> <li>Compare contractual language with glossary definitions for privacy and retention to spot gaps.</li> <li>Flag terms needing legal guidance through the intake form so questions are tracked.</li> </ul>"},{"location":"roles/#focus-areas_4","title":"Focus areas","text":"<ul> <li>Governance &amp; Risk (31 terms)</li> <li>Operations &amp; Monitoring (8 terms)</li> <li>Foundations (1 term)</li> <li>LLM Core (1 term)</li> </ul>"},{"location":"roles/#recommended-terms_4","title":"Recommended terms","text":"<ul> <li>ai assurance \u2014 Discipline that produces evidence, controls, and attestations showing an AI system meets agreed safety, compliance, and performance thresholds.</li> <li>ai incident response \u2014 Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.</li> <li>algorithmic audit \u2014 Independent review of an AI system\u2019s data, design, and outcomes to verify compliance, fairness, and risk controls.</li> <li>algorithmic bias \u2014 Systematic unfairness in model outputs that disadvantages certain groups or outcomes.</li> <li>algorithmic impact assessment \u2014 Structured review that documents how an AI system may affect people, processes, and compliance obligations.</li> <li>alignment \u2014 Making sure AI systems optimize for human values, policies, and intended outcomes.</li> <li>assurance case \u2014 Structured argument that proves an AI system meets safety and compliance expectations.</li> <li>consent management \u2014 Practices that capture, honor, and audit user permissions across AI features.</li> <li>data lineage \u2014 Traceable record of how data moves, transforms, and is used across AI systems.</li> <li>data minimization \u2014 Principle of collecting and retaining only the data necessary for a defined purpose.</li> <li>data redaction \u2014 Removal or masking of sensitive fields before data is stored, shared, or used for model training.</li> <li>data retention \u2014 Policies defining how long data is stored, where it lives, and how it is deleted.</li> <li>dataset card \u2014 Structured documentation describing a dataset\u2019s purpose, composition, risks, and usage constraints.</li> <li>differential privacy \u2014 Mathematical framework that limits how much any single record influences published data or model outputs.</li> <li>evaluation \u2014 Systematic measurement of model performance, safety, and reliability using defined tests.</li> <li>fairness metrics \u2014 Quantitative measures that evaluate whether model performance is equitable across groups.</li> <li>guardrails \u2014 Controls that constrain model behavior to comply with safety, legal, or brand requirements.</li> <li>hallucination \u2014 When an AI model presents fabricated or unsupported information as fact.</li> <li>impact mitigation plan \u2014 Action plan that tracks risks, mitigations, owners, and timelines for an AI deployment.</li> <li>model card \u2014 Standardized documentation describing a model\u2019s purpose, data, performance, and limitations.</li> <li>model drift \u2014 Gradual mismatch between model assumptions and real-world data that degrades performance over time.</li> <li>model governance \u2014 Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.</li> <li>model interpretability \u2014 Ability to explain how a model arrives at its predictions in ways stakeholders understand.</li> <li>privacy \u2014 Principle of limiting data collection, use, and exposure to protect individuals\u2019 information.</li> <li>privacy budget \u2014 Quantitative limit on how much privacy loss is allowed when applying differential privacy.</li> <li>privacy impact assessment \u2014 Structured review that evaluates how a system collects, uses, and safeguards personal data.</li> <li>red teaming \u2014 Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.</li> <li>responsible ai \u2014 Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.</li> <li>risk register \u2014 Central list of identified AI risks, their owners, mitigations, and review status.</li> <li>safety review board \u2014 Cross-functional committee that approves high-risk AI launches and monitors mitigations.</li> <li>transparency report \u2014 Periodic disclosure that details how an AI system operates, what data it handles, and how risks are mitigated.</li> <li>voice cloning \u2014 Technique that replicates a person\u2019s voice using generative models trained on audio samples.</li> </ul>"},{"location":"roles/#security-trust","title":"Security &amp; Trust","text":"<p>Safeguard data, access, and abuse prevention.</p> <p>Action plan - Bookmark the Glossary Search filtered to this role and review the top 5 unfamiliar terms. - Schedule a sync with partner roles listed under each term to clarify ownership and open questions. - Capture insights in your runbook or onboarding guide so future teammates ramp faster.</p>"},{"location":"roles/#guided-learning-path_5","title":"Guided learning path","text":"<ol> <li>Study Operations &amp; Monitoring entries for logging and detection requirements.</li> <li>Review tool and agent terminology to assess abuse surface areas.</li> <li>Coordinate with product/legal on incident response and disclosure expectations.</li> </ol>"},{"location":"roles/#practice-checklist_5","title":"Practice checklist","text":"<ul> <li>Audit incident response and tool-use entries to confirm abuse-prevention controls are documented.</li> <li>Plan a tabletop exercise using the glossary's scenario examples and log outcomes.</li> </ul>"},{"location":"roles/#focus-areas_5","title":"Focus areas","text":"<ul> <li>Governance &amp; Risk (21 terms)</li> <li>Operations &amp; Monitoring (11 terms)</li> <li>LLM Core (2 terms)</li> <li>Foundations (1 term)</li> <li>Retrieval &amp; RAG (1 term)</li> </ul>"},{"location":"roles/#recommended-terms_5","title":"Recommended terms","text":"<ul> <li>ai assurance \u2014 Discipline that produces evidence, controls, and attestations showing an AI system meets agreed safety, compliance, and performance thresholds.</li> <li>ai circuit breaker \u2014 Automated control that halts model responses or tool access when risk thresholds are exceeded.</li> <li>ai incident response \u2014 Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.</li> <li>algorithmic bias \u2014 Systematic unfairness in model outputs that disadvantages certain groups or outcomes.</li> <li>content moderation \u2014 Workflows and tools that review, filter, and act on user-generated content to enforce policy.</li> <li>data minimization \u2014 Principle of collecting and retaining only the data necessary for a defined purpose.</li> <li>data redaction \u2014 Removal or masking of sensitive fields before data is stored, shared, or used for model training.</li> <li>data retention \u2014 Policies defining how long data is stored, where it lives, and how it is deleted.</li> <li>differential privacy \u2014 Mathematical framework that limits how much any single record influences published data or model outputs.</li> <li>escalation policy \u2014 Playbook that defines when and how AI systems route control to human reviewers.</li> <li>evaluation \u2014 Systematic measurement of model performance, safety, and reliability using defined tests.</li> <li>guardrail policy \u2014 Documented rules and prompts that define allowed, blocked, and escalated behaviors for AI systems.</li> <li>incident taxonomy \u2014 Standardized categories used to tag, analyze, and report AI incidents consistently.</li> <li>jailbreak prompt \u2014 Crafted input that persuades a model to ignore safety instructions and produce disallowed responses.</li> <li>ml observability \u2014 Practices and tooling that surface model health through metrics, traces, and alerts across the lifecycle.</li> <li>ml ops \u2014 Operational discipline that manages ML models from experimentation through deployment and monitoring.</li> <li>model drift \u2014 Gradual mismatch between model assumptions and real-world data that degrades performance over time.</li> <li>privacy impact assessment \u2014 Structured review that evaluates how a system collects, uses, and safeguards personal data.</li> <li>prompt injection \u2014 Attack that inserts malicious instructions into model inputs to override original prompts or policies.</li> <li>red teaming \u2014 Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.</li> <li>retrieval \u2014 Process of selecting relevant documents or vectors from a corpus in response to a query.</li> <li>robust prompting \u2014 Prompt design techniques that harden models against injections, ambiguity, and unsafe outputs.</li> <li>safety classifier \u2014 Model that detects policy-violating or risky content before or after generation.</li> <li>safety review board \u2014 Cross-functional committee that approves high-risk AI launches and monitors mitigations.</li> <li>safety spec \u2014 Document that codifies allowed, disallowed, and escalated behaviours for an AI system so teams can enforce safety and policy expectations.</li> <li>shadow deployment \u2014 Deploying an AI system alongside the existing workflow without user impact to collect telemetry.</li> <li>synthetic data \u2014 Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.</li> <li>voice cloning \u2014 Technique that replicates a person\u2019s voice using generative models trained on audio samples.</li> </ul>"},{"location":"roles/#communications-enablement","title":"Communications &amp; Enablement","text":"<p>Craft messaging, disclosure, and stakeholder education.</p> <p>Action plan - Bookmark the Glossary Search filtered to this role and review the top 5 unfamiliar terms. - Schedule a sync with partner roles listed under each term to clarify ownership and open questions. - Capture insights in your runbook or onboarding guide so future teammates ramp faster.</p>"},{"location":"roles/#guided-learning-path_6","title":"Guided learning path","text":"<ol> <li>Scan definitions tagged for Governance &amp; Risk to prep stakeholder messaging.</li> <li>Collect relatable examples from the glossary to use in enablement materials.</li> <li>Draft a narrative that links technical terms to user-facing value and risk mitigations.</li> </ol>"},{"location":"roles/#practice-checklist_6","title":"Practice checklist","text":"<ul> <li>Draft an FAQ using glossary language to keep messaging consistent across teams.</li> <li>Tag enablement tickets with relevant glossary links so stakeholders can self-serve context.</li> </ul>"},{"location":"roles/#focus-areas_6","title":"Focus areas","text":"<ul> <li>Governance &amp; Risk (19 terms)</li> <li>Operations &amp; Monitoring (7 terms)</li> <li>Foundations (4 terms)</li> <li>LLM Core (3 terms)</li> <li>Retrieval &amp; RAG (2 terms)</li> <li>Agents &amp; Tooling (1 term)</li> </ul>"},{"location":"roles/#recommended-terms_6","title":"Recommended terms","text":"<ul> <li>ai incident response \u2014 Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.</li> <li>algorithmic audit \u2014 Independent review of an AI system\u2019s data, design, and outcomes to verify compliance, fairness, and risk controls.</li> <li>algorithmic bias \u2014 Systematic unfairness in model outputs that disadvantages certain groups or outcomes.</li> <li>algorithmic impact assessment \u2014 Structured review that documents how an AI system may affect people, processes, and compliance obligations.</li> <li>alignment \u2014 Making sure AI systems optimize for human values, policies, and intended outcomes.</li> <li>clip \u2014 Multimodal model that embeds images and text into a shared space using contrastive learning.</li> <li>content moderation \u2014 Workflows and tools that review, filter, and act on user-generated content to enforce policy.</li> <li>diffusion model \u2014 Generative model that iteratively denoises random noise to synthesize images, audio, or other data.</li> <li>evaluation \u2014 Systematic measurement of model performance, safety, and reliability using defined tests.</li> <li>generative ai \u2014 Family of models that produce new content\u2014text, images, code\u2014rather than only making predictions.</li> <li>guardrails \u2014 Controls that constrain model behavior to comply with safety, legal, or brand requirements.</li> <li>hallucination \u2014 When an AI model presents fabricated or unsupported information as fact.</li> <li>human handoff \u2014 Moment when an AI workflow transfers control to a human for review or action.</li> <li>model card \u2014 Standardized documentation describing a model\u2019s purpose, data, performance, and limitations.</li> <li>model drift \u2014 Gradual mismatch between model assumptions and real-world data that degrades performance over time.</li> <li>model governance \u2014 Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.</li> <li>privacy \u2014 Principle of limiting data collection, use, and exposure to protect individuals\u2019 information.</li> <li>red teaming \u2014 Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.</li> <li>responsible ai \u2014 Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.</li> <li>retrieval \u2014 Process of selecting relevant documents or vectors from a corpus in response to a query.</li> <li>safety evaluation \u2014 Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.</li> <li>safety spec \u2014 Document that codifies allowed, disallowed, and escalated behaviours for an AI system so teams can enforce safety and policy expectations.</li> <li>transparency report \u2014 Periodic disclosure that details how an AI system operates, what data it handles, and how risks are mitigated.</li> <li>voice cloning \u2014 Technique that replicates a person\u2019s voice using generative models trained on audio samples.</li> </ul>"},{"location":"search/","title":"Glossary Search","text":"<p>Use the interactive search below to scan terms, aliases, categories, roles, and statuses. Layer filters to narrow the glossary down to the exact concepts your team needs.</p> Quick filters Product Engineering Policy &amp; Risk Legal &amp; Compliance Governance &amp; Risk LLM Core Operations &amp; Monitoring Draft Reviewed Approved Clear filters <p>Tip: combine a role with a category to see the vocabulary most relevant to your current project.</p> Search Category All categories Status All statuses Role All roles Sort by Last reviewed (newest) A \u2192 Z Category Showing glossary results. <p>Tip: copy the page URL after selecting filters to share a saved view with your team.</p>"},{"location":"term-request/","title":"Term Request Intake","text":"<p>Help us capture confusing AI vocabulary quickly without overwhelming employees.</p>"},{"location":"term-request/#how-to-submit-a-request","title":"How to submit a request","text":"<ol> <li>Open the short form at <code>[Replace with internal URL]</code> from any device.</li> <li>Spend two minutes filling in the fields below\u2014plain language is perfect.</li> <li>Expect a response within five business days; we will confirm whether the term joins the backlog or if we need more context.</li> </ol> <p>Tip: if you prefer email or chat, copy the question set into your message so everyone shares the same details.</p>"},{"location":"term-request/#form-questions-copypaste-into-your-tool-of-choice","title":"Form questions (copy/paste into your tool of choice)","text":"<ul> <li>Your name and team \u2013 we reach out if we need clarifications.</li> <li>Term or phrase \u2013 what needs defining? Include common aliases if you know them.</li> <li>Where you saw it \u2013 meeting, customer call, policy doc, etc.</li> <li>Why it matters \u2013 describe the task you were trying to finish when the term caused friction.</li> <li>Audience impact \u2013 who else might struggle (engineers, sales, legal, executives)?</li> <li>Urgency \u2013 now, this quarter, or evergreen? Helps prioritize the backlog.</li> <li>Helpful links \u2013 optional articles, specs, or policy notes to review.</li> </ul> <p>Keep the form mobile-friendly: short labels, sentence-case instructions, and confirmation copy that thanks the requester for their help.</p>"},{"location":"term-request/#what-happens-next","title":"What happens next","text":"<ul> <li>Requests feed into the keyword backlog during the weekly triage review.</li> <li>We tag each item with a cluster and owner, then update the status column so submitters can check progress.</li> <li>When a term ships, we notify the requester and link to the published page.</li> </ul>"},{"location":"term-request/#accessibility-and-ux-considerations","title":"Accessibility and UX considerations","text":"<ul> <li>Use large touch targets and 16px+ text if you build the form on the intranet.</li> <li>Offer alternative submission paths (Slack message, email) for colleagues using assistive tech.</li> <li>Provide a confirmation message that restates the submitted term and next steps in plain language.</li> <li>Store responses in a shared folder so contributors can spot patterns without re-asking employees.</li> </ul> <p>A lightweight form plus transparent follow-up keeps the glossary grounded in real employee needs while preventing information overload.</p>"},{"location":"includes/related/agent-executor/","title":"Agent executor","text":"<p>Related terms</p> <ul> <li>agentic ai</li> <li>tool use</li> <li>memory strategy</li> <li>escalation policy</li> <li>evaluation harness</li> <li>self-critique loop</li> <li>human handoff</li> <li>function calling</li> </ul>"},{"location":"includes/related/agentic-ai/","title":"Agentic ai","text":"<p>Related terms</p> <ul> <li>agent executor</li> <li>responsible ai</li> <li>model governance</li> <li>memory strategy</li> <li>ai incident response</li> <li>ai assurance</li> <li>escalation policy</li> <li>safety review board</li> </ul>"},{"location":"includes/related/ai-assurance/","title":"Ai assurance","text":"<p>Related terms</p> <ul> <li>model governance</li> <li>algorithmic audit</li> <li>responsible ai</li> <li>algorithmic impact assessment</li> <li>assurance case</li> <li>safety review board</li> <li>evaluation</li> <li>safety evaluation</li> </ul>"},{"location":"includes/related/ai-circuit-breaker/","title":"Ai circuit breaker","text":"<p>Related terms</p> <ul> <li>agentic ai</li> <li>escalation policy</li> <li>content moderation</li> <li>guardrail policy</li> <li>jailbreak prompt</li> <li>safety classifier</li> <li>ai incident response</li> <li>guardrails</li> </ul>"},{"location":"includes/related/ai-incident-response/","title":"Ai incident response","text":"<p>Related terms</p> <ul> <li>red teaming</li> <li>safety evaluation</li> <li>safety review board</li> <li>impact mitigation plan</li> <li>incident taxonomy</li> <li>escalation policy</li> <li>safety spec</li> <li>model governance</li> </ul>"},{"location":"includes/related/algorithmic-audit/","title":"Algorithmic audit","text":"<p>Related terms</p> <ul> <li>ai assurance</li> <li>algorithmic impact assessment</li> <li>evaluation</li> <li>model governance</li> <li>responsible ai</li> <li>transparency report</li> <li>safety evaluation</li> <li>ai incident response</li> </ul>"},{"location":"includes/related/algorithmic-bias/","title":"Algorithmic bias","text":"<p>Related terms</p> <ul> <li>algorithmic impact assessment</li> <li>model governance</li> <li>bias-variance tradeoff</li> <li>algorithmic audit</li> <li>ai assurance</li> <li>alignment</li> <li>evaluation</li> <li>fairness metrics</li> </ul>"},{"location":"includes/related/algorithmic-impact-assessment/","title":"Algorithmic impact assessment","text":"<p>Related terms</p> <ul> <li>impact mitigation plan</li> <li>model governance</li> <li>ai assurance</li> <li>responsible ai</li> <li>algorithmic audit</li> <li>algorithmic bias</li> <li>evaluation</li> <li>privacy impact assessment</li> </ul>"},{"location":"includes/related/alignment/","title":"Alignment","text":"<p>Related terms</p> <ul> <li>constitutional ai</li> <li>algorithmic bias</li> <li>model governance</li> <li>responsible ai</li> <li>ai assurance</li> <li>fine-tuning</li> <li>evaluation</li> <li>algorithmic impact assessment</li> </ul>"},{"location":"includes/related/assurance-case/","title":"Assurance case","text":"<p>Related terms</p> <ul> <li>ai assurance</li> <li>safety review board</li> <li>safety spec</li> <li>model governance</li> <li>algorithmic audit</li> <li>safety evaluation</li> <li>responsible ai</li> <li>risk register</li> </ul>"},{"location":"includes/related/attention/","title":"Attention","text":"<p>Related terms</p> <ul> <li>context window</li> <li>kv cache</li> <li>generative ai</li> <li>token</li> <li>decoding</li> <li>repetition penalty</li> <li>retrieval-augmented generation</li> <li>instruction tuning</li> </ul>"},{"location":"includes/related/beam-search/","title":"Beam search","text":"<p>Related terms</p> <ul> <li>decoding</li> <li>greedy decoding</li> <li>reranking</li> <li>top-k sampling</li> <li>repetition penalty</li> <li>log probability</li> <li>fine-tuning</li> <li>attention</li> </ul>"},{"location":"includes/related/bias-variance-tradeoff/","title":"Bias variance tradeoff","text":"<p>Related terms</p> <ul> <li>algorithmic bias</li> <li>overfitting</li> <li>cross-validation</li> <li>regularization</li> <li>generalization</li> <li>fine-tuning</li> <li>validation set</li> <li>model drift</li> </ul>"},{"location":"includes/related/chain-of-thought-prompting/","title":"Chain of thought prompting","text":"<p>Related terms</p> <ul> <li>prompt engineering</li> <li>system prompt</li> <li>function calling</li> <li>self-critique loop</li> <li>robust prompting</li> <li>hallucination</li> <li>self-consistency decoding</li> <li>agentic ai</li> </ul>"},{"location":"includes/related/chunking/","title":"Chunking","text":"<p>Related terms</p> <ul> <li>retrieval</li> <li>retrieval-augmented generation</li> <li>embedding</li> <li>context window</li> <li>token</li> <li>clip</li> <li>reranking</li> <li>vector store</li> </ul>"},{"location":"includes/related/clip/","title":"Clip","text":"<p>Related terms</p> <ul> <li>embedding</li> <li>chunking</li> <li>generative ai</li> <li>retrieval-augmented generation</li> <li>attention</li> <li>retrieval</li> <li>fine-tuning</li> <li>preference dataset</li> </ul>"},{"location":"includes/related/confusion-matrix/","title":"Confusion matrix","text":"<p>Related terms</p> <ul> <li>recall</li> <li>precision</li> <li>roc auc</li> <li>f1 score</li> <li>cross-validation</li> <li>model drift</li> <li>target variable</li> <li>loss function</li> </ul>"},{"location":"includes/related/consent-management/","title":"Consent management","text":"<p>Related terms</p> <ul> <li>privacy</li> <li>responsible ai</li> <li>ai assurance</li> <li>privacy impact assessment</li> <li>data retention</li> <li>data minimization</li> <li>model governance</li> <li>content moderation</li> </ul>"},{"location":"includes/related/constitutional-ai/","title":"Constitutional ai","text":"<p>Related terms</p> <ul> <li>alignment</li> <li>fine-tuning</li> <li>safety spec</li> <li>agentic ai</li> <li>instruction tuning</li> <li>system prompt</li> <li>red teaming</li> <li>self-critique loop</li> </ul>"},{"location":"includes/related/content-moderation/","title":"Content moderation","text":"<p>Related terms</p> <ul> <li>safety classifier</li> <li>guardrails</li> <li>escalation policy</li> <li>safety evaluation</li> <li>ai incident response</li> <li>guardrail policy</li> <li>red teaming</li> <li>safety spec</li> </ul>"},{"location":"includes/related/context-window/","title":"Context window","text":"<p>Related terms</p> <ul> <li>token</li> <li>repetition penalty</li> <li>attention</li> <li>memory strategy</li> <li>chunking</li> <li>decoding</li> <li>prompt engineering</li> <li>kv cache</li> </ul>"},{"location":"includes/related/cross-validation/","title":"Cross validation","text":"<p>Related terms</p> <ul> <li>overfitting</li> <li>validation set</li> <li>bias-variance tradeoff</li> <li>test set</li> <li>fine-tuning</li> <li>generalization</li> <li>training data</li> <li>model drift</li> </ul>"},{"location":"includes/related/data-lineage/","title":"Data lineage","text":"<p>Related terms</p> <ul> <li>ml observability</li> <li>data retention</li> <li>synthetic data</li> <li>ml ops</li> <li>feature engineering</li> <li>dataset card</li> <li>model drift</li> <li>model governance</li> </ul>"},{"location":"includes/related/data-minimization/","title":"Data minimization","text":"<p>Related terms</p> <ul> <li>privacy</li> <li>data retention</li> <li>data redaction</li> <li>privacy impact assessment</li> <li>synthetic data</li> <li>privacy budget</li> <li>consent management</li> <li>algorithmic audit</li> </ul>"},{"location":"includes/related/data-redaction/","title":"Data redaction","text":"<p>Related terms</p> <ul> <li>data minimization</li> <li>privacy</li> <li>synthetic data</li> <li>privacy impact assessment</li> <li>data retention</li> <li>transparency report</li> <li>dataset card</li> <li>differential privacy</li> </ul>"},{"location":"includes/related/data-retention/","title":"Data retention","text":"<p>Related terms</p> <ul> <li>data minimization</li> <li>privacy</li> <li>consent management</li> <li>data lineage</li> <li>memory strategy</li> <li>privacy impact assessment</li> <li>data redaction</li> <li>dataset card</li> </ul>"},{"location":"includes/related/dataset-card/","title":"Dataset card","text":"<p>Related terms</p> <ul> <li>model card</li> <li>synthetic data</li> <li>data minimization</li> <li>training data</li> <li>synthetic data evaluation</li> <li>data retention</li> <li>privacy</li> <li>risk register</li> </ul>"},{"location":"includes/related/decoding/","title":"Decoding","text":"<p>Related terms</p> <ul> <li>greedy decoding</li> <li>log probability</li> <li>top-p sampling</li> <li>temperature</li> <li>repetition penalty</li> <li>top-k sampling</li> <li>beam search</li> <li>generative ai</li> </ul>"},{"location":"includes/related/differential-privacy/","title":"Differential privacy","text":"<p>Related terms</p> <ul> <li>privacy</li> <li>privacy budget</li> <li>synthetic data</li> <li>privacy impact assessment</li> <li>data minimization</li> <li>synthetic data evaluation</li> <li>consent management</li> <li>data redaction</li> </ul>"},{"location":"includes/related/diffusion-model/","title":"Diffusion model","text":"<p>Related terms</p> <ul> <li>generative ai</li> <li>fine-tuning</li> <li>decoding</li> <li>knowledge distillation</li> <li>voice cloning</li> <li>model drift</li> <li>hallucination</li> <li>top-k sampling</li> </ul>"},{"location":"includes/related/direct-preference-optimization/","title":"Direct preference optimization","text":"<p>Related terms</p> <ul> <li>preference dataset</li> <li>reinforcement learning from human feedback</li> <li>reward model</li> <li>fine-tuning</li> <li>instruction tuning</li> <li>top-k sampling</li> <li>training data</li> <li>target variable</li> </ul>"},{"location":"includes/related/embedding/","title":"Embedding","text":"<p>Related terms</p> <ul> <li>retrieval</li> <li>vector store</li> <li>retrieval-augmented generation</li> <li>chunking</li> <li>clip</li> <li>generative ai</li> <li>reranking</li> <li>token</li> </ul>"},{"location":"includes/related/escalation-policy/","title":"Escalation policy","text":"<p>Related terms</p> <ul> <li>content moderation</li> <li>ai incident response</li> <li>guardrail policy</li> <li>ai assurance</li> <li>impact mitigation plan</li> <li>agentic ai</li> <li>human handoff</li> <li>guardrails</li> </ul>"},{"location":"includes/related/evaluation-harness/","title":"Evaluation harness","text":"<p>Related terms</p> <ul> <li>evaluation</li> <li>ml ops</li> <li>ai assurance</li> <li>safety evaluation</li> <li>ml observability</li> <li>algorithmic audit</li> <li>red teaming</li> <li>agentic ai</li> </ul>"},{"location":"includes/related/evaluation/","title":"Evaluation","text":"<p>Related terms</p> <ul> <li>safety evaluation</li> <li>evaluation harness</li> <li>ai assurance</li> <li>algorithmic audit</li> <li>algorithmic impact assessment</li> <li>model governance</li> <li>red teaming</li> <li>ml observability</li> </ul>"},{"location":"includes/related/f1-score/","title":"F1 score","text":"<p>Related terms</p> <ul> <li>recall</li> <li>precision</li> <li>confusion matrix</li> <li>roc auc</li> <li>loss function</li> <li>target variable</li> <li>reranking</li> <li>fairness metrics</li> </ul>"},{"location":"includes/related/fairness-metrics/","title":"Fairness metrics","text":"<p>Related terms</p> <ul> <li>algorithmic bias</li> <li>validation set</li> <li>precision</li> <li>fine-tuning</li> <li>evaluation</li> <li>recall</li> <li>safety evaluation</li> <li>impact mitigation plan</li> </ul>"},{"location":"includes/related/feature-engineering/","title":"Feature engineering","text":"<p>Related terms</p> <ul> <li>data lineage</li> <li>decoding</li> <li>model interpretability</li> <li>model card</li> <li>tool use</li> <li>ml ops</li> <li>validation set</li> <li>dataset card</li> </ul>"},{"location":"includes/related/fine-tuning/","title":"Fine tuning","text":"<p>Related terms</p> <ul> <li>instruction tuning</li> <li>validation set</li> <li>overfitting</li> <li>model drift</li> <li>regularization</li> <li>training data</li> <li>generalization</li> <li>evaluation</li> </ul>"},{"location":"includes/related/function-calling/","title":"Function calling","text":"<p>Related terms</p> <ul> <li>tool use</li> <li>prompt engineering</li> <li>robust prompting</li> <li>system prompt</li> <li>chain-of-thought prompting</li> <li>agentic ai</li> <li>prompt injection</li> <li>decoding</li> </ul>"},{"location":"includes/related/generalization/","title":"Generalization","text":"<p>Related terms</p> <ul> <li>regularization</li> <li>overfitting</li> <li>fine-tuning</li> <li>training data</li> <li>bias-variance tradeoff</li> <li>cross-validation</li> <li>ml ops</li> <li>model drift</li> </ul>"},{"location":"includes/related/generative-ai/","title":"Generative ai","text":"<p>Related terms</p> <ul> <li>hallucination</li> <li>decoding</li> <li>model governance</li> <li>diffusion model</li> <li>synthetic data</li> <li>constitutional ai</li> <li>embedding</li> <li>red teaming</li> </ul>"},{"location":"includes/related/gradient-descent/","title":"Gradient descent","text":"<p>Related terms</p> <ul> <li>regularization</li> <li>overfitting</li> <li>fine-tuning</li> <li>training data</li> <li>loss function</li> <li>quantization</li> <li>instruction tuning</li> <li>generalization</li> </ul>"},{"location":"includes/related/greedy-decoding/","title":"Greedy decoding","text":"<p>Related terms</p> <ul> <li>decoding</li> <li>repetition penalty</li> <li>top-k sampling</li> <li>beam search</li> <li>top-p sampling</li> <li>temperature</li> <li>fine-tuning</li> <li>system prompt</li> </ul>"},{"location":"includes/related/guardrail-policy/","title":"Guardrail policy","text":"<p>Related terms</p> <ul> <li>guardrails</li> <li>safety spec</li> <li>safety evaluation</li> <li>escalation policy</li> <li>ai assurance</li> <li>safety classifier</li> <li>content moderation</li> <li>safety review board</li> </ul>"},{"location":"includes/related/guardrails/","title":"Guardrails","text":"<p>Related terms</p> <ul> <li>guardrail policy</li> <li>safety evaluation</li> <li>safety spec</li> <li>safety classifier</li> <li>content moderation</li> <li>ai assurance</li> <li>ai incident response</li> <li>model governance</li> </ul>"},{"location":"includes/related/hallucination/","title":"Hallucination","text":"<p>Related terms</p> <ul> <li>generative ai</li> <li>decoding</li> <li>repetition penalty</li> <li>constitutional ai</li> <li>top-p sampling</li> <li>chain-of-thought prompting</li> <li>precision</li> <li>overfitting</li> </ul>"},{"location":"includes/related/human-handoff/","title":"Human handoff","text":"<p>Related terms</p> <ul> <li>escalation policy</li> <li>agentic ai</li> <li>agent executor</li> <li>tool use</li> <li>memory strategy</li> <li>shadow deployment</li> <li>evaluation harness</li> <li>self-critique loop</li> </ul>"},{"location":"includes/related/impact-mitigation-plan/","title":"Impact mitigation plan","text":"<p>Related terms</p> <ul> <li>algorithmic impact assessment</li> <li>ai incident response</li> <li>ai assurance</li> <li>red teaming</li> <li>model governance</li> <li>risk register</li> <li>responsible ai</li> <li>safety evaluation</li> </ul>"},{"location":"includes/related/incident-taxonomy/","title":"Incident taxonomy","text":"<p>Related terms</p> <ul> <li>ai incident response</li> <li>safety classifier</li> <li>safety evaluation</li> <li>safety spec</li> <li>content moderation</li> <li>risk register</li> <li>algorithmic impact assessment</li> <li>impact mitigation plan</li> </ul>"},{"location":"includes/related/instruction-tuning/","title":"Instruction tuning","text":"<p>Related terms</p> <ul> <li>fine-tuning</li> <li>reinforcement learning from human feedback</li> <li>constitutional ai</li> <li>preference dataset</li> <li>training data</li> <li>prompt engineering</li> <li>validation set</li> <li>reward model</li> </ul>"},{"location":"includes/related/jailbreak-prompt/","title":"Jailbreak prompt","text":"<p>Related terms</p> <ul> <li>prompt injection</li> <li>robust prompting</li> <li>guardrail policy</li> <li>red teaming</li> <li>system prompt</li> <li>safety classifier</li> <li>guardrails</li> <li>prompt engineering</li> </ul>"},{"location":"includes/related/knowledge-distillation/","title":"Knowledge distillation","text":"<p>Related terms</p> <ul> <li>fine-tuning</li> <li>diffusion model</li> <li>overfitting</li> <li>training data</li> <li>synthetic data</li> <li>instruction tuning</li> <li>synthetic data evaluation</li> <li>low-rank adaptation</li> </ul>"},{"location":"includes/related/kv-cache/","title":"Kv cache","text":"<p>Related terms</p> <ul> <li>attention</li> <li>context window</li> <li>vector store</li> <li>repetition penalty</li> <li>low-rank adaptation</li> <li>memory strategy</li> <li>top-k sampling</li> <li>temperature</li> </ul>"},{"location":"includes/related/log-probability/","title":"Log probability","text":"<p>Related terms</p> <ul> <li>decoding</li> <li>top-p sampling</li> <li>repetition penalty</li> <li>ml observability</li> <li>temperature</li> <li>safety classifier</li> <li>precision</li> <li>prompt engineering</li> </ul>"},{"location":"includes/related/loss-function/","title":"Loss function","text":"<p>Related terms</p> <ul> <li>target variable</li> <li>regularization</li> <li>algorithmic bias</li> <li>generalization</li> <li>gradient descent</li> <li>roc auc</li> <li>precision</li> <li>fairness metrics</li> </ul>"},{"location":"includes/related/low-rank-adaptation/","title":"Low rank adaptation","text":"<p>Related terms</p> <ul> <li>fine-tuning</li> <li>reranking</li> <li>kv cache</li> <li>top-k sampling</li> <li>mixture of experts</li> <li>regularization</li> <li>vector store</li> <li>retrieval-augmented generation</li> </ul>"},{"location":"includes/related/memory-strategy/","title":"Memory strategy","text":"<p>Related terms</p> <ul> <li>agentic ai</li> <li>data retention</li> <li>agent executor</li> <li>context window</li> <li>guardrail policy</li> <li>red teaming</li> <li>ai incident response</li> <li>consent management</li> </ul>"},{"location":"includes/related/mixture-of-experts/","title":"Mixture of experts","text":"<p>Related terms</p> <ul> <li>fine-tuning</li> <li>decoding</li> <li>tool use</li> <li>generative ai</li> <li>ml ops</li> <li>evaluation harness</li> <li>model governance</li> <li>ml observability</li> </ul>"},{"location":"includes/related/ml-observability/","title":"Ml observability","text":"<p>Related terms</p> <ul> <li>ml ops</li> <li>evaluation</li> <li>model governance</li> <li>algorithmic audit</li> <li>ai assurance</li> <li>evaluation harness</li> <li>algorithmic impact assessment</li> <li>log probability</li> </ul>"},{"location":"includes/related/ml-ops/","title":"Ml ops","text":"<p>Related terms</p> <ul> <li>model governance</li> <li>ml observability</li> <li>ai incident response</li> <li>evaluation</li> <li>evaluation harness</li> <li>algorithmic impact assessment</li> <li>red teaming</li> <li>ai assurance</li> </ul>"},{"location":"includes/related/model-card/","title":"Model card","text":"<p>Related terms</p> <ul> <li>dataset card</li> <li>model governance</li> <li>safety spec</li> <li>ai assurance</li> <li>model drift</li> <li>model interpretability</li> <li>decoding</li> <li>tool use</li> </ul>"},{"location":"includes/related/model-drift/","title":"Model drift","text":"<p>Related terms</p> <ul> <li>fine-tuning</li> <li>ml ops</li> <li>model governance</li> <li>evaluation</li> <li>ai assurance</li> <li>ml observability</li> <li>ai incident response</li> <li>model card</li> </ul>"},{"location":"includes/related/model-governance/","title":"Model governance","text":"<p>Related terms</p> <ul> <li>responsible ai</li> <li>ai assurance</li> <li>algorithmic impact assessment</li> <li>ml ops</li> <li>safety review board</li> <li>algorithmic audit</li> <li>ai incident response</li> <li>algorithmic bias</li> </ul>"},{"location":"includes/related/model-interpretability/","title":"Model interpretability","text":"<p>Related terms</p> <ul> <li>model governance</li> <li>algorithmic audit</li> <li>evaluation</li> <li>algorithmic impact assessment</li> <li>ai assurance</li> <li>ml observability</li> <li>transparency report</li> <li>model card</li> </ul>"},{"location":"includes/related/overfitting/","title":"Overfitting","text":"<p>Related terms</p> <ul> <li>cross-validation</li> <li>regularization</li> <li>bias-variance tradeoff</li> <li>fine-tuning</li> <li>generalization</li> <li>training data</li> <li>validation set</li> <li>model drift</li> </ul>"},{"location":"includes/related/precision/","title":"Precision","text":"<p>Related terms</p> <ul> <li>recall</li> <li>confusion matrix</li> <li>roc auc</li> <li>safety classifier</li> <li>target variable</li> <li>f1 score</li> <li>log probability</li> <li>fairness metrics</li> </ul>"},{"location":"includes/related/preference-dataset/","title":"Preference dataset","text":"<p>Related terms</p> <ul> <li>direct preference optimization</li> <li>reward model</li> <li>reinforcement learning from human feedback</li> <li>instruction tuning</li> <li>temperature</li> <li>safety classifier</li> <li>fine-tuning</li> <li>system prompt</li> </ul>"},{"location":"includes/related/privacy-budget/","title":"Privacy budget","text":"<p>Related terms</p> <ul> <li>differential privacy</li> <li>privacy</li> <li>privacy impact assessment</li> <li>data minimization</li> <li>consent management</li> <li>data retention</li> <li>synthetic data</li> <li>data redaction</li> </ul>"},{"location":"includes/related/privacy-impact-assessment/","title":"Privacy impact assessment","text":"<p>Related terms</p> <ul> <li>privacy</li> <li>algorithmic impact assessment</li> <li>data minimization</li> <li>transparency report</li> <li>ai assurance</li> <li>data redaction</li> <li>consent management</li> <li>synthetic data</li> </ul>"},{"location":"includes/related/privacy/","title":"Privacy","text":"<p>Related terms</p> <ul> <li>privacy impact assessment</li> <li>data minimization</li> <li>differential privacy</li> <li>consent management</li> <li>synthetic data</li> <li>transparency report</li> <li>responsible ai</li> <li>privacy budget</li> </ul>"},{"location":"includes/related/prompt-engineering/","title":"Prompt engineering","text":"<p>Related terms</p> <ul> <li>system prompt</li> <li>robust prompting</li> <li>chain-of-thought prompting</li> <li>function calling</li> <li>prompt injection</li> <li>safety spec</li> <li>decoding</li> <li>tool use</li> </ul>"},{"location":"includes/related/prompt-injection/","title":"Prompt injection","text":"<p>Related terms</p> <ul> <li>jailbreak prompt</li> <li>robust prompting</li> <li>system prompt</li> <li>prompt engineering</li> <li>red teaming</li> <li>safety classifier</li> <li>function calling</li> <li>tool use</li> </ul>"},{"location":"includes/related/quantization/","title":"Quantization","text":"<p>Related terms</p> <ul> <li>gradient descent</li> <li>regularization</li> <li>knowledge distillation</li> <li>overfitting</li> <li>attention</li> <li>kv cache</li> <li>token</li> <li>fine-tuning</li> </ul>"},{"location":"includes/related/recall/","title":"Recall","text":"<p>Related terms</p> <ul> <li>precision</li> <li>f1 score</li> <li>confusion matrix</li> <li>roc auc</li> <li>target variable</li> <li>safety classifier</li> <li>fairness metrics</li> <li>top-p sampling</li> </ul>"},{"location":"includes/related/red-teaming/","title":"Red teaming","text":"<p>Related terms</p> <ul> <li>ai incident response</li> <li>safety evaluation</li> <li>ai assurance</li> <li>impact mitigation plan</li> <li>evaluation</li> <li>content moderation</li> <li>model governance</li> <li>responsible ai</li> </ul>"},{"location":"includes/related/regularization/","title":"Regularization","text":"<p>Related terms</p> <ul> <li>generalization</li> <li>overfitting</li> <li>fine-tuning</li> <li>bias-variance tradeoff</li> <li>training data</li> <li>gradient descent</li> <li>model drift</li> <li>loss function</li> </ul>"},{"location":"includes/related/reinforcement-learning-from-human-feedback/","title":"Reinforcement learning from human feedback","text":"<p>Related terms</p> <ul> <li>reward model</li> <li>preference dataset</li> <li>direct preference optimization</li> <li>instruction tuning</li> <li>fine-tuning</li> <li>constitutional ai</li> <li>safety classifier</li> <li>target variable</li> </ul>"},{"location":"includes/related/repetition-penalty/","title":"Repetition penalty","text":"<p>Related terms</p> <ul> <li>token</li> <li>decoding</li> <li>greedy decoding</li> <li>context window</li> <li>log probability</li> <li>robust prompting</li> <li>hallucination</li> <li>system prompt</li> </ul>"},{"location":"includes/related/reranking/","title":"Reranking","text":"<p>Related terms</p> <ul> <li>retrieval</li> <li>retrieval-augmented generation</li> <li>beam search</li> <li>embedding</li> <li>vector store</li> <li>low-rank adaptation</li> <li>chunking</li> <li>fine-tuning</li> </ul>"},{"location":"includes/related/responsible-ai/","title":"Responsible ai","text":"<p>Related terms</p> <ul> <li>model governance</li> <li>ai assurance</li> <li>safety review board</li> <li>algorithmic impact assessment</li> <li>algorithmic audit</li> <li>agentic ai</li> <li>ai incident response</li> <li>privacy</li> </ul>"},{"location":"includes/related/retrieval-augmented-generation/","title":"Retrieval augmented generation","text":"<p>Related terms</p> <ul> <li>retrieval</li> <li>chunking</li> <li>reranking</li> <li>embedding</li> <li>tool use</li> <li>generative ai</li> <li>context window</li> <li>clip</li> </ul>"},{"location":"includes/related/retrieval/","title":"Retrieval","text":"<p>Related terms</p> <ul> <li>retrieval-augmented generation</li> <li>reranking</li> <li>embedding</li> <li>chunking</li> <li>vector store</li> <li>data lineage</li> <li>tool use</li> <li>clip</li> </ul>"},{"location":"includes/related/reward-model/","title":"Reward model","text":"<p>Related terms</p> <ul> <li>reinforcement learning from human feedback</li> <li>preference dataset</li> <li>direct preference optimization</li> <li>constitutional ai</li> <li>fine-tuning</li> <li>instruction tuning</li> <li>target variable</li> <li>alignment</li> </ul>"},{"location":"includes/related/risk-register/","title":"Risk register","text":"<p>Related terms</p> <ul> <li>safety review board</li> <li>impact mitigation plan</li> <li>ai assurance</li> <li>algorithmic impact assessment</li> <li>model governance</li> <li>assurance case</li> <li>responsible ai</li> <li>safety spec</li> </ul>"},{"location":"includes/related/robust-prompting/","title":"Robust prompting","text":"<p>Related terms</p> <ul> <li>system prompt</li> <li>prompt injection</li> <li>jailbreak prompt</li> <li>prompt engineering</li> <li>safety spec</li> <li>function calling</li> <li>safety classifier</li> <li>chain-of-thought prompting</li> </ul>"},{"location":"includes/related/roc-auc/","title":"Roc auc","text":"<p>Related terms</p> <ul> <li>precision</li> <li>recall</li> <li>confusion matrix</li> <li>safety classifier</li> <li>log probability</li> <li>cross-validation</li> <li>algorithmic bias</li> <li>fairness metrics</li> </ul>"},{"location":"includes/related/safety-classifier/","title":"Safety classifier","text":"<p>Related terms</p> <ul> <li>safety evaluation</li> <li>content moderation</li> <li>safety spec</li> <li>guardrails</li> <li>guardrail policy</li> <li>jailbreak prompt</li> <li>ai incident response</li> <li>safety review board</li> </ul>"},{"location":"includes/related/safety-evaluation/","title":"Safety evaluation","text":"<p>Related terms</p> <ul> <li>safety spec</li> <li>evaluation</li> <li>safety review board</li> <li>safety classifier</li> <li>ai assurance</li> <li>guardrails</li> <li>ai incident response</li> <li>guardrail policy</li> </ul>"},{"location":"includes/related/safety-review-board/","title":"Safety review board","text":"<p>Related terms</p> <ul> <li>safety evaluation</li> <li>responsible ai</li> <li>ai assurance</li> <li>model governance</li> <li>ai incident response</li> <li>safety spec</li> <li>risk register</li> <li>algorithmic impact assessment</li> </ul>"},{"location":"includes/related/safety-spec/","title":"Safety spec","text":"<p>Related terms</p> <ul> <li>safety evaluation</li> <li>guardrail policy</li> <li>guardrails</li> <li>safety classifier</li> <li>safety review board</li> <li>ai incident response</li> <li>ai assurance</li> <li>content moderation</li> </ul>"},{"location":"includes/related/self-consistency-decoding/","title":"Self consistency decoding","text":"<p>Related terms</p> <ul> <li>chain-of-thought prompting</li> <li>self-critique loop</li> <li>hallucination</li> <li>retrieval-augmented generation</li> <li>repetition penalty</li> <li>constitutional ai</li> <li>decoding</li> <li>top-k sampling</li> </ul>"},{"location":"includes/related/self-critique-loop/","title":"Self critique loop","text":"<p>Related terms</p> <ul> <li>prompt engineering</li> <li>system prompt</li> <li>chain-of-thought prompting</li> <li>constitutional ai</li> <li>agent executor</li> <li>evaluation harness</li> <li>agentic ai</li> <li>evaluation</li> </ul>"},{"location":"includes/related/shadow-deployment/","title":"Shadow deployment","text":"<p>Related terms</p> <ul> <li>evaluation harness</li> <li>ml ops</li> <li>ml observability</li> <li>content moderation</li> <li>red teaming</li> <li>evaluation</li> <li>agentic ai</li> <li>function calling</li> </ul>"},{"location":"includes/related/synthetic-data-evaluation/","title":"Synthetic data evaluation","text":"<p>Related terms</p> <ul> <li>synthetic data</li> <li>validation set</li> <li>evaluation</li> <li>algorithmic bias</li> <li>privacy</li> <li>dataset card</li> <li>privacy impact assessment</li> <li>differential privacy</li> </ul>"},{"location":"includes/related/synthetic-data/","title":"Synthetic data","text":"<p>Related terms</p> <ul> <li>synthetic data evaluation</li> <li>privacy</li> <li>data redaction</li> <li>data minimization</li> <li>differential privacy</li> <li>privacy impact assessment</li> <li>dataset card</li> <li>generative ai</li> </ul>"},{"location":"includes/related/system-prompt/","title":"System prompt","text":"<p>Related terms</p> <ul> <li>prompt engineering</li> <li>robust prompting</li> <li>safety spec</li> <li>prompt injection</li> <li>jailbreak prompt</li> <li>content moderation</li> <li>tool use</li> <li>chain-of-thought prompting</li> </ul>"},{"location":"includes/related/target-variable/","title":"Target variable","text":"<p>Related terms</p> <ul> <li>loss function</li> <li>training data</li> <li>precision</li> <li>fine-tuning</li> <li>recall</li> <li>reward model</li> <li>preference dataset</li> <li>test set</li> </ul>"},{"location":"includes/related/temperature/","title":"Temperature","text":"<p>Related terms</p> <ul> <li>top-k sampling</li> <li>top-p sampling</li> <li>decoding</li> <li>log probability</li> <li>greedy decoding</li> <li>fine-tuning</li> <li>preference dataset</li> <li>system prompt</li> </ul>"},{"location":"includes/related/test-set/","title":"Test set","text":"<p>Related terms</p> <ul> <li>validation set</li> <li>training data</li> <li>cross-validation</li> <li>evaluation</li> <li>fine-tuning</li> <li>generalization</li> <li>overfitting</li> <li>synthetic data evaluation</li> </ul>"},{"location":"includes/related/token/","title":"Token","text":"<p>Related terms</p> <ul> <li>context window</li> <li>repetition penalty</li> <li>decoding</li> <li>chunking</li> <li>log probability</li> <li>greedy decoding</li> <li>embedding</li> <li>prompt engineering</li> </ul>"},{"location":"includes/related/tool-use/","title":"Tool use","text":"<p>Related terms</p> <ul> <li>function calling</li> <li>agentic ai</li> <li>system prompt</li> <li>prompt engineering</li> <li>model governance</li> <li>agent executor</li> <li>ml ops</li> <li>content moderation</li> </ul>"},{"location":"includes/related/top-k-sampling/","title":"Top k sampling","text":"<p>Related terms</p> <ul> <li>top-p sampling</li> <li>temperature</li> <li>decoding</li> <li>greedy decoding</li> <li>fine-tuning</li> <li>validation set</li> <li>cross-validation</li> <li>log probability</li> </ul>"},{"location":"includes/related/top-p-sampling/","title":"Top p sampling","text":"<p>Related terms</p> <ul> <li>top-k sampling</li> <li>temperature</li> <li>decoding</li> <li>log probability</li> <li>greedy decoding</li> <li>precision</li> <li>fine-tuning</li> <li>safety classifier</li> </ul>"},{"location":"includes/related/training-data/","title":"Training data","text":"<p>Related terms</p> <ul> <li>validation set</li> <li>overfitting</li> <li>test set</li> <li>fine-tuning</li> <li>generalization</li> <li>target variable</li> <li>regularization</li> <li>dataset card</li> </ul>"},{"location":"includes/related/transparency-report/","title":"Transparency report","text":"<p>Related terms</p> <ul> <li>algorithmic audit</li> <li>privacy</li> <li>privacy impact assessment</li> <li>ai assurance</li> <li>algorithmic impact assessment</li> <li>responsible ai</li> <li>ai incident response</li> <li>model interpretability</li> </ul>"},{"location":"includes/related/validation-set/","title":"Validation set","text":"<p>Related terms</p> <ul> <li>test set</li> <li>fine-tuning</li> <li>cross-validation</li> <li>training data</li> <li>overfitting</li> <li>evaluation</li> <li>synthetic data evaluation</li> <li>top-k sampling</li> </ul>"},{"location":"includes/related/vector-store/","title":"Vector store","text":"<p>Related terms</p> <ul> <li>retrieval</li> <li>embedding</li> <li>reranking</li> <li>data lineage</li> <li>kv cache</li> <li>retrieval-augmented generation</li> <li>chunking</li> <li>data retention</li> </ul>"},{"location":"includes/related/voice-cloning/","title":"Voice cloning","text":"<p>Related terms</p> <ul> <li>synthetic data</li> <li>diffusion model</li> <li>jailbreak prompt</li> <li>synthetic data evaluation</li> <li>privacy</li> <li>prompt injection</li> <li>decoding</li> <li>red teaming</li> </ul>"},{"location":"terms/","title":"Overview","text":""},{"location":"terms/#glossary-terms","title":"Glossary Terms","text":"<p>Total entries: 109</p> <ul> <li>agent executor \u2014 Controller layer that schedules planning, tool calls, and stop conditions so an AI agent completes tasks safely.</li> <li>agentic ai \u2014 Systems that plan, act, and iterate with minimal human prompts by chaining model calls and tools.</li> <li>ai assurance \u2014 Discipline that produces evidence, controls, and attestations showing an AI system meets agreed safety, compliance, and performance thresholds.</li> <li>ai circuit breaker \u2014 Automated control that halts model responses or tool access when risk thresholds are exceeded.</li> <li>ai incident response \u2014 Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.</li> <li>algorithmic audit \u2014 Independent review of an AI system\u2019s data, design, and outcomes to verify compliance, fairness, and risk controls.</li> <li>algorithmic bias \u2014 Systematic unfairness in model outputs that disadvantages certain groups or outcomes.</li> <li>algorithmic impact assessment \u2014 Structured review that documents how an AI system may affect people, processes, and compliance obligations.</li> <li>alignment \u2014 Making sure AI systems optimize for human values, policies, and intended outcomes.</li> <li>assurance case \u2014 Structured argument that proves an AI system meets safety and compliance expectations.</li> <li>attention \u2014 Technique enabling models to weight input tokens differently when producing each output.</li> <li>beam search \u2014 Deterministic decoding that keeps the top scoring sequences across multiple beams before selecting the final output.</li> <li>bias-variance tradeoff \u2014 Balance between underfitting and overfitting: low bias increases variance, while high bias lowers variance but misses patterns.</li> <li>chain-of-thought prompting \u2014 Prompting technique that asks models to reason through intermediate steps before giving a final answer.</li> <li>chunking \u2014 Splitting source documents into manageable pieces before indexing or feeding them to models.</li> <li>clip \u2014 Multimodal model that embeds images and text into a shared space using contrastive learning.</li> <li>confusion matrix \u2014 Table that summarizes true/false positives and negatives to diagnose classification performance.</li> <li>consent management \u2014 Practices that capture, honor, and audit user permissions across AI features.</li> <li>constitutional ai \u2014 Alignment approach where models critique and revise their own outputs against a written set of principles.</li> <li>content moderation \u2014 Workflows and tools that review, filter, and act on user-generated content to enforce policy.</li> <li>context window \u2014 Maximum number of tokens a model can consider at once during prompting or inference.</li> <li>cross-validation \u2014 Evaluation technique that splits data into multiple folds to estimate model performance on unseen samples.</li> <li>data lineage \u2014 Traceable record of how data moves, transforms, and is used across AI systems.</li> <li>data minimization \u2014 Principle of collecting and retaining only the data necessary for a defined purpose.</li> <li>data redaction \u2014 Removal or masking of sensitive fields before data is stored, shared, or used for model training.</li> <li>data retention \u2014 Policies defining how long data is stored, where it lives, and how it is deleted.</li> <li>dataset card \u2014 Structured documentation describing a dataset\u2019s purpose, composition, risks, and usage constraints.</li> <li>decoding \u2014 Algorithms that turn model probability distributions into output tokens during generation.</li> <li>differential privacy \u2014 Mathematical framework that limits how much any single record influences published data or model outputs.</li> <li>diffusion model \u2014 Generative model that iteratively denoises random noise to synthesize images, audio, or other data.</li> <li>direct preference optimization \u2014 Alignment technique that fine-tunes models directly on preference data without training a separate reward model.</li> <li>embedding \u2014 Dense numerical representation that captures semantic meaning of text, images, or other data.</li> <li>escalation policy \u2014 Playbook that defines when and how AI systems route control to human reviewers.</li> <li>evaluation \u2014 Systematic measurement of model performance, safety, and reliability using defined tests.</li> <li>evaluation harness \u2014 Automated pipeline that replays tasks, scores outputs, and reports regressions for AI systems.</li> <li>f1 score \u2014 Harmonic mean of precision and recall, balancing false positives and false negatives.</li> <li>fairness metrics \u2014 Quantitative measures that evaluate whether model performance is equitable across groups.</li> <li>feature engineering \u2014 Transforming raw data into model-ready features that improve signal, fairness, and maintainability.</li> <li>fine-tuning \u2014 Additional training that adapts a pretrained model to a specific task or domain.</li> <li>function calling \u2014 LLM capability that lets prompts invoke predefined functions and return structured arguments.</li> <li>generalization \u2014 Model's ability to sustain performance on unseen data rather than memorising the training set.</li> <li>generative ai \u2014 Family of models that produce new content\u2014text, images, code\u2014rather than only making predictions.</li> <li>gradient descent \u2014 Iterative optimization algorithm that updates model parameters in the direction of the negative gradient to minimize a loss function.</li> <li>greedy decoding \u2014 Strategy that selects the highest-probability token at each step, producing deterministic outputs.</li> <li>guardrail policy \u2014 Documented rules and prompts that define allowed, blocked, and escalated behaviors for AI systems.</li> <li>guardrails \u2014 Controls that constrain model behavior to comply with safety, legal, or brand requirements.</li> <li>hallucination \u2014 When an AI model presents fabricated or unsupported information as fact.</li> <li>human handoff \u2014 Moment when an AI workflow transfers control to a human for review or action.</li> <li>impact mitigation plan \u2014 Action plan that tracks risks, mitigations, owners, and timelines for an AI deployment.</li> <li>incident taxonomy \u2014 Standardized categories used to tag, analyze, and report AI incidents consistently.</li> <li>instruction tuning \u2014 Supervised training that teaches models to follow natural-language instructions using curated examples.</li> <li>jailbreak prompt \u2014 Crafted input that persuades a model to ignore safety instructions and produce disallowed responses.</li> <li>knowledge distillation \u2014 Technique that trains a smaller student model to mimic a larger teacher model\u2019s behavior.</li> <li>kv cache \u2014 Stored attention keys and values reused across decoding steps to speed sequential generation.</li> <li>log probability \u2014 Logarithm of a token\u2019s probability, used to inspect model confidence and guide decoding tweaks.</li> <li>loss function \u2014 Mathematical rule that scores how far model predictions deviate from desired targets.</li> <li>low-rank adaptation \u2014 Parameter-efficient fine-tuning that injects low-rank update matrices into transformer weights.</li> <li>memory strategy \u2014 Deliberate approach for when an AI agent stores, retrieves, or forgets context across tasks.</li> <li>mixture of experts \u2014 Neural architecture that routes tokens to specialized submodels to scale capacity efficiently.</li> <li>ml observability \u2014 Practices and tooling that surface model health through metrics, traces, and alerts across the lifecycle.</li> <li>ml ops \u2014 Operational discipline that manages ML models from experimentation through deployment and monitoring.</li> <li>model card \u2014 Standardized documentation describing a model\u2019s purpose, data, performance, and limitations.</li> <li>model drift \u2014 Gradual mismatch between model assumptions and real-world data that degrades performance over time.</li> <li>model governance \u2014 Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.</li> <li>model interpretability \u2014 Ability to explain how a model arrives at its predictions in ways stakeholders understand.</li> <li>overfitting \u2014 When a model memorizes training data patterns so closely that it performs poorly on new samples.</li> <li>precision \u2014 Share of predicted positives that are actually correct for a given classifier.</li> <li>preference dataset \u2014 Labeled comparisons of model outputs that capture which responses humans prefer.</li> <li>privacy \u2014 Principle of limiting data collection, use, and exposure to protect individuals\u2019 information.</li> <li>privacy budget \u2014 Quantitative limit on how much privacy loss is allowed when applying differential privacy.</li> <li>privacy impact assessment \u2014 Structured review that evaluates how a system collects, uses, and safeguards personal data.</li> <li>prompt engineering \u2014 Crafting and testing prompts to steer model behavior toward desired outcomes.</li> <li>prompt injection \u2014 Attack that inserts malicious instructions into model inputs to override original prompts or policies.</li> <li>quantization \u2014 Technique that compresses model weights into lower-precision formats to shrink size and speed inference.</li> <li>recall \u2014 Share of actual positives a model successfully identifies.</li> <li>red teaming \u2014 Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.</li> <li>regularization \u2014 Techniques that add penalties or constraints during training to reduce overfitting and improve generalisation.</li> <li>reinforcement learning from human feedback \u2014 Training approach that tunes a model using reward signals learned from human preference data.</li> <li>repetition penalty \u2014 Decoding adjustment that down-weights tokens already generated to reduce loops and repeated phrases.</li> <li>reranking \u2014 Step that refines retrieval results using a more precise but slower scoring model.</li> <li>responsible ai \u2014 Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.</li> <li>retrieval \u2014 Process of selecting relevant documents or vectors from a corpus in response to a query.</li> <li>retrieval-augmented generation \u2014 Workflow that grounds a generative model with retrieved context before producing output.</li> <li>reward model \u2014 Model trained on human preferences that scores AI responses for alignment or quality.</li> <li>risk register \u2014 Central list of identified AI risks, their owners, mitigations, and review status.</li> <li>robust prompting \u2014 Prompt design techniques that harden models against injections, ambiguity, and unsafe outputs.</li> <li>roc auc \u2014 Metric summarizing binary classifier performance by measuring area under the ROC curve.</li> <li>safety classifier \u2014 Model that detects policy-violating or risky content before or after generation.</li> <li>safety evaluation \u2014 Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.</li> <li>safety review board \u2014 Cross-functional committee that approves high-risk AI launches and monitors mitigations.</li> <li>safety spec \u2014 Document that codifies allowed, disallowed, and escalated behaviours for an AI system so teams can enforce safety and policy expectations.</li> <li>self-consistency decoding \u2014 Decoding strategy that samples multiple reasoning paths and aggregates the most consistent answer.</li> <li>self-critique loop \u2014 Pattern where a model reviews its own outputs, critiques them, and produces revisions before responding.</li> <li>shadow deployment \u2014 Deploying an AI system alongside the existing workflow without user impact to collect telemetry.</li> <li>synthetic data \u2014 Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.</li> <li>synthetic data evaluation \u2014 Process for measuring fidelity, utility, privacy, and bias of synthetic datasets before use.</li> <li>system prompt \u2014 Foundational instruction that sets role, tone, and guardrails for an AI assistant before user input.</li> <li>target variable \u2014 Outcome the model is trained to predict, providing the signal for calculating loss.</li> <li>temperature \u2014 Decoding parameter that controls how random or deterministic a model\u2019s outputs are.</li> <li>test set \u2014 Final evaluation split reserved for measuring real-world performance after all model tuning is finished.</li> <li>token \u2014 Smallest unit of text a model processes after tokenization, such as a word fragment or character.</li> <li>tool use \u2014 Pattern where a model selects external tools or functions to handle parts of a task.</li> <li>top-k sampling \u2014 Decoding method that samples from the k most probable next tokens to balance diversity and control.</li> <li>top-p sampling \u2014 Decoding strategy that samples from the smallest set of tokens whose probabilities sum to p.</li> <li>training data \u2014 Labeled examples the model learns from before it ever sees validation or test inputs.</li> <li>transparency report \u2014 Periodic disclosure that details how an AI system operates, what data it handles, and how risks are mitigated.</li> <li>validation set \u2014 Dataset slice used to tune hyperparameters and compare experiments without touching the test set.</li> <li>vector store \u2014 Database optimized to store embeddings and execute similarity search over vectors.</li> <li>voice cloning \u2014 Technique that replicates a person\u2019s voice using generative models trained on audio samples.</li> </ul>"},{"location":"terms/agent-executor/","title":"agent executor","text":""},{"location":"terms/agent-executor/#agent-executor","title":"agent executor","text":"<p>Aliases: agent run loop, agent controller Categories: Agents &amp; Tooling Roles: Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/agent-executor/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/agent-executor/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Audit exposed tools against the safeguards described and document approval paths.</li> <li>Test hand-offs with human reviewers to confirm the safety expectations captured here are met.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/agent-executor/#short-definition","title":"Short definition","text":"<p>Controller layer that schedules planning, tool calls, and stop conditions so an AI agent completes tasks safely.</p>"},{"location":"terms/agent-executor/#long-definition","title":"Long definition","text":"<p>An agent executor is the control loop that wraps a language model with planning, tool routing, memory updates, and termination rules. It interprets the model's action plans, selects the right tool or retriever, enforces guardrails, and decides when the task is complete or needs human escalation. Mature executors persist interim state, log each decision for observability, and cap iteration budgets to avoid runaway loops. Product teams set policies for the executor about tool eligibility or human approval, while engineers own the orchestration logic that keeps the agent explainable and reversible. Without a disciplined executor, agent architectures tend to overrun budgets, trigger risky actions, or hide provenance needed for governance reviews.</p>"},{"location":"terms/agent-executor/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Clarify what control layer governs autonomous workflows so you can scope approvals and accountability.</li> <li>Engineer: Treat the executor as orchestrator code: build deterministic plans, enforce stop criteria, and log every action.</li> </ul>"},{"location":"terms/agent-executor/#examples","title":"Examples","text":"<p>Do - Cap the number of tool invocations per task and emit structured traces for monitoring. - Define escalation triggers that halt the loop when confidence drops or policies flag sensitive data.</p> <p>Don't - Let the agent call privileged tools without a permissions check in the executor. - Ship an executor without human-readable logs or telemetry for audit reviews.</p>"},{"location":"terms/agent-executor/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: governance, risk_management, transparency</li> <li>Risk notes: Unbounded loops or opaque routing decisions can create safety, compliance, and cost exposures.</li> </ul>"},{"location":"terms/agent-executor/#relationships","title":"Relationships","text":"<ul> <li>Broader: agentic ai, tool use</li> <li>Related: guardrails, retrieval-augmented generation, ml ops</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'agent executor'.</p>"},{"location":"terms/agent-executor/#citations","title":"Citations","text":"<ul> <li>LangChain Documentation \u2013 Agent Executors</li> <li>Anthropic Docs \u2013 Building Agentic Workflows</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/agent-executor.yml</code></p>"},{"location":"terms/agentic-ai/","title":"agentic ai","text":""},{"location":"terms/agentic-ai/#agentic-ai","title":"agentic ai","text":"<p>Aliases: ai agents, autonomous agent Categories: Agents &amp; Tooling Roles: Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/agentic-ai/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/agentic-ai/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Audit exposed tools against the safeguards described and document approval paths.</li> <li>Test hand-offs with human reviewers to confirm the safety expectations captured here are met.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/agentic-ai/#short-definition","title":"Short definition","text":"<p>Systems that plan, act, and iterate with minimal human prompts by chaining model calls and tools.</p>"},{"location":"terms/agentic-ai/#long-definition","title":"Long definition","text":"<p>Agentic AI describes architectures where models make decisions about what actions to take next, often using planning loops, tool calls, and memory to pursue a goal. Unlike single-shot prompting, agentic systems break work into steps, call APIs, and reflect on intermediate results before proceeding. They power use cases like research assistants, workflow automation, and incident triage bots. Engineers combine language models with planners, vector memories, and policy checks to maintain control. Product teams set guardrails on autonomy levels, defining when humans approve steps or review logs. Governance stakeholders focus on accountability, ensuring agent actions are auditable, reversible, and aligned with policy. Because agentic AI increases the surface area for safety incidents or costly loops, monitoring and termination criteria are essential. Investing in agent design principles helps organizations harness automation without losing visibility or violating compliance commitments.</p>"},{"location":"terms/agentic-ai/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Agentic AI strings tasks together so the assistant can take initiative instead of waiting for every instruction.</li> <li>Engineer: Orchestrate planning, tool invocation, and memory components around an LLM to execute multi-step objectives.</li> </ul>"},{"location":"terms/agentic-ai/#examples","title":"Examples","text":"<p>Do - Define termination conditions and escalation triggers before enabling autonomous execution.</p> <p>Don't - Allow agents unfettered access to production systems without audit trails or throttling.</p>"},{"location":"terms/agentic-ai/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, risk_management</li> <li>Risk notes: Autonomous loops can drift from intent or amplify harmful behaviors if oversight is weak.</li> </ul>"},{"location":"terms/agentic-ai/#relationships","title":"Relationships","text":"<ul> <li>Broader: generative ai</li> <li>Narrower: tool use, retrieval-augmented generation</li> <li>Related: guardrails, system prompt, incident response</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'agentic ai'.</p>"},{"location":"terms/agentic-ai/#citations","title":"Citations","text":"<ul> <li>Stanford HAI \u2013 What Are Agentic AI Systems?</li> <li>LangChain Concepts \u2013 Agents</li> <li>Generative Agents \u2013 Interactive Simulacra of Human Behavior</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/agentic-ai.yml</code></p>"},{"location":"terms/ai-assurance/","title":"ai assurance","text":""},{"location":"terms/ai-assurance/#ai-assurance","title":"ai assurance","text":"<p>Aliases: AI assurance program Categories: Governance &amp; Risk Roles: Product &amp; Program Managers, Engineering &amp; Platform, Policy &amp; Risk, Legal &amp; Compliance, Security &amp; Trust Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/ai-assurance/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/ai-assurance/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/ai-assurance/#short-definition","title":"Short definition","text":"<p>Discipline that produces evidence, controls, and attestations showing an AI system meets agreed safety, compliance, and performance thresholds.</p>"},{"location":"terms/ai-assurance/#long-definition","title":"Long definition","text":"<p>AI assurance is the end-to-end discipline that gives stakeholders confidence an AI system behaves as intended across its lifecycle. Teams collect structured evidence\u2014model cards, test results, evaluation dashboards, bias and privacy assessments\u2014and compare them against assurance criteria derived from policy, regulation, and business risk appetite. Assurance programs span pre-launch discovery, red teaming, and documentation, followed by post-launch monitoring and re-certification when data, models, or prompts change. Product and engineering leads own the technical mitigations, while policy, legal, and security partners review controls, disclosure obligations, and escalation paths. Mature organisations use playbooks, independent reviewers, and audit trails so they can demonstrate compliance with frameworks such as the NIST AI RMF or government procurement requirements. Without an assurance function, leaders lack defensible evidence for regulators and customers, and harmful behaviour may slip into production unchecked.</p>"},{"location":"terms/ai-assurance/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Confirms our AI launches have evidence, owners, and guardrails that withstand regulatory and customer scrutiny.</li> <li>Engineer: Defines the artefacts, tests, and controls you must deliver before handover and keep current after updates.</li> </ul>"},{"location":"terms/ai-assurance/#examples","title":"Examples","text":"<p>Do - Assemble an assurance dossier that bundles eval results, model cards, and mitigation plans before launch approval. - Schedule periodic re-certification when training data, prompts, or external regulations change.</p> <p>Don't - Rely on a single accuracy metric as proof of readiness without documenting safety or fairness tests. - Ship critical updates without notifying assurance reviewers or updating evidence trails.</p>"},{"location":"terms/ai-assurance/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, risk_management, transparency</li> <li>Risk notes: Missing assurance evidence makes it impossible to prove compliance, increasing regulatory, contractual, and safety exposure.</li> </ul>"},{"location":"terms/ai-assurance/#relationships","title":"Relationships","text":"<ul> <li>Broader: responsible ai</li> <li>Related: model governance, algorithmic audit, evaluation, ai incident response</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'ai assurance'.</p>"},{"location":"terms/ai-assurance/#citations","title":"Citations","text":"<ul> <li>arXiv \u2013 A Survey on AI Assurance</li> <li>OECD AI Glossary</li> <li>Partnership on AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/ai-assurance.yml</code></p>"},{"location":"terms/ai-circuit-breaker/","title":"ai circuit breaker","text":""},{"location":"terms/ai-circuit-breaker/#ai-circuit-breaker","title":"ai circuit breaker","text":"<p>Aliases: kill switch, automated shutdown Categories: Operations &amp; Monitoring Roles: Engineering &amp; Platform, Security &amp; Trust, Policy &amp; Risk, Product &amp; Program Managers Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/ai-circuit-breaker/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/ai-circuit-breaker/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Instrument dashboards or alerts that reflect the metrics highlighted in this definition.</li> <li>Update incident response or on-call runbooks with the glossary's do/don't scenarios.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/ai-circuit-breaker/#short-definition","title":"Short definition","text":"<p>Automated control that halts model responses or tool access when risk thresholds are exceeded.</p>"},{"location":"terms/ai-circuit-breaker/#long-definition","title":"Long definition","text":"<p>An AI circuit breaker detects abnormal behavior\u2014such as rapid policy violations, runaway tool use, or anomalous telemetry\u2014and stops the system from continuing without human approval. It can disable outputs, revoke tool permissions, or route all traffic to human agents. Security and engineering teams design detection logic, policy leaders set thresholds, and product teams coordinate user messaging. Circuit breakers complement escalation policies and incident response plans, providing a fast path to contain harm. Without them, incidents may escalate faster than humans can intervene manually.</p>"},{"location":"terms/ai-circuit-breaker/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Mandate circuit breakers for high-impact automations to cap worst-case scenarios.</li> <li>Engineer: Log activations, reasons, and resolution notes so the control remains auditable.</li> </ul>"},{"location":"terms/ai-circuit-breaker/#examples","title":"Examples","text":"<p>Do - Trigger a breaker if safety classifiers flag repeated violations within a session. - Simulate breaker events during chaos exercises to ensure recovery paths work.</p> <p>Don't - Leave breaker logic undocumented or dependent on a single engineer. - Ignore alerts after a breaker fires; reset only after root cause analysis.</p>"},{"location":"terms/ai-circuit-breaker/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: risk_management, monitoring, security</li> <li>Risk notes: Without emergency stops, AI incidents can cause cascading harm before humans respond.</li> </ul>"},{"location":"terms/ai-circuit-breaker/#relationships","title":"Relationships","text":"<ul> <li>Broader: ai incident response</li> <li>Related: escalation policy, safety classifier, guardrail policy</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'ai circuit breaker'.</p>"},{"location":"terms/ai-circuit-breaker/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF \u2013 Protection Function</li> <li>Microsoft \u2013 Safe Deployment Practices</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/ai-circuit-breaker.yml</code></p>"},{"location":"terms/ai-incident-response/","title":"ai incident response","text":""},{"location":"terms/ai-incident-response/#ai-incident-response","title":"ai incident response","text":"<p>Aliases: model incident response, ai escalation Categories: Operations &amp; Monitoring, Governance &amp; Risk Roles: Communications &amp; Enablement, Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/ai-incident-response/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/ai-incident-response/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Instrument dashboards or alerts that reflect the metrics highlighted in this definition.</li> <li>Update incident response or on-call runbooks with the glossary's do/don't scenarios.</li> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/ai-incident-response/#short-definition","title":"Short definition","text":"<p>Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.</p>"},{"location":"terms/ai-incident-response/#long-definition","title":"Long definition","text":"<p>AI incident response adapts traditional incident management to the unique risks of machine learning systems. It defines how teams detect unusual behavior, declare incidents, assemble cross-functional responders, communicate with stakeholders, and deploy mitigations or rollbacks. Triggers include safety violations, hallucinations with material impact, security breaches, or regulatory inquiries. Effective programs maintain runbooks that cover data isolation, prompt freezes, feature flags, and guardrail adjustments. Product, engineering, legal, and communications partners collaborate to assess severity, user impact, and reporting obligations. Governance frameworks expect documented incident response procedures with clear owners and timelines, particularly for high-risk deployments. After-action reviews capture learnings that feed back into evaluation suites, prompts, and monitoring. Without a disciplined incident response plan, organizations risk delayed containment, regulatory penalties, and erosion of user trust.</p>"},{"location":"terms/ai-incident-response/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: AI incident response is the playbook that keeps harm contained and stakeholders informed when something goes wrong.</li> <li>Engineer: Detect anomalies, page the on-call rotation, freeze risky components, and coordinate fixes across data, model, and infra teams.</li> </ul>"},{"location":"terms/ai-incident-response/#examples","title":"Examples","text":"<p>Do - Run quarterly tabletop exercises to rehearse AI incident response workflows.</p> <p>Don't - Silence alerts or skip postmortems once an incident is closed.</p>"},{"location":"terms/ai-incident-response/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: risk_management, accountability</li> <li>Risk notes: Lack of documented response plans prolongs harmful behavior and undermines regulatory reporting.</li> </ul>"},{"location":"terms/ai-incident-response/#relationships","title":"Relationships","text":"<ul> <li>Broader: model governance</li> <li>Related: red teaming, guardrails, ml observability</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'ai incident response'.</p>"},{"location":"terms/ai-incident-response/#citations","title":"Citations","text":"<ul> <li>NIST AI Risk Management Framework</li> <li>CISA \u2013 AI Cybersecurity Collaboration Playbook</li> <li>NIST \u2013 Computer Security Incident Handling Guide</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/ai-incident-response.yml</code></p>"},{"location":"terms/algorithmic-audit/","title":"algorithmic audit","text":""},{"location":"terms/algorithmic-audit/#algorithmic-audit","title":"algorithmic audit","text":"<p>Aliases: algorithm accountability audit Categories: Governance &amp; Risk Roles: Product &amp; Program Managers, Engineering &amp; Platform, Policy &amp; Risk, Legal &amp; Compliance, Communications &amp; Enablement Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/algorithmic-audit/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> </ul>"},{"location":"terms/algorithmic-audit/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/algorithmic-audit/#short-definition","title":"Short definition","text":"<p>Independent review of an AI system\u2019s data, design, and outcomes to verify compliance, fairness, and risk controls.</p>"},{"location":"terms/algorithmic-audit/#long-definition","title":"Long definition","text":"<p>An algorithmic audit is a structured examination of an AI system carried out by internal or external reviewers to assess whether it behaves responsibly and complies with legal, ethical, and contractual obligations. Auditors inspect training data provenance, modeling choices, evaluation methods, documentation, and deployment safeguards. They recreate metrics for accuracy, fairness, robustness, privacy, and security, and look for gaps between intended and observed behaviour. Audits often include stakeholder interviews, policy mapping, bias testing, and reproducibility checks, culminating in findings and remediation plans with accountable owners. Organisations commission audits pre-launch or after material changes, and regulators increasingly require independent assurance for high-risk AI services. Successful audits depend on traceable artefacts\u2014model cards, change logs, governance decisions\u2014and cross-functional collaboration across engineering, product, policy, and communications teams. Skipping or under-scoping audits makes it difficult to prove compliance, potentially exposing users to harm and the organisation to legal or reputational damage.</p>"},{"location":"terms/algorithmic-audit/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Demonstrates to regulators, customers, and partners that our AI undergoes independent scrutiny with corrective action plans.</li> <li>Engineer: Clarifies the evidence, metrics, and documentation auditors need to reproduce and validate model behaviour.</li> </ul>"},{"location":"terms/algorithmic-audit/#examples","title":"Examples","text":"<p>Do - Engage an independent reviewer to replicate fairness, privacy, and robustness tests before public launch. - Track remediation actions from audit findings through to closure with documented owners and deadlines.</p> <p>Don't - Limit audits to marketing claims without sharing code, datasets, or evaluation pipelines. - Ignore communications planning\u2014audits should prepare messaging for customers and regulators.</p>"},{"location":"terms/algorithmic-audit/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, fairness, transparency</li> <li>Risk notes: Inadequate audits hide systemic issues, undermining compliance readiness and eroding stakeholder trust.</li> </ul>"},{"location":"terms/algorithmic-audit/#relationships","title":"Relationships","text":"<ul> <li>Broader: ai assurance</li> <li>Related: model governance, fairness metrics, evaluation, red teaming</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'algorithmic audit'.</p>"},{"location":"terms/algorithmic-audit/#citations","title":"Citations","text":"<ul> <li>USENIX \u2013 Algorithmic Auditing 101</li> <li>NIST AI Risk Management Framework</li> <li>OECD AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/algorithmic-audit.yml</code></p>"},{"location":"terms/algorithmic-bias/","title":"algorithmic bias","text":""},{"location":"terms/algorithmic-bias/#algorithmic-bias","title":"algorithmic bias","text":"<p>Aliases: systemic bias, ai bias Categories: Governance &amp; Risk Roles: Policy &amp; Risk, Legal &amp; Compliance, Product &amp; Program Managers, Communications &amp; Enablement, Security &amp; Trust Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p> <p>Put it into practice</p> <p>Run the Governance Dashboard checklist before launch.</p>"},{"location":"terms/algorithmic-bias/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/algorithmic-bias/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/algorithmic-bias/#short-definition","title":"Short definition","text":"<p>Systematic unfairness in model outputs that disadvantages certain groups or outcomes.</p>"},{"location":"terms/algorithmic-bias/#long-definition","title":"Long definition","text":"<p>Algorithmic bias arises when models produce systematically skewed results that disadvantage individuals or groups, often reflecting historical inequities in training data, feature selection, or objective functions. Bias can manifest through discriminatory false positives, unequal error rates, or exclusionary recommendations. Organizations must examine the full pipeline\u2014data collection, labeling, modeling, evaluation, and deployment\u2014to identify root causes. Product teams collaborate with policy, legal, and communications partners to define fairness objectives, while engineers implement bias detection metrics, reweighting, and post-processing adjustments. Governance frameworks (NIST AI RMF, EU AI Act) require documentation of bias assessments, stakeholder engagement, and remediation plans. Failing to manage algorithmic bias can create legal liability, reputational damage, and harm to marginalized communities. Continuous monitoring, diverse evaluation cohorts, and community feedback loops are essential for sustained mitigation.</p>"},{"location":"terms/algorithmic-bias/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Algorithmic bias is when the AI treats groups unfairly, risking customer trust and regulatory violations.</li> <li>Engineer: Quantify and mitigate disparities across subpopulations using metrics like equalized odds, demographic parity, and subgroup ROC analysis.</li> </ul>"},{"location":"terms/algorithmic-bias/#examples","title":"Examples","text":"<p>Do - Include fairness metrics in evaluation pipelines and report them alongside accuracy. - Engage impacted stakeholders when designing mitigation strategies.</p> <p>Don't - Launch features without testing performance across sensitive attributes. - Assume bias is solved after one mitigation; monitor continuously.</p>"},{"location":"terms/algorithmic-bias/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: fairness, transparency</li> <li>Risk notes: Unmitigated bias can violate anti-discrimination laws, trigger regulatory action, and erode brand trust.</li> </ul>"},{"location":"terms/algorithmic-bias/#relationships","title":"Relationships","text":"<ul> <li>Broader: responsible ai</li> <li>Related: safety evaluation, red teaming, privacy impact assessment</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'algorithmic bias'.</p>"},{"location":"terms/algorithmic-bias/#citations","title":"Citations","text":"<ul> <li>Wikipedia \u2013 Algorithmic Bias</li> <li>Nature \u2013 The Fight Against Algorithmic Bias</li> <li>Brookings \u2013 Algorithmic Bias Detection and Mitigation</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/algorithmic-bias.yml</code></p>"},{"location":"terms/algorithmic-impact-assessment/","title":"algorithmic impact assessment","text":""},{"location":"terms/algorithmic-impact-assessment/#algorithmic-impact-assessment","title":"algorithmic impact assessment","text":"<p>Aliases: AIA, AI impact assessment Categories: Governance &amp; Risk Roles: Policy &amp; Risk, Legal &amp; Compliance, Product &amp; Program Managers, Communications &amp; Enablement, Engineering &amp; Platform Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/algorithmic-impact-assessment/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/algorithmic-impact-assessment/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/algorithmic-impact-assessment/#short-definition","title":"Short definition","text":"<p>Structured review that documents how an AI system may affect people, processes, and compliance obligations.</p>"},{"location":"terms/algorithmic-impact-assessment/#long-definition","title":"Long definition","text":"<p>Algorithmic impact assessments (AIAs) extend privacy and safety assessments to machine learning systems. They inventory intended uses, stakeholders, datasets, and model decisions; surface potential harms or rights impacts; and log mitigation plans before launch. Mature teams combine qualitative stakeholder interviews with quantitative evidence from evaluations, bias testing, and monitoring. The process forces clarity on accountability, human oversight, documentation, and escalation paths so downstream teams can manage risk. Governments increasingly mandate AIAs for high-risk use cases, making them a practical way to align product roadmaps with regulatory expectations. When revisited after major releases, AIAs become living records that connect model behavior to governance controls and public transparency.</p>"},{"location":"terms/algorithmic-impact-assessment/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: An AIA proves we understand who an AI system could impact and how we will keep those risks managed.</li> <li>Engineer: Document data, model, and deployment assumptions so policy and legal partners can catch misaligned behavior before launch.</li> </ul>"},{"location":"terms/algorithmic-impact-assessment/#examples","title":"Examples","text":"<p>Do - Collect evaluation metrics, human feedback, and red-team findings as evidence in the AIA dossier.</p> <p>Don't - Treat the AIA as a one-time checklist that never gets updated after launch.</p>"},{"location":"terms/algorithmic-impact-assessment/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, risk_management, transparency</li> <li>Risk notes: Skipping AIAs leaves organizations unable to show regulators how risks were identified or mitigated.</li> </ul>"},{"location":"terms/algorithmic-impact-assessment/#relationships","title":"Relationships","text":"<ul> <li>Broader: responsible ai, model governance</li> <li>Related: privacy impact assessment, red teaming, evaluation</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'algorithmic impact assessment'.</p>"},{"location":"terms/algorithmic-impact-assessment/#citations","title":"Citations","text":"<ul> <li>OECD AI Glossary</li> <li>AI Now Institute Lexicon</li> <li>Partnership on AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/algorithmic-impact-assessment.yml</code></p>"},{"location":"terms/alignment/","title":"alignment","text":""},{"location":"terms/alignment/#alignment","title":"alignment","text":"<p>Aliases: AI alignment, value alignment Categories: Governance &amp; Risk Roles: Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/alignment/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/alignment/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/alignment/#short-definition","title":"Short definition","text":"<p>Making sure AI systems optimize for human values, policies, and intended outcomes.</p>"},{"location":"terms/alignment/#long-definition","title":"Long definition","text":"<p>Alignment is the multidisciplinary effort to design AI systems whose goals, behaviors, and outputs remain consistent with human intent and societal norms. It spans technical research\u2014such as reward modeling, constitutional AI, interpretability, and adversarial training\u2014and organizational governance, including policy frameworks, oversight committees, and escalation paths. Alignment work acknowledges that models learn from imperfect data and may pursue proxy objectives that conflict with human priorities. Product leaders use alignment roadmaps to decide which features require human-in-the-loop review, while engineers translate alignment goals into metrics, eval harnesses, and guardrails. Regulators and standards bodies, including NIST and ISO, emphasize alignment as part of trustworthy AI, requiring documentation of assumptions, residual risks, and impact mitigation strategies. Sustainable alignment programs treat it as an ongoing lifecycle activity rather than a one-time tuning exercise.</p>"},{"location":"terms/alignment/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Alignment is how we make sure the AI keeps serving our mission and values as it evolves.</li> <li>Engineer: Research and governance toolkit ensuring loss functions, feedback loops, and guardrails drive behavior toward intended objectives.</li> </ul>"},{"location":"terms/alignment/#examples","title":"Examples","text":"<p>Do - Document alignment hypotheses and track eval metrics tied to specific risk scenarios.</p> <p>Don't - Assume alignment is solved after one fine-tuning pass without continuous monitoring.</p>"},{"location":"terms/alignment/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, transparency, validity</li> <li>Risk notes: Weak alignment programs allow models to pursue proxy goals that conflict with legal or ethical obligations.</li> </ul>"},{"location":"terms/alignment/#relationships","title":"Relationships","text":"<ul> <li>Broader: responsible AI</li> <li>Narrower: constitutional AI, reinforcement learning from human feedback</li> <li>Related: guardrails, evaluation, red teaming</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'alignment'.</p>"},{"location":"terms/alignment/#citations","title":"Citations","text":"<ul> <li>Wikipedia \u2013 AI Alignment</li> <li>OpenAI \u2013 Learning from Human Preferences</li> <li>Microsoft \u2013 Responsible AI Principles</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/alignment.yml</code></p>"},{"location":"terms/assurance-case/","title":"assurance case","text":""},{"location":"terms/assurance-case/#assurance-case","title":"assurance case","text":"<p>Aliases: safety case, structured assurance argument Categories: Governance &amp; Risk Roles: Policy &amp; Risk, Legal &amp; Compliance, Engineering &amp; Platform Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/assurance-case/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/assurance-case/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/assurance-case/#short-definition","title":"Short definition","text":"<p>Structured argument that proves an AI system meets safety and compliance expectations.</p>"},{"location":"terms/assurance-case/#long-definition","title":"Long definition","text":"<p>An assurance case is a structured, evidence-backed argument that a system satisfies defined safety, ethics, and regulatory claims. Borrowed from aviation and medical devices, it organizes claims, supporting evidence, and reasoning into a traceable hierarchy so reviewers can judge whether an AI system is trustworthy. In practice, the case links risk registers, evaluation results, mitigations, and operating procedures to each claim. Policy and legal teams determine the claims needed for compliance, engineering curates the technical evidence, and product leaders maintain change logs so the case stays current across releases. Without a living assurance case, it is hard to demonstrate due diligence to regulators or to coordinate sign-off across governance bodies.</p>"},{"location":"terms/assurance-case/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Use assurance cases to consolidate the evidence your board and regulators expect before high-risk launches.</li> <li>Engineer: Flow evaluation metrics, mitigations, and incident learnings into a single structured argument for review.</li> </ul>"},{"location":"terms/assurance-case/#examples","title":"Examples","text":"<p>Do - Map every top risk in the register to a specific mitigation and evaluation artifact within the case. - Schedule periodic refreshes so control owners update evidence before expiry.</p> <p>Don't - Rely on unstructured wikis or slides that obscure which claims the evidence supports. - Freeze the case after initial approval, causing drift between documentation and production behavior.</p>"},{"location":"terms/assurance-case/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: governance, accountability, risk_management</li> <li>Risk notes: Missing or outdated assurance arguments weaken regulatory defenses and slow executive approvals.</li> </ul>"},{"location":"terms/assurance-case/#relationships","title":"Relationships","text":"<ul> <li>Broader: ai assurance, model governance</li> <li>Related: risk register, impact mitigation plan, transparency report</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'assurance case'.</p>"},{"location":"terms/assurance-case/#citations","title":"Citations","text":"<ul> <li>OECD AI Glossary</li> <li>Safety Case Overview</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/assurance-case.yml</code></p>"},{"location":"terms/attention/","title":"attention","text":""},{"location":"terms/attention/#attention","title":"attention","text":"<p>Aliases: attention mechanism, self-attention Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/attention/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/attention/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/attention/#short-definition","title":"Short definition","text":"<p>Technique enabling models to weight input tokens differently when producing each output.</p>"},{"location":"terms/attention/#long-definition","title":"Long definition","text":"<p>Attention assigns dynamic importance scores to tokens so a model can focus on the most relevant parts of the sequence when generating or interpreting outputs. In transformer architectures, self-attention computes query, key, and value projections that interact through scaled dot products, allowing every token to attend to every other token in the same layer. Multi-head attention repeats this operation across parallel subspaces, capturing nuanced relationships such as syntax, long-range dependencies, and positional context. The mechanism replaced recurrent networks for many language and vision tasks by enabling parallel processing and rich contextual reasoning. Engineers diagnose quality issues by inspecting attention patterns, tuning head counts, or constraining context windows to manage memory. Governance teams monitor attention configurations because they influence explainability\u2014saliency maps and attribution methods often rely on attention weights to justify model decisions in regulated settings.</p>"},{"location":"terms/attention/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Attention is how the model decides which words or pixels matter most before answering.</li> <li>Engineer: QKV projections with softmax-normalized weights that let each token aggregate information from the entire sequence.</li> </ul>"},{"location":"terms/attention/#examples","title":"Examples","text":"<p>Do - Profile attention head usage to identify redundant heads before applying pruning or distillation.</p> <p>Don't - Assume longer context windows automatically improve answers without verifying attention saturation and memory usage.</p>"},{"location":"terms/attention/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, robustness</li> <li>Risk notes: Opaque attention patterns can hinder explainability obligations in regulated workflows.</li> </ul>"},{"location":"terms/attention/#relationships","title":"Relationships","text":"<ul> <li>Broader: transformer</li> <li>Narrower: cross-attention, multi-head attention</li> <li>Related: context window, kv cache, token</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'attention'.</p>"},{"location":"terms/attention/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/attention.yml</code></p>"},{"location":"terms/beam-search/","title":"beam search","text":""},{"location":"terms/beam-search/#beam-search","title":"beam search","text":"<p>Aliases: beam decoding, multi-path decoding Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p> <p>Put it into practice</p> <p>Pair with the Prompt Engineering Playbook when crafting deterministic flows.</p>"},{"location":"terms/beam-search/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/beam-search/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/beam-search/#short-definition","title":"Short definition","text":"<p>Deterministic decoding that keeps the top scoring sequences across multiple beams before selecting the final output.</p>"},{"location":"terms/beam-search/#long-definition","title":"Long definition","text":"<p>Beam search expands several candidate continuations in parallel, keeping only the highest-scoring sequences at each step according to their cumulative log probabilities. By exploring multiple beams instead of a single greedy path, the method can uncover higher-quality completions and reduce the chance of getting stuck in locally optimal\u2014but globally poor\u2014answers. Teams tune beam width and length penalties to balance diversity against compute cost, because wider beams demand more memory and latency. Product workflows that require structured or citation-heavy responses often pair beam search with reranking or guardrails to ensure final selections remain compliant. Engineers also log intermediate beams to debug why a response was chosen and to audit near-miss alternatives. Governance stakeholders review beam configurations in high-stakes deployments to confirm that deterministic choices align with evaluation baselines and do not inadvertently suppress required disclosures.</p>"},{"location":"terms/beam-search/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Beam search is a quality knob\u2014explore a few strong answer paths before picking the best one.</li> <li>Engineer: Maintain N top-scoring sequences using cumulative log probabilities; apply length penalties and reranking before selecting the final candidate.</li> </ul>"},{"location":"terms/beam-search/#examples","title":"Examples","text":"<p>Do - Monitor latency impact when increasing beam width beyond 4 to ensure SLAs still hold. - Analyze discarded beams during incident reviews to understand alternative outputs.</p> <p>Don't - Ship beam search without configuring length penalties, which can bias toward verbose responses. - Use the same beam width across all locales without measuring cost and accuracy trade-offs.</p>"},{"location":"terms/beam-search/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: robustness, transparency</li> <li>Risk notes: Misconfigured beams can surface repetitive or off-policy content; document parameters and align them with evaluation coverage.</li> </ul>"},{"location":"terms/beam-search/#relationships","title":"Relationships","text":"<ul> <li>Broader: decoding</li> <li>Related: greedy decoding, top-k sampling, log probability</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'beam search'.</p>"},{"location":"terms/beam-search/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/beam-search.yml</code></p>"},{"location":"terms/bias-variance-tradeoff/","title":"bias-variance tradeoff","text":""},{"location":"terms/bias-variance-tradeoff/#bias-variance-tradeoff","title":"bias-variance tradeoff","text":"<p>Aliases: bias variance trade-off, generalization tradeoff Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Policy &amp; Risk Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/bias-variance-tradeoff/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> </ul>"},{"location":"terms/bias-variance-tradeoff/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Add this concept to onboarding materials so teammates share a common baseline.</li> <li>Link supporting research or documentation in your internal wiki for deeper study.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/bias-variance-tradeoff/#short-definition","title":"Short definition","text":"<p>Balance between underfitting and overfitting: low bias increases variance, while high bias lowers variance but misses patterns.</p>"},{"location":"terms/bias-variance-tradeoff/#long-definition","title":"Long definition","text":"<p>The bias-variance tradeoff describes how model complexity influences generalization. High-bias models make strong simplifying assumptions, often underfitting by missing real structure in the data. Low-bias models capture more nuance but can exhibit high variance, reacting strongly to noise and overfitting. Practitioners seek a sweet spot where both error sources are minimized. Diagnostics include validation curves that plot training and test error against model complexity, or Monte Carlo simulations that estimate variance across resampled datasets. Techniques such as regularization, ensemble learning, and cross-validation help navigate the tradeoff. Governance teams consider this tradeoff when assessing reliability: models tuned solely for accuracy may become unstable in production, while overly conservative models can entrench bias and miss meaningful signals. Documenting the rationale behind chosen complexity levels supports compliance and future audits.</p>"},{"location":"terms/bias-variance-tradeoff/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: The bias-variance tradeoff explains why simplifying too much misses insight, but over-optimizing creates fragile, noisy behavior.</li> <li>Engineer: Decompose generalization error into bias and variance terms; use validation diagnostics, regularization, and ensembles to reach the lowest combined error.</li> </ul>"},{"location":"terms/bias-variance-tradeoff/#examples","title":"Examples","text":"<p>Do - Plot learning curves to identify whether adding capacity improves validation performance. - Use k-fold cross-validation to estimate variance before promoting a model.</p> <p>Don't - Rely solely on training metrics when evaluating model quality. - Select the most complex architecture without evidence it improves validation outcomes.</p>"},{"location":"terms/bias-variance-tradeoff/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, transparency</li> <li>Risk notes: Ignoring the tradeoff leads to brittle models that either underperform or fail compliance evaluations in the field.</li> </ul>"},{"location":"terms/bias-variance-tradeoff/#relationships","title":"Relationships","text":"<ul> <li>Broader: model training</li> <li>Related: overfitting, cross-validation, regularization</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'bias-variance tradeoff'.</p>"},{"location":"terms/bias-variance-tradeoff/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> <li>scikit-learn \u2013 Underfitting vs. Overfitting</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/bias-variance-tradeoff.yml</code></p>"},{"location":"terms/chain-of-thought-prompting/","title":"chain-of-thought prompting","text":""},{"location":"terms/chain-of-thought-prompting/#chain-of-thought-prompting","title":"chain-of-thought prompting","text":"<p>Aliases: cot prompting, step-by-step prompting Categories: LLM Core Roles: Engineering &amp; Platform, Data Science &amp; Research, Product &amp; Program Managers Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/chain-of-thought-prompting/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/chain-of-thought-prompting/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/chain-of-thought-prompting/#short-definition","title":"Short definition","text":"<p>Prompting technique that asks models to reason through intermediate steps before giving a final answer.</p>"},{"location":"terms/chain-of-thought-prompting/#long-definition","title":"Long definition","text":"<p>Chain-of-thought prompting instructs a model to show its reasoning by generating step-by-step explanations. The extra reasoning tokens often improve accuracy on tasks like math, code, and logic by encouraging deliberate thinking. Engineers use chain-of-thought to debug and audit reasoning, while policy teams review steps for safety or bias. Product teams decide when to expose the chain to end users versus using it internally for verification. The technique increases latency and risk of leaking sensitive reasoning, so outputs must still be checked by evaluations or secondary models.</p>"},{"location":"terms/chain-of-thought-prompting/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Apply chain-of-thought selectively where explainability or reasoning accuracy matters most.</li> <li>Engineer: Pair reasoning traces with validators to catch hallucinated logic or policy violations.</li> </ul>"},{"location":"terms/chain-of-thought-prompting/#examples","title":"Examples","text":"<p>Do - Prompt models with <code>Let's reason step by step</code> for complex calculations. - Filter intermediate reasoning before displaying responses to end users.</p> <p>Don't - Assume longer reasoning always means higher accuracy. - Expose sensitive internal policies in reasoning traces.</p>"},{"location":"terms/chain-of-thought-prompting/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, risk_management, monitoring</li> <li>Risk notes: Verbose reasoning can leak policy details or amplify biased logic if unchecked.</li> </ul>"},{"location":"terms/chain-of-thought-prompting/#relationships","title":"Relationships","text":"<ul> <li>Broader: prompt engineering</li> <li>Related: self-consistency decoding, robust prompting, evaluation</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'chain-of-thought prompting'.</p>"},{"location":"terms/chain-of-thought-prompting/#citations","title":"Citations","text":"<ul> <li>Google \u2013 Chain of Thought Prompting</li> <li>DeepMind \u2013 Improving Reasoning in Language Models</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/chain-of-thought-prompting.yml</code></p>"},{"location":"terms/chunking/","title":"chunking","text":""},{"location":"terms/chunking/#chunking","title":"chunking","text":"<p>Aliases: document chunking, segmentation Categories: Retrieval &amp; RAG Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/chunking/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/chunking/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Validate retrieval quality using the evaluation guidance referenced in this entry.</li> <li>Ensure knowledge sources named here appear in your data governance inventory.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/chunking/#short-definition","title":"Short definition","text":"<p>Splitting source documents into manageable pieces before indexing or feeding them to models.</p>"},{"location":"terms/chunking/#long-definition","title":"Long definition","text":"<p>Chunking divides large documents, transcripts, or code repositories into smaller segments that fit within a model\u2019s context window and maintain topical coherence. Effective chunking balances granularity: slices must be large enough to preserve meaning yet small enough to avoid wasting tokens on irrelevant text. Techniques include fixed-length windows, semantic segmentation based on headings, and overlap strategies that keep shared context between adjacent chunks. In retrieval-augmented generation systems, chunking quality directly influences whether relevant passages appear in the prompt and whether citations map to recognizable sections. Engineers document chunking parameters alongside embedding models so they can reproduce indexes and evaluate recall. Governance teams assess chunking for privacy and intellectual property concerns, ensuring sensitive data is not inadvertently duplicated or exposed beyond its intended audience when recombining chunks.</p>"},{"location":"terms/chunking/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Chunking is how we slice big documents so the AI can reason over them without losing the thread.</li> <li>Engineer: Segmentation pipeline defining window size, overlap, and heuristics that prepare content for embedding and retrieval.</li> </ul>"},{"location":"terms/chunking/#examples","title":"Examples","text":"<p>Do - Tune chunk size and overlap per document type and validate with retrieval relevance tests.</p> <p>Don't - Use a single chunking strategy for code, policy docs, and conversations without benchmarking.</p>"},{"location":"terms/chunking/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: data_quality, privacy</li> <li>Risk notes: Poor chunking can duplicate sensitive data or break context needed for accurate, compliant answers.</li> </ul>"},{"location":"terms/chunking/#relationships","title":"Relationships","text":"<ul> <li>Broader: data preprocessing</li> <li>Narrower: semantic chunking</li> <li>Related: retrieval, vector store, reranking</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'chunking'.</p>"},{"location":"terms/chunking/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/chunking.yml</code></p>"},{"location":"terms/clip/","title":"clip","text":""},{"location":"terms/clip/#clip","title":"clip","text":"<p>Aliases: contrastive language-image pretraining, clip model Categories: Foundations, Retrieval &amp; RAG Roles: Engineering &amp; Platform, Data Science &amp; Research, Product &amp; Program Managers, Communications &amp; Enablement Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/clip/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> </ul>"},{"location":"terms/clip/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Add this concept to onboarding materials so teammates share a common baseline.</li> <li>Link supporting research or documentation in your internal wiki for deeper study.</li> <li>Validate retrieval quality using the evaluation guidance referenced in this entry.</li> <li>Ensure knowledge sources named here appear in your data governance inventory.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/clip/#short-definition","title":"Short definition","text":"<p>Multimodal model that embeds images and text into a shared space using contrastive learning.</p>"},{"location":"terms/clip/#long-definition","title":"Long definition","text":"<p>CLIP (Contrastive Language-Image Pretraining) jointly trains an image encoder and a text encoder so that semantically related images and captions map to nearby vectors. The approach uses large-scale image-text pairs scraped from the web and optimizes a contrastive loss that pushes matching pairs together while separating non-matching ones. Once trained, CLIP can perform zero-shot classification, retrieval, and multimodal search by comparing similarity between embeddings. Product teams leverage CLIP to improve content moderation, recommendation, and creative tooling without task-specific labels. Engineers integrate CLIP embeddings into vector stores or downstream fine-tuning pipelines, paying attention to bias, licensing, and safety constraints inherited from web-scale training data. Communications and policy teams monitor CLIP use because it can expose cultural biases and sensitive associations unless mitigated through filtering and evaluation.</p>"},{"location":"terms/clip/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: CLIP understands images and text together, letting you search or classify visuals using natural language.</li> <li>Engineer: Encode images and text with separate transformers trained via contrastive loss; use cosine similarity for retrieval, zero-shot classification, or RAG pipelines.</li> </ul>"},{"location":"terms/clip/#examples","title":"Examples","text":"<p>Do - Audit embeddings for demographic bias before deploying search or moderation features. - Cache CLIP embeddings and align them with domain-specific prompts to improve precision.</p> <p>Don't - Assume CLIP is license-clean; review dataset provenance and usage restrictions. - Ignore safety filters when exposing CLIP-powered features to end users.</p>"},{"location":"terms/clip/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: data_quality, transparency</li> <li>Risk notes: CLIP inherits web-scale bias and copyright concerns; record how outputs are filtered and evaluated.</li> </ul>"},{"location":"terms/clip/#relationships","title":"Relationships","text":"<ul> <li>Broader: embedding</li> <li>Related: retrieval, synthetic data, guardrails</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'clip'.</p>"},{"location":"terms/clip/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/clip.yml</code></p>"},{"location":"terms/confusion-matrix/","title":"confusion matrix","text":""},{"location":"terms/confusion-matrix/#confusion-matrix","title":"confusion matrix","text":"<p>Aliases: contingency table, error matrix Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers, Policy &amp; Risk Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/confusion-matrix/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> </ul>"},{"location":"terms/confusion-matrix/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Add this concept to onboarding materials so teammates share a common baseline.</li> <li>Link supporting research or documentation in your internal wiki for deeper study.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/confusion-matrix/#short-definition","title":"Short definition","text":"<p>Table that summarizes true/false positives and negatives to diagnose classification performance.</p>"},{"location":"terms/confusion-matrix/#long-definition","title":"Long definition","text":"<p>A confusion matrix breaks down classification results into true positives, true negatives, false positives, and false negatives. Visualizing performance in this way helps teams understand error patterns, detect class imbalance issues, and compute derived metrics such as precision, recall, specificity, and accuracy. Product managers use confusion matrices to explain trade-offs to stakeholders, while engineers leverage them to tune thresholds, resampling strategies, or loss functions. Policy and compliance reviewers rely on confusion matrix evidence to confirm that sensitive cohorts are not disproportionately affected. Confusion matrices are most informative when compared across slices\u2014by time, geography, or demographic attributes\u2014to catch drift and fairness concerns.</p>"},{"location":"terms/confusion-matrix/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: A confusion matrix shows where the AI is right or wrong so you can see which mistakes matter most.</li> <li>Engineer: Tabulate TP, TN, FP, FN counts; analyze row/column distributions and normalize per class to diagnose bias and guide mitigation.</li> </ul>"},{"location":"terms/confusion-matrix/#examples","title":"Examples","text":"<p>Do - Normalize confusion matrices to compare error rates across classes of different sizes. - Track confusion matrices over time to spot drift and guardrail regressions.</p> <p>Don't - Rely on aggregate accuracy without inspecting confusion patterns. - Hide confusion matrices from stakeholders\u2014they clarify trade-offs.</p>"},{"location":"terms/confusion-matrix/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, validity</li> <li>Risk notes: Ignoring confusion matrix signals can allow harmful error patterns to persist unnoticed.</li> </ul>"},{"location":"terms/confusion-matrix/#relationships","title":"Relationships","text":"<ul> <li>Broader: evaluation</li> <li>Related: precision, recall, roc auc</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'confusion matrix'.</p>"},{"location":"terms/confusion-matrix/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/confusion-matrix.yml</code></p>"},{"location":"terms/consent-management/","title":"consent management","text":""},{"location":"terms/consent-management/#consent-management","title":"consent management","text":"<p>Aliases: consent governance, user consent tracking Categories: Governance &amp; Risk Roles: Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Engineering &amp; Platform Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/consent-management/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/consent-management/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/consent-management/#short-definition","title":"Short definition","text":"<p>Practices that capture, honor, and audit user permissions across AI features.</p>"},{"location":"terms/consent-management/#long-definition","title":"Long definition","text":"<p>Consent management ensures AI systems only process data in ways users or data subjects have authorized. It covers how organizations collect consent, represent it in data pipelines, respect withdrawal requests, and surface disclosures in product experiences. Legal and policy teams define the consent taxonomy, product teams design user flows, and engineers propagate consent flags through storage, training, and inference systems. Mature programs monitor for drift and provide regulators with evidence that consent was honored. Weak consent management risks privacy violations, regulatory fines, and erosion of customer trust.</p>"},{"location":"terms/consent-management/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Treat consent as an auditable control so expansion plans avoid regulatory setbacks.</li> <li>Engineer: Propagate consent metadata end-to-end and block models from accessing disallowed data.</li> </ul>"},{"location":"terms/consent-management/#examples","title":"Examples","text":"<p>Do - Record granular consent states and expiration dates for every data subject. - Automate purge workflows when users revoke consent.</p> <p>Don't - Bundle broad AI permissions into generic terms-of-service text without explicit choice. - Retrofit consent flags after data has already trained foundational models.</p>"},{"location":"terms/consent-management/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: privacy, accountability, governance</li> <li>Risk notes: Ignoring consent boundaries invites regulatory penalties and reputational harm.</li> </ul>"},{"location":"terms/consent-management/#relationships","title":"Relationships","text":"<ul> <li>Broader: privacy, data minimization</li> <li>Related: impact mitigation plan, transparency report, risk register</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'consent management'.</p>"},{"location":"terms/consent-management/#citations","title":"Citations","text":"<ul> <li>Wikipedia \u2013 Data Minimisation</li> <li>ICO \u2013 Data Protection Guidance</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/consent-management.yml</code></p>"},{"location":"terms/constitutional-ai/","title":"constitutional ai","text":""},{"location":"terms/constitutional-ai/#constitutional-ai","title":"constitutional ai","text":"<p>Aliases: principle-guided alignment, self-critique alignment Categories: Governance &amp; Risk, LLM Core Roles: Product &amp; Program Managers, Engineering &amp; Platform, Policy &amp; Risk Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/constitutional-ai/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> </ul>"},{"location":"terms/constitutional-ai/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/constitutional-ai/#short-definition","title":"Short definition","text":"<p>Alignment approach where models critique and revise their own outputs against a written set of principles.</p>"},{"location":"terms/constitutional-ai/#long-definition","title":"Long definition","text":"<p>Constitutional AI trains or steers language models with a \"constitution\"\u2014explicit guidelines that describe desired and disallowed behaviors. During training or inference the model generates an answer, critiques it against the constitution, and revises the response to better satisfy those rules. Teams can combine human red teaming with model self-critique to iterate on the constitution, expanding coverage for safety, tone, or compliance requirements. Because principles are documented, product and policy partners can review, update, and audit them as new regulations emerge. Engineering groups wire the constitution into reinforcement learning loops, guardrail services, or inference-time post-processing. The technique complements human feedback and provides a transparent way to encode organizational values directly into model behavior.</p>"},{"location":"terms/constitutional-ai/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Constitutional AI makes our safety playbook explicit so models can self-correct toward company values.</li> <li>Engineer: Use principle prompts to drive self-critique loops or policy filters that nudge generations toward compliant behavior.</li> </ul>"},{"location":"terms/constitutional-ai/#examples","title":"Examples","text":"<p>Do - Version control the constitution and sync revisions with evaluation runs before deployment.</p> <p>Don't - Ship new principles without testing how they affect harmless user journeys.</p>"},{"location":"terms/constitutional-ai/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, validity, transparency</li> <li>Risk notes: Opaque or outdated constitutions create false confidence that safety requirements are being enforced.</li> </ul>"},{"location":"terms/constitutional-ai/#relationships","title":"Relationships","text":"<ul> <li>Broader: alignment</li> <li>Related: guardrails, red teaming, fine-tuning, temperature</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'constitutional ai'.</p>"},{"location":"terms/constitutional-ai/#citations","title":"Citations","text":"<ul> <li>Microsoft Responsible AI Overview</li> <li>Deepchecks AI Glossary</li> <li>AI Glossary &amp; Learning Hub</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/constitutional-ai.yml</code></p>"},{"location":"terms/content-moderation/","title":"content moderation","text":""},{"location":"terms/content-moderation/#content-moderation","title":"content moderation","text":"<p>Aliases: trust and safety, policy enforcement Categories: Governance &amp; Risk, Operations &amp; Monitoring Roles: Policy &amp; Risk, Communications &amp; Enablement, Product &amp; Program Managers, Security &amp; Trust, Engineering &amp; Platform Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p> <p>Put it into practice</p> <p>Reference the Governance Dashboard for monitoring obligations.</p>"},{"location":"terms/content-moderation/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/content-moderation/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Instrument dashboards or alerts that reflect the metrics highlighted in this definition.</li> <li>Update incident response or on-call runbooks with the glossary's do/don't scenarios.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/content-moderation/#short-definition","title":"Short definition","text":"<p>Workflows and tools that review, filter, and act on user-generated content to enforce policy.</p>"},{"location":"terms/content-moderation/#long-definition","title":"Long definition","text":"<p>Content moderation combines automation, human review, and escalation procedures to detect policy violations such as hate speech, harassment, misinformation, or disallowed imagery. AI systems often provide the first layer of moderation by classifying or scoring content for human queues, requiring careful tuning of precision/recall trade-offs. Policy teams define enforcement rules, while communications and legal stakeholders handle appeals and transparency reports. Engineering teams maintain moderation pipelines, logging, and guardrails; security teams ensure abuse detection remains resilient. Effective moderation programs rely on measurement, red-teaming, and incident response to adapt to adversarial users and evolving regulations.</p>"},{"location":"terms/content-moderation/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Content moderation protects users and the brand by keeping AI outputs and user posts within policy.</li> <li>Engineer: Blend classifiers, heuristic filters, and human review; monitor performance, appeals, and adversarial attempts.</li> </ul>"},{"location":"terms/content-moderation/#examples","title":"Examples","text":"<p>Do - Audit moderation models for bias against protected groups. - Publish user-facing guidelines and escalation paths.</p> <p>Don't - Rely solely on automation without human oversight for edge cases. - Ignore feedback loops when policies change.</p>"},{"location":"terms/content-moderation/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: risk_management, accountability</li> <li>Risk notes: Weak moderation exposes users to harm, invites regulatory fines, and erodes trust.</li> </ul>"},{"location":"terms/content-moderation/#relationships","title":"Relationships","text":"<ul> <li>Broader: guardrails</li> <li>Related: safety evaluation, incident response, algorithmic bias</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'content moderation'.</p>"},{"location":"terms/content-moderation/#citations","title":"Citations","text":"<ul> <li>European Commission \u2013 Content Moderation Policy</li> <li>NIST AI RMF Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/content-moderation.yml</code></p>"},{"location":"terms/context-window/","title":"context window","text":""},{"location":"terms/context-window/#context-window","title":"context window","text":"<p>Aliases: context length, sequence length limit Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/context-window/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/context-window/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/context-window/#short-definition","title":"Short definition","text":"<p>Maximum number of tokens a model can consider at once during prompting or inference.</p>"},{"location":"terms/context-window/#long-definition","title":"Long definition","text":"<p>The context window defines how many tokens a model can ingest in a single forward pass, combining both prompt input and generated output. Because transformer attention scales quadratically with sequence length, vendors ship models with fixed limits that balance cost, latency, and accuracy. Exceeding the window forces truncation or sliding-window strategies that can drop critical instructions. Product teams map context size to user scenarios\u2014for example, how long a document can be summarized or how many chat messages remain in memory. Engineers manage context budgets through prompt compression, retrieval chunking, and caching mechanisms like recurrent attention or stateful decoding. Governance stakeholders examine context policies to ensure sensitive data is not unintentionally persisted across conversations or logged beyond retention periods, especially when long windows capture personally identifiable information.</p>"},{"location":"terms/context-window/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: A context window is the attention span of the model\u2014go past it and instructions fall off.</li> <li>Engineer: Token budget for input plus output per request; drives attention memory use and dictates truncation strategies.</li> </ul>"},{"location":"terms/context-window/#examples","title":"Examples","text":"<p>Do - Document context requirements by user journey so prompts stay within safe token limits.</p> <p>Don't - Assume extending the context window eliminates the need for retrieval or prompt optimization.</p>"},{"location":"terms/context-window/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, privacy</li> <li>Risk notes: Oversized contexts can capture unnecessary personal data and complicate retention or deletion obligations.</li> </ul>"},{"location":"terms/context-window/#relationships","title":"Relationships","text":"<ul> <li>Broader: prompt engineering</li> <li>Narrower: long-context models</li> <li>Related: token, attention, kv cache</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'context window'.</p>"},{"location":"terms/context-window/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>NIST AI RMF Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/context-window.yml</code></p>"},{"location":"terms/cross-validation/","title":"cross-validation","text":""},{"location":"terms/cross-validation/#cross-validation","title":"cross-validation","text":"<p>Aliases: k-fold validation, cv Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/cross-validation/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/cross-validation/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Add this concept to onboarding materials so teammates share a common baseline.</li> <li>Link supporting research or documentation in your internal wiki for deeper study.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/cross-validation/#short-definition","title":"Short definition","text":"<p>Evaluation technique that splits data into multiple folds to estimate model performance on unseen samples.</p>"},{"location":"terms/cross-validation/#long-definition","title":"Long definition","text":"<p>Cross-validation repeatedly partitions a labeled dataset into complementary subsets to assess how a model generalizes. In k-fold cross-validation, the dataset is divided into k folds; the model trains on k-1 folds and validates on the remaining fold, cycling until every fold has served as the validation set. Aggregating results reduces variance compared with a single train-test split and surfaces instability caused by small datasets or class imbalance. Variants such as stratified, time-series, and leave-one-out cross-validation address specific domains. Product managers rely on cross-validation when comparing model candidates, while engineers use fold-level diagnostics to catch overfitting and data leakage before production. Governance teams view cross-validation artifacts as evidence that evaluation processes are robust and reproducible, particularly for regulated scenarios where a single split could mask risk.</p>"},{"location":"terms/cross-validation/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Cross-validation is the rehearsal that shows how a model behaves across different slices before real customers see it.</li> <li>Engineer: Partition data into k folds, train/validate across permutations, and average metrics to estimate generalization; track per-fold variance for risk analysis.</li> </ul>"},{"location":"terms/cross-validation/#examples","title":"Examples","text":"<p>Do - Use stratified folds when label imbalance could skew results. - Store per-fold metrics and random seeds for reproducibility.</p> <p>Don't - Leak validation data by reusing preprocessing steps fitted on the full dataset. - Rely on a single train/test split for high-stakes decisions.</p>"},{"location":"terms/cross-validation/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, transparency</li> <li>Risk notes: Skipping cross-validation invites optimistic bias and limits evidence required for audits or legal reviews.</li> </ul>"},{"location":"terms/cross-validation/#relationships","title":"Relationships","text":"<ul> <li>Broader: evaluation</li> <li>Related: overfitting, bias-variance tradeoff, model drift</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'cross-validation'.</p>"},{"location":"terms/cross-validation/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/cross-validation.yml</code></p>"},{"location":"terms/data-lineage/","title":"data lineage","text":""},{"location":"terms/data-lineage/#data-lineage","title":"data lineage","text":"<p>Aliases: lineage tracking, data provenance Categories: Operations &amp; Monitoring Roles: Engineering &amp; Platform, Data Science &amp; Research, Policy &amp; Risk, Legal &amp; Compliance Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/data-lineage/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> </ul>"},{"location":"terms/data-lineage/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Instrument dashboards or alerts that reflect the metrics highlighted in this definition.</li> <li>Update incident response or on-call runbooks with the glossary's do/don't scenarios.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/data-lineage/#short-definition","title":"Short definition","text":"<p>Traceable record of how data moves, transforms, and is used across AI systems.</p>"},{"location":"terms/data-lineage/#long-definition","title":"Long definition","text":"<p>Data lineage maps the lifecycle of data from ingestion through processing, training, evaluation, and deployment. It records sources, transformations, storage locations, and downstream consumers so teams can audit usage, troubleshoot quality issues, and respond to regulatory requests. Engineering automates lineage capture in pipelines, data scientists log transformations in notebooks, and policy teams use lineage to verify compliance with consent and retention rules. Effective lineage feeds governance dashboards, reminds owners to retire obsolete datasets, and documents which models rely on each asset. Without lineage, it is difficult to honor deletion requests, explain model behavior, or investigate incidents.</p>"},{"location":"terms/data-lineage/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Treat lineage as foundational infrastructure for audit readiness and trust.</li> <li>Engineer: Instrument pipelines to emit lineage metadata and expose it via searchable catalogs.</li> </ul>"},{"location":"terms/data-lineage/#examples","title":"Examples","text":"<p>Do - Link training datasets to their upstream sources and associated dataset cards. - Tag lineage records with retention and consent attributes.</p> <p>Don't - Rely on tribal knowledge to track where sensitive data flows. - Break lineage chains when copying data to ad hoc storage.</p>"},{"location":"terms/data-lineage/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, governance, risk_management</li> <li>Risk notes: Absent lineage hides policy violations and slows incident RCA.</li> </ul>"},{"location":"terms/data-lineage/#relationships","title":"Relationships","text":"<ul> <li>Broader: ml ops</li> <li>Related: consent management, dataset card, risk register</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'data lineage'.</p>"},{"location":"terms/data-lineage/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF \u2013 Data Management</li> <li>Google \u2013 Data Lineage for ML</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/data-lineage.yml</code></p>"},{"location":"terms/data-minimization/","title":"data minimization","text":""},{"location":"terms/data-minimization/#data-minimization","title":"data minimization","text":"<p>Aliases: data minimisation, minimal data collection Categories: Governance &amp; Risk Roles: Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/data-minimization/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/data-minimization/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/data-minimization/#short-definition","title":"Short definition","text":"<p>Principle of collecting and retaining only the data necessary for a defined purpose.</p>"},{"location":"terms/data-minimization/#long-definition","title":"Long definition","text":"<p>Data minimization limits the amount and duration of personal data collected, processed, or stored. Regulations such as GDPR and CCPA mandate this principle so organizations cannot hoard information \u201cjust in case.\u201d In AI projects, minimization applies to training datasets, prompt logs, telemetry, and derived embeddings. Product teams collaborate with legal and security to define clear purposes, expiration timelines, and deletion workflows. Engineers implement technical controls such as field-level encryption, redaction, and retention policies. Adoption of data minimization reduces breach impact, compliance risk, and model bias stemming from unnecessary sensitive attributes. Documenting minimization choices is also a prerequisite for privacy impact assessments and external audits.</p>"},{"location":"terms/data-minimization/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Data minimization keeps us from collecting more personal information than we need, reducing risk.</li> <li>Engineer: Implement least-data pipelines, redact sensitive fields, and enforce retention limits that map to documented business purposes.</li> </ul>"},{"location":"terms/data-minimization/#examples","title":"Examples","text":"<p>Do - Align prompts and telemetry retention with published privacy notices. - Automate deletion of training data that no longer supports the model\u2019s purpose.</p> <p>Don't - Copy entire databases into model pipelines without reviewing necessity. - Keep user data after opt-outs or contract end dates.</p>"},{"location":"terms/data-minimization/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: privacy, risk_management</li> <li>Risk notes: Excess data increases breach exposure, drives regulatory penalties, and can introduce bias into models.</li> </ul>"},{"location":"terms/data-minimization/#relationships","title":"Relationships","text":"<ul> <li>Broader: privacy</li> <li>Related: privacy impact assessment, differential privacy, incident response</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'data minimization'.</p>"},{"location":"terms/data-minimization/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>ICO \u2013 Data Minimisation Principle</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/data-minimization.yml</code></p>"},{"location":"terms/data-redaction/","title":"data redaction","text":""},{"location":"terms/data-redaction/#data-redaction","title":"data redaction","text":"<p>Aliases: pii redaction, sensitive data masking Categories: Governance &amp; Risk Roles: Engineering &amp; Platform, Policy &amp; Risk, Legal &amp; Compliance, Security &amp; Trust Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/data-redaction/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/data-redaction/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/data-redaction/#short-definition","title":"Short definition","text":"<p>Removal or masking of sensitive fields before data is stored, shared, or used for model training.</p>"},{"location":"terms/data-redaction/#long-definition","title":"Long definition","text":"<p>Data redaction removes or obfuscates sensitive information\u2014such as personal identifiers, secrets, or confidential business details\u2014before data is logged, retrained, or surfaced to analysts. Techniques include masking, hashing, token substitution, and selective deletion. Policy and legal teams define what must be redacted, engineers implement automated pipelines, and security verifies effectiveness through sampling and audits. Mature programs document redaction effectiveness, expose dashboards to auditors, and tie results to incident response drills. Redaction protects privacy, reduces breach impact, and supports compliance with consent and retention requirements. Incomplete redaction can leak sensitive information into training data or logs.</p>"},{"location":"terms/data-redaction/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Require automated redaction in telemetry and training pipelines to reduce regulatory exposure.</li> <li>Engineer: Build redaction into ingestion pipelines and confirm outputs with monitoring.</li> </ul>"},{"location":"terms/data-redaction/#examples","title":"Examples","text":"<p>Do - Redact PII at the edge before persisting chat transcripts. - Track redaction effectiveness with periodic manual reviews.</p> <p>Don't - Rely solely on manual processes to strip sensitive data. - Redact without documenting which fields are affected, confusing downstream users.</p>"},{"location":"terms/data-redaction/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: privacy, risk_management, security</li> <li>Risk notes: Poor redaction leaks sensitive data into models, logs, or analytics workflows.</li> </ul>"},{"location":"terms/data-redaction/#relationships","title":"Relationships","text":"<ul> <li>Broader: data minimization</li> <li>Related: consent management, data retention, privacy impact assessment</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'data redaction'.</p>"},{"location":"terms/data-redaction/#citations","title":"Citations","text":"<ul> <li>NIST Privacy Framework</li> <li>Google \u2013 Data Loss Prevention Redaction</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/data-redaction.yml</code></p>"},{"location":"terms/data-retention/","title":"data retention","text":""},{"location":"terms/data-retention/#data-retention","title":"data retention","text":"<p>Aliases: retention policy, data lifecycle Categories: Governance &amp; Risk Roles: Legal &amp; Compliance, Policy &amp; Risk, Security &amp; Trust, Product &amp; Program Managers Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p> <p>Put it into practice</p> <p>Map retention updates to the Governance Dashboard.</p>"},{"location":"terms/data-retention/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/data-retention/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/data-retention/#short-definition","title":"Short definition","text":"<p>Policies defining how long data is stored, where it lives, and how it is deleted.</p>"},{"location":"terms/data-retention/#long-definition","title":"Long definition","text":"<p>Data retention policies specify how long different data types are stored, why they are kept, where they reside, and how they are securely deleted. For AI systems, retention applies to raw inputs, training datasets, prompt logs, embeddings, and evaluation artifacts. Legal and policy teams determine retention limits based on regulations and contracts, while engineering and security implement technical controls such as automated deletion, encryption, and retention metadata. Product managers need visibility into retention rules to communicate with customers and plan features that rely on historical data. Documentation of retention schedules is often required for privacy impact assessments and audits.</p>"},{"location":"terms/data-retention/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Data retention makes sure we only keep the information we need, for as long as we\u2019re allowed.</li> <li>Engineer: Implement retention tags, automated deletion jobs, and secure archival; track compliance with privacy and contractual obligations.</li> </ul>"},{"location":"terms/data-retention/#examples","title":"Examples","text":"<p>Do - Document retention periods for each dataset used in training and inference. - Verify deletion workflows during audits and incident response drills.</p> <p>Don't - Keep prompt logs indefinitely without legal approval. - Mix data from different retention schedules in the same storage bucket.</p>"},{"location":"terms/data-retention/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: privacy, risk_management</li> <li>Risk notes: Ignoring retention requirements can lead to regulatory fines, litigation, and security incidents.</li> </ul>"},{"location":"terms/data-retention/#relationships","title":"Relationships","text":"<ul> <li>Broader: privacy</li> <li>Related: data minimization, privacy impact assessment, incident response</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'data retention'.</p>"},{"location":"terms/data-retention/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>NIST \u2013 Privacy Framework</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/data-retention.yml</code></p>"},{"location":"terms/dataset-card/","title":"dataset card","text":""},{"location":"terms/dataset-card/#dataset-card","title":"dataset card","text":"<p>Aliases: dataset documentation, data sheet Categories: Governance &amp; Risk Roles: Data Science &amp; Research, Policy &amp; Risk, Legal &amp; Compliance, Product &amp; Program Managers Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/dataset-card/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/dataset-card/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/dataset-card/#short-definition","title":"Short definition","text":"<p>Structured documentation describing a dataset\u2019s purpose, composition, risks, and usage constraints.</p>"},{"location":"terms/dataset-card/#long-definition","title":"Long definition","text":"<p>A dataset card records how and why data was collected, what populations it represents, known biases, licensing, and recommended uses. Inspired by model cards and \u201cdatasheets for datasets,\u201d the card gives reviewers the context needed to judge fitness for training or evaluation. Data scientists author technical details, policy and legal teams capture risk considerations, and product teams reference the card when scoping features. Keeping cards up to date enables auditors to trace claims back to source data and supports transparency commitments.</p>"},{"location":"terms/dataset-card/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Require dataset cards before approving new training data to avoid surprises later.</li> <li>Engineer: Link data pipelines and model training jobs back to the referenced card for traceability.</li> </ul>"},{"location":"terms/dataset-card/#examples","title":"Examples","text":"<p>Do - Document demographic coverage, labeling processes, and known limitations. - Include contact points for questions or takedown requests.</p> <p>Don't - Publish datasets without clarifying licensing or consent status. - Let cards drift after significant data updates.</p>"},{"location":"terms/dataset-card/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, governance, accountability</li> <li>Risk notes: Missing documentation obscures bias, licensing, and consent concerns tied to data.</li> </ul>"},{"location":"terms/dataset-card/#relationships","title":"Relationships","text":"<ul> <li>Broader: model governance</li> <li>Related: model card, consent management, transparency report</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'dataset card'.</p>"},{"location":"terms/dataset-card/#citations","title":"Citations","text":"<ul> <li>Datasheets for Datasets</li> <li>Stanford HAI \u2013 Dataset Cards</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/dataset-card.yml</code></p>"},{"location":"terms/decoding/","title":"decoding","text":""},{"location":"terms/decoding/#decoding","title":"decoding","text":"<p>Aliases: text decoding, generation decoding Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/decoding/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/decoding/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/decoding/#short-definition","title":"Short definition","text":"<p>Algorithms that turn model probability distributions into output tokens during generation.</p>"},{"location":"terms/decoding/#long-definition","title":"Long definition","text":"<p>Decoding encompasses the strategies used to convert a model\u2019s token probabilities into concrete outputs. Common methods include greedy decoding, beam search, top-k sampling, and nucleus (top-p) sampling, each trading off determinism, diversity, latency, and risk. Selection of a decoding algorithm affects user experience, safety posture, and evaluation results far more than many teams realize. Product managers configure decoding policies by use case\u2014for example, deterministic responses for support scenarios and higher-variance sampling for ideation tools. Engineers implement controls such as temperature scaling, repetition penalties, and logprob thresholds to manage failure modes like hallucinations or loops. Governance programs capture decoding settings in change logs because updates can materially alter risk assessments. Understanding decoding is foundational to building reliable guardrails, interpreting evaluation results, and communicating the behavior of generative systems to stakeholders.</p>"},{"location":"terms/decoding/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Decoding is the decision logic that turns the model\u2019s probabilities into the words customers see.</li> <li>Engineer: Apply algorithms like greedy, top-k, or top-p to sample from the softmax distribution while enforcing constraints and penalties.</li> </ul>"},{"location":"terms/decoding/#examples","title":"Examples","text":"<p>Do - Document decoding parameters alongside each release note for regulated experiences.</p> <p>Don't - Mix decoding strategies across channels without coordinating evaluation coverage.</p>"},{"location":"terms/decoding/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, robustness</li> <li>Risk notes: Untracked decoding changes can invalidate safety testing and confuse incident investigations.</li> </ul>"},{"location":"terms/decoding/#relationships","title":"Relationships","text":"<ul> <li>Broader: generative ai</li> <li>Narrower: greedy decoding, top-k sampling, top-p sampling</li> <li>Related: temperature, log probability, guardrails</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'decoding'.</p>"},{"location":"terms/decoding/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/decoding.yml</code></p>"},{"location":"terms/differential-privacy/","title":"differential privacy","text":""},{"location":"terms/differential-privacy/#differential-privacy","title":"differential privacy","text":"<p>Aliases: DP, epsilon-differential privacy Categories: Governance &amp; Risk Roles: Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Security &amp; Trust Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/differential-privacy/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/differential-privacy/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/differential-privacy/#short-definition","title":"Short definition","text":"<p>Mathematical framework that limits how much any single record influences published data or model outputs.</p>"},{"location":"terms/differential-privacy/#long-definition","title":"Long definition","text":"<p>Differential privacy protects individuals in a dataset by adding calibrated noise to statistics or training procedures so the presence or absence of any one person becomes indistinguishable. The framework is governed by privacy budgets\u2014epsilon and delta\u2014which quantify acceptable leakage. In AI systems, teams apply differential privacy when releasing analytics, training embeddings, or sharing evaluation datasets. Engineers integrate mechanisms like DP-SGD, Laplace noise, or randomized response, tracking accumulated budgets across queries. Legal and policy partners evaluate whether privacy guarantees meet regulatory requirements, especially when models ingest sensitive or regulated data. Security teams monitor for side-channel attacks that could combine multiple noisy outputs to infer personal information. Differential privacy is not a silver bullet; product and research groups balance utility loss against risk mitigation, document assumptions, and communicate residual exposure to stakeholders.</p>"},{"location":"terms/differential-privacy/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Differential privacy lets you learn from user data while keeping any single person unidentifiable.</li> <li>Engineer: Bound information leakage by injecting calibrated noise; manage cumulative epsilon/delta budgets across analytics or training steps.</li> </ul>"},{"location":"terms/differential-privacy/#examples","title":"Examples","text":"<p>Do - Track privacy budgets in dashboards so analysts know when to stop issuing queries. - Explain residual risk and utility trade-offs in launch documentation.</p> <p>Don't - Assume a single noisy release protects against repeated queries without monitoring budget depletion. - Mix differentially private and non-private data exports without clear labeling.</p>"},{"location":"terms/differential-privacy/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: privacy, risk_management</li> <li>Risk notes: Incorrect epsilon or budget accounting can create a false sense of protection and trigger regulatory exposure.</li> </ul>"},{"location":"terms/differential-privacy/#relationships","title":"Relationships","text":"<ul> <li>Broader: privacy</li> <li>Narrower: differentially private SGD</li> <li>Related: synthetic data, guardrails, model governance</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'differential privacy'.</p>"},{"location":"terms/differential-privacy/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/differential-privacy.yml</code></p>"},{"location":"terms/diffusion-model/","title":"diffusion model","text":""},{"location":"terms/diffusion-model/#diffusion-model","title":"diffusion model","text":"<p>Aliases: denoising diffusion model, score-based model Categories: Foundations, LLM Core Roles: Engineering &amp; Platform, Data Science &amp; Research, Product &amp; Program Managers, Communications &amp; Enablement Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/diffusion-model/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> </ul>"},{"location":"terms/diffusion-model/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Add this concept to onboarding materials so teammates share a common baseline.</li> <li>Link supporting research or documentation in your internal wiki for deeper study.</li> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/diffusion-model/#short-definition","title":"Short definition","text":"<p>Generative model that iteratively denoises random noise to synthesize images, audio, or other data.</p>"},{"location":"terms/diffusion-model/#long-definition","title":"Long definition","text":"<p>Diffusion models generate content by reversing a noising process. During training, clean data samples are progressively corrupted with Gaussian noise. The model learns to predict the noise at each step, effectively mapping from noisy inputs back to structure. During inference, the reverse process starts with pure noise and iteratively denoises toward a coherent sample over dozens or hundreds of timesteps. Diffusion models deliver state-of-the-art image and audio synthesis quality, control, and diversity compared with GANs or VAEs, but they can be computationally intensive. Product teams use guidance techniques, safety filters, and prompt engineering to align outputs with brand expectations. Engineers manage sampler choice, scheduler parameters, and hardware acceleration to meet latency targets. Governance stakeholders evaluate diffusion workflows for intellectual property, misinformation, and safety risks, since the models can produce realistic but fabricated content.</p>"},{"location":"terms/diffusion-model/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Diffusion models create images or audio by gradually refining random noise into something recognizable.</li> <li>Engineer: Train with forward noising and reverse denoising steps; deploy with schedulers (DDIM, Euler) and classifier-free guidance to control quality and speed.</li> </ul>"},{"location":"terms/diffusion-model/#examples","title":"Examples","text":"<p>Do - Log prompt, seed, and sampler metadata to reproduce outputs for audits. - Apply content moderation and watermarking to manage safety and attribution.</p> <p>Don't - Assume diffusion outputs are free from copyright or bias concerns. - Ignore the compute cost of small timestep adjustments on production workloads.</p>"},{"location":"terms/diffusion-model/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: risk_management, transparency</li> <li>Risk notes: Hyper-realistic outputs raise IP, misinformation, and safety challenges that must be documented and mitigated.</li> </ul>"},{"location":"terms/diffusion-model/#relationships","title":"Relationships","text":"<ul> <li>Broader: generative ai</li> <li>Related: synthetic data, guardrails, safety evaluation</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'diffusion model'.</p>"},{"location":"terms/diffusion-model/#citations","title":"Citations","text":"<ul> <li>Wikipedia AI Glossary</li> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/diffusion-model.yml</code></p>"},{"location":"terms/direct-preference-optimization/","title":"direct preference optimization","text":""},{"location":"terms/direct-preference-optimization/#direct-preference-optimization","title":"direct preference optimization","text":"<p>Aliases: dpo, preference optimization without rlhf Categories: LLM Core Roles: Engineering &amp; Platform, Data Science &amp; Research, Product &amp; Program Managers Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/direct-preference-optimization/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/direct-preference-optimization/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/direct-preference-optimization/#short-definition","title":"Short definition","text":"<p>Alignment technique that fine-tunes models directly on preference data without training a separate reward model.</p>"},{"location":"terms/direct-preference-optimization/#long-definition","title":"Long definition","text":"<p>Direct preference optimization (DPO) replaces the reward-model-and-RL step in traditional RLHF with a closed-form objective that pushes the model toward preferred responses. Using paired comparisons, it optimizes log-likelihood ratios so preferred outputs receive higher probability while disfavored ones decrease. DPO simplifies infrastructure and reduces instability caused by reinforcement learning. Engineers still need high-quality preference datasets and evaluation benchmarks, while policy teams confirm that annotator guidance reflects safety requirements. Poor datasets or hyperparameter choices can lead to overfitting, regressions, or policy drift, so monitoring remains critical.</p>"},{"location":"terms/direct-preference-optimization/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: DPO lowers alignment overhead but still requires tight governance on data quality and evaluations.</li> <li>Engineer: Track win rates and safety metrics before and after DPO to ensure improvements hold.</li> </ul>"},{"location":"terms/direct-preference-optimization/#examples","title":"Examples","text":"<p>Do - Re-run safety and quality benchmarks after each optimization pass. - Blend DPO with instruction tuning refreshes to keep behavior current.</p> <p>Don't - Assume DPO eliminates the need for human review or reward analyses. - Train on preference data without validating annotator consistency.</p>"},{"location":"terms/direct-preference-optimization/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: measurement, risk_management</li> <li>Risk notes: Misaligned preference data can push the model toward unsafe behavior even without RL instabilities.</li> </ul>"},{"location":"terms/direct-preference-optimization/#relationships","title":"Relationships","text":"<ul> <li>Broader: reinforcement learning from human feedback</li> <li>Related: preference dataset, instruction tuning, reward model</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'direct preference optimization'.</p>"},{"location":"terms/direct-preference-optimization/#citations","title":"Citations","text":"<ul> <li>Stanford \u2013 Direct Preference Optimization</li> <li>Direct Preference Optimization (OpenReview)</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/direct-preference-optimization.yml</code></p>"},{"location":"terms/embedding/","title":"embedding","text":""},{"location":"terms/embedding/#embedding","title":"embedding","text":"<p>Aliases: vector embedding, representation vector Categories: Retrieval &amp; RAG Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/embedding/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/embedding/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Validate retrieval quality using the evaluation guidance referenced in this entry.</li> <li>Ensure knowledge sources named here appear in your data governance inventory.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/embedding/#short-definition","title":"Short definition","text":"<p>Dense numerical representation that captures semantic meaning of text, images, or other data.</p>"},{"location":"terms/embedding/#long-definition","title":"Long definition","text":"<p>Embeddings map pieces of information\u2014such as words, sentences, images, or audio\u2014into dense vectors so similar concepts cluster in vector space. Modern foundation models learn embeddings during pretraining, optimizing them to capture contextual meaning, syntactic roles, and domain signals that make downstream tasks like retrieval, classification, and recommendation more effective. When teams build retrieval-augmented generation systems, they compute embeddings for documents and queries, then compare them with similarity metrics like cosine distance or dot product. The quality of those vectors influences whether the right context is retrieved, how well clustering and anomaly detection work, and how fast nearest-neighbor searches execute. Governance programs scrutinize embedding pipelines because biased or outdated vectors can encode harmful associations and propagate them into products. Maintaining embeddings thus requires dataset curation, versioning, and monitoring to detect drift as language or policies change.</p>"},{"location":"terms/embedding/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Embeddings are a way to translate concepts into numbers so the system can find and compare ideas efficiently.</li> <li>Engineer: Model-generated vectors where distance metrics approximate semantic similarity; used for retrieval, clustering, ranking, and transfer learning.</li> </ul>"},{"location":"terms/embedding/#examples","title":"Examples","text":"<p>Do - Version embedding models and store associated tokenizer metadata to support reproducibility.</p> <p>Don't - Mix embeddings from different models without recalculating similarity thresholds or rescoring.</p>"},{"location":"terms/embedding/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: data_quality, accountability</li> <li>Risk notes: Embedding drift or biased training data can surface discriminatory outputs when used in search or ranking.</li> </ul>"},{"location":"terms/embedding/#relationships","title":"Relationships","text":"<ul> <li>Broader: representation learning</li> <li>Narrower: sentence embedding, image embedding</li> <li>Related: retrieval, vector store, chunking</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'embedding'.</p>"},{"location":"terms/embedding/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>NIST AI RMF Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/embedding.yml</code></p>"},{"location":"terms/escalation-policy/","title":"escalation policy","text":""},{"location":"terms/escalation-policy/#escalation-policy","title":"escalation policy","text":"<p>Aliases: human escalation policy, handoff policy Categories: Governance &amp; Risk Roles: Product &amp; Program Managers, Policy &amp; Risk, Security &amp; Trust, Engineering &amp; Platform Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/escalation-policy/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/escalation-policy/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/escalation-policy/#short-definition","title":"Short definition","text":"<p>Playbook that defines when and how AI systems route control to human reviewers.</p>"},{"location":"terms/escalation-policy/#long-definition","title":"Long definition","text":"<p>An escalation policy documents the conditions that trigger human intervention during automated workflows. For AI systems, it specifies risk thresholds, user signals, compliance events, and operational failures that require a person to review or take over. The policy names accountable roles, time-to-response expectations, and communication paths so incidents resolve quickly. Product teams embed escalation hooks in UX flows, engineering implements the routing logic, and security or policy leaders ensure the policy covers legal and ethical obligations. Without a maintained policy, escalations become ad hoc, increasing the chance that sensitive actions remain unreviewed or stuck in queues.</p>"},{"location":"terms/escalation-policy/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Use escalation policies to prove that sensitive AI decisions receive timely human scrutiny.</li> <li>Engineer: Wire instrumentation that triggers the policy reliably and logs every escalation for auditing.</li> </ul>"},{"location":"terms/escalation-policy/#examples","title":"Examples","text":"<p>Do - Define severity tiers with maximum response windows for each reviewer group. - Test escalation paths during chaos exercises to confirm alerts reach on-call owners.</p> <p>Don't - Rely on verbal agreements about who will intervene when policies fire. - Allow blocked escalations to sit without rerouting or notifying backups.</p>"},{"location":"terms/escalation-policy/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: governance, monitoring, risk_management</li> <li>Risk notes: Undefined escalation paths leave high-impact failures unreviewed and erode regulatory trust.</li> </ul>"},{"location":"terms/escalation-policy/#relationships","title":"Relationships","text":"<ul> <li>Broader: ai incident response, guardrail policy</li> <li>Related: human handoff, risk register, safety spec</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'escalation policy'.</p>"},{"location":"terms/escalation-policy/#citations","title":"Citations","text":"<ul> <li>Microsoft \u2013 AI Safety Guidance</li> <li>Partnership on AI \u2013 Responsible AI for Customer Service</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/escalation-policy.yml</code></p>"},{"location":"terms/evaluation-harness/","title":"evaluation harness","text":""},{"location":"terms/evaluation-harness/#evaluation-harness","title":"evaluation harness","text":"<p>Aliases: eval harness, agent evaluation pipeline Categories: Operations &amp; Monitoring Roles: Engineering &amp; Platform, Data Science &amp; Research, Product &amp; Program Managers Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/evaluation-harness/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/evaluation-harness/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Instrument dashboards or alerts that reflect the metrics highlighted in this definition.</li> <li>Update incident response or on-call runbooks with the glossary's do/don't scenarios.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/evaluation-harness/#short-definition","title":"Short definition","text":"<p>Automated pipeline that replays tasks, scores outputs, and reports regressions for AI systems.</p>"},{"location":"terms/evaluation-harness/#long-definition","title":"Long definition","text":"<p>An evaluation harness packages datasets, prompts, grading logic, and reporting into a repeatable testing loop for AI systems. Teams use it to replay golden tasks, compare new model checkpoints against baselines, and surface regressions before releases. Mature harnesses capture qualitative rubrics, automated metrics, and policy checks in the same run so results are auditable. Product managers wire the harness into launch gates, engineers integrate it with CI/CD, and data scientists own the benchmarks and scoring logic. When paired with incident telemetry, the harness becomes the source of truth for whether mitigations are holding. Without one, evaluation drifts to ad-hoc notebook experiments, making safety, equity, and quality decisions opaque.</p>"},{"location":"terms/evaluation-harness/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Tie model releases to a standard testing loop so you can watch risk and quality trends across launches.</li> <li>Engineer: Embed the harness into CI so every model or prompt change ships with quantitative and policy signals.</li> </ul>"},{"location":"terms/evaluation-harness/#examples","title":"Examples","text":"<p>Do - Schedule nightly harness runs against red-team prompts and archive reports in the governance dashboard. - Track score deltas by dataset slice so regressions in protected classes trigger blocking alerts.</p> <p>Don't - Rely on one-off notebooks or manual spot checks to validate safety-critical behaviors. - Ignore harness failures when product metrics trend up; that hides governance or fairness regressions.</p>"},{"location":"terms/evaluation-harness/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: measurement, monitoring, risk_management</li> <li>Risk notes: Skipping automation makes it impossible to prove controls work or to show due diligence during audits.</li> </ul>"},{"location":"terms/evaluation-harness/#relationships","title":"Relationships","text":"<ul> <li>Broader: evaluation, ml ops</li> <li>Narrower: robust prompting, synthetic data evaluation</li> <li>Related: safety evaluation, red teaming, responsible ai</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'evaluation harness'.</p>"},{"location":"terms/evaluation-harness/#citations","title":"Citations","text":"<ul> <li>Microsoft \u2013 AI Red Team Guide</li> <li>Wikipedia \u2013 Test Harness</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/evaluation-harness.yml</code></p>"},{"location":"terms/evaluation/","title":"evaluation","text":""},{"location":"terms/evaluation/#evaluation","title":"evaluation","text":"<p>Aliases: model evaluation, AI evaluation Categories: Operations &amp; Monitoring, Governance &amp; Risk Roles: Communications &amp; Enablement, Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/evaluation/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/evaluation/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Instrument dashboards or alerts that reflect the metrics highlighted in this definition.</li> <li>Update incident response or on-call runbooks with the glossary's do/don't scenarios.</li> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/evaluation/#short-definition","title":"Short definition","text":"<p>Systematic measurement of model performance, safety, and reliability using defined tests.</p>"},{"location":"terms/evaluation/#long-definition","title":"Long definition","text":"<p>Evaluation is the disciplined practice of testing AI systems against quantitative and qualitative criteria before and after deployment. It extends beyond accuracy metrics to encompass robustness, bias detection, factual correctness, latency, and safety stress tests such as red teaming or jailbreak attempts. Teams build eval suites that blend automated metrics\u2014like BLEU, accuracy@k, or toxicity scores\u2014with human review checklists tailored to critical user journeys. Continuous evaluation supports regression detection when prompts, datasets, or infrastructure change. Governance frameworks treat evaluations as audit artifacts: they document assumptions, thresholds, and sign-offs required before promoting models to production. Mature programs integrate evaluation pipelines into CI/CD, enabling reproducibility and traceability. Without rigorous evaluation, organizations cannot credibly claim their models meet compliance obligations or user expectations.</p>"},{"location":"terms/evaluation/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Evaluation is our quality gate\u2014it proves the AI delivers safe, reliable outcomes before we launch.</li> <li>Engineer: Automated and human-in-the-loop test harnesses measuring task metrics, robustness, bias, and safety across model releases.</li> </ul>"},{"location":"terms/evaluation/#examples","title":"Examples","text":"<p>Do - Run targeted red-team scenarios alongside quantitative metrics before shipping new prompts or fine-tuned models.</p> <p>Don't - Rely on a single aggregate score without examining subgroup performance or qualitative feedback.</p>"},{"location":"terms/evaluation/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, accountability</li> <li>Risk notes: Skipping or weakening evaluations increases the likelihood of undetected harmful behaviors in production.</li> </ul>"},{"location":"terms/evaluation/#relationships","title":"Relationships","text":"<ul> <li>Broader: model governance</li> <li>Narrower: safety evaluation, capability evaluation</li> <li>Related: guardrails, alignment, red teaming</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'evaluation'.</p>"},{"location":"terms/evaluation/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Google ML Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/evaluation.yml</code></p>"},{"location":"terms/f1-score/","title":"f1 score","text":""},{"location":"terms/f1-score/#f1-score","title":"f1 score","text":"<p>Aliases: f-score, harmonic mean of precision and recall Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/f1-score/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/f1-score/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Add this concept to onboarding materials so teammates share a common baseline.</li> <li>Link supporting research or documentation in your internal wiki for deeper study.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/f1-score/#short-definition","title":"Short definition","text":"<p>Harmonic mean of precision and recall, balancing false positives and false negatives.</p>"},{"location":"terms/f1-score/#long-definition","title":"Long definition","text":"<p>The F1 score combines precision and recall into a single metric by calculating their harmonic mean. It is especially useful when positive class distribution is imbalanced and teams need a balanced view of false positives and false negatives. An F1 score close to 1.0 indicates strong performance on both metrics, while low values signal trade-offs that require attention. Product managers use F1 to compare model variants quickly, and engineers track the score across cohorts to detect regressions after retraining or threshold changes. Because F1 weights precision and recall equally, decision-makers should confirm that assumption matches business priorities; otherwise metrics like F-beta or cost-based evaluation may be more appropriate.</p>"},{"location":"terms/f1-score/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: F1 score summarizes how well the AI avoids both false alarms and missed detections.</li> <li>Engineer: 2 * (precision * recall) / (precision + recall); monitor per-class F1 and consider F-beta when recall or precision matters more.</li> </ul>"},{"location":"terms/f1-score/#examples","title":"Examples","text":"<p>Do - Report F1 alongside precision, recall, and confusion matrices for context. - Track F1 trends after model updates to catch regressions early.</p> <p>Don't - Rely on F1 when business costs heavily favor precision or recall. - Compare F1 across datasets without consistent class distributions.</p>"},{"location":"terms/f1-score/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, transparency</li> <li>Risk notes: Misinterpreting F1 can hide risk trade-offs; document why the weight between precision and recall fits the use case.</li> </ul>"},{"location":"terms/f1-score/#relationships","title":"Relationships","text":"<ul> <li>Broader: evaluation</li> <li>Related: precision, recall, confusion matrix</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'f1 score'.</p>"},{"location":"terms/f1-score/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/f1-score.yml</code></p>"},{"location":"terms/fairness-metrics/","title":"fairness metrics","text":""},{"location":"terms/fairness-metrics/#fairness-metrics","title":"fairness metrics","text":"<p>Aliases: fairness measures, fairness evaluation Categories: Governance &amp; Risk, Operations &amp; Monitoring Roles: Policy &amp; Risk, Legal &amp; Compliance, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p> <p>Put it into practice</p> <p>Coordinate with the Role Starter Packs for governance actions.</p>"},{"location":"terms/fairness-metrics/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/fairness-metrics/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Instrument dashboards or alerts that reflect the metrics highlighted in this definition.</li> <li>Update incident response or on-call runbooks with the glossary's do/don't scenarios.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/fairness-metrics/#short-definition","title":"Short definition","text":"<p>Quantitative measures that evaluate whether model performance is equitable across groups.</p>"},{"location":"terms/fairness-metrics/#long-definition","title":"Long definition","text":"<p>Fairness metrics quantify disparities between demographic or contextual groups. Common measures include demographic parity, equalized odds, equal opportunity, predictive parity, and subgroup-specific precision/recall. Teams apply these metrics during model evaluation, post-deployment monitoring, and incident response to detect fairness regressions. Choosing the right metric depends on regulatory requirements and domain priorities; not all metrics can be optimized simultaneously. Policy and legal stakeholders define fairness thresholds, while engineers build pipelines that compute metrics for each release and in production dashboards. Findings feed into model cards, governance reviews, and mitigation plans.</p>"},{"location":"terms/fairness-metrics/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Fairness metrics tell us whether different groups get comparable outcomes from the AI.</li> <li>Engineer: Compute metrics such as equalized odds, statistical parity difference, or subgroup ROC; monitor trends across releases and production.</li> </ul>"},{"location":"terms/fairness-metrics/#examples","title":"Examples","text":"<p>Do - Align fairness thresholds with business and regulatory requirements. - Track fairness metrics alongside traditional accuracy metrics in CI/CD.</p> <p>Don't - Rely on a single fairness metric without understanding trade-offs. - Evaluate overall fairness without subgroup data or feedback from impacted communities.</p>"},{"location":"terms/fairness-metrics/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: fairness, transparency</li> <li>Risk notes: Failure to monitor fairness metrics can lead to discrimination, legal liability, and loss of trust.</li> </ul>"},{"location":"terms/fairness-metrics/#relationships","title":"Relationships","text":"<ul> <li>Broader: algorithmic bias</li> <li>Related: model interpretability, evaluation, model card</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'fairness metrics'.</p>"},{"location":"terms/fairness-metrics/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Wikipedia \u2013 Fairness in Machine Learning</li> <li>Google ML Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/fairness-metrics.yml</code></p>"},{"location":"terms/feature-engineering/","title":"feature engineering","text":""},{"location":"terms/feature-engineering/#feature-engineering","title":"feature engineering","text":"<p>Aliases: feature design, feature extraction Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/feature-engineering/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/feature-engineering/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Add this concept to onboarding materials so teammates share a common baseline.</li> <li>Link supporting research or documentation in your internal wiki for deeper study.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/feature-engineering/#short-definition","title":"Short definition","text":"<p>Transforming raw data into model-ready features that improve signal, fairness, and maintainability.</p>"},{"location":"terms/feature-engineering/#long-definition","title":"Long definition","text":"<p>Feature engineering covers the techniques used to convert raw logs, text, images, or structured records into inputs a model can learn from. It spans cleaning, normalization, aggregation, embedding, and generation of domain-specific indicators. Thoughtful feature engineering improves generalization, reduces noise, and encodes policy guardrails early in the pipeline. Data scientists prototype feature pipelines, engineers productionize them with monitoring and lineage, and product teams align feature choices with user journeys and governance constraints. Documented feature logic also accelerates audits and root cause analysis when behaviour shifts in production.</p>"},{"location":"terms/feature-engineering/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Feature engineering is how we turn messy data into reliable signals the model can use.</li> <li>Engineer: Version feature code, validate statistical drift, and partner with governance to review sensitive attributes.</li> </ul>"},{"location":"terms/feature-engineering/#examples","title":"Examples","text":"<p>Do - Log transformations and scaling choices alongside the model so they stay in sync across environments. - Run fairness and privacy checks when introducing features derived from sensitive sources.</p> <p>Don't - Ship handcrafted features without automated tests or lineage traces. - Overfit by generating thousands of highly specific features with no regularization plan.</p>"},{"location":"terms/feature-engineering/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: data_quality, privacy</li> <li>Risk notes: Opaque feature pipelines hide bias, privacy violations, and drift triggers, making incidents harder to manage.</li> </ul>"},{"location":"terms/feature-engineering/#relationships","title":"Relationships","text":"<ul> <li>Related: training data, regularization, bias-variance tradeoff, ml ops</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'feature engineering'.</p>"},{"location":"terms/feature-engineering/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>NIST AI Risk Management Framework</li> <li>Microsoft Responsible AI Standard</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/feature-engineering.yml</code></p>"},{"location":"terms/fine-tuning/","title":"fine-tuning","text":""},{"location":"terms/fine-tuning/#fine-tuning","title":"fine-tuning","text":"<p>Aliases: model adaptation, supervised fine-tuning Categories: Optimization &amp; Efficiency Roles: Data Science &amp; Research, Engineering &amp; Platform Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/fine-tuning/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/fine-tuning/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Record before-and-after performance metrics when applying this optimisation technique.</li> <li>Document trade-offs for product and policy partners using the glossary's language.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/fine-tuning/#short-definition","title":"Short definition","text":"<p>Additional training that adapts a pretrained model to a specific task or domain.</p>"},{"location":"terms/fine-tuning/#long-definition","title":"Long definition","text":"<p>Fine-tuning continues training from a pretrained checkpoint using a curated dataset that reflects the target task, tone, or policy. By adjusting weights on top of broad foundation knowledge, teams achieve better accuracy and alignment than prompt engineering alone. Approaches include supervised fine-tuning, reinforcement learning from human feedback, parameter-efficient methods like LoRA, and combinations with synthetic data generation. Product leaders plan fine-tuning roadmaps to differentiate experiences or enforce brand voice, while engineers manage hyperparameters, data balancing, and evaluation suites to prevent catastrophic forgetting. Governance stakeholders scrutinize fine-tuning inputs for licensing, privacy, and bias risks, requiring documentation of provenance and review sign-offs. Because fine-tuned models can drift from base guarantees, organizations version checkpoints, run regression tests, and maintain rollback plans to satisfy compliance obligations and operational reliability.</p>"},{"location":"terms/fine-tuning/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Fine-tuning teaches a general model to speak in the organization\u2019s voice and handle domain-specific tasks.</li> <li>Engineer: Continue training a pretrained model on labeled or preference data, tracking hyperparameters, evals, and release packaging.</li> </ul>"},{"location":"terms/fine-tuning/#examples","title":"Examples","text":"<p>Do - Store data lineage and evaluation results for every fine-tuned checkpoint before deployment.</p> <p>Don't - Blend proprietary and open datasets without clarifying licenses and usage rights.</p>"},{"location":"terms/fine-tuning/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, validity</li> <li>Risk notes: Uncontrolled fine-tuning can override safety mitigations or introduce licensed data without traceability.</li> </ul>"},{"location":"terms/fine-tuning/#relationships","title":"Relationships","text":"<ul> <li>Broader: model training</li> <li>Narrower: low-rank adaptation, reinforcement learning from human feedback</li> <li>Related: knowledge distillation, evaluation, alignment</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'fine-tuning'.</p>"},{"location":"terms/fine-tuning/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>NIST AI RMF Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/fine-tuning.yml</code></p>"},{"location":"terms/function-calling/","title":"function calling","text":""},{"location":"terms/function-calling/#function-calling","title":"function calling","text":"<p>Aliases: tool calling, structured output invocation Categories: Agents &amp; Tooling, LLM Core Roles: Engineering &amp; Platform, Product &amp; Program Managers, Data Science &amp; Research Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/function-calling/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> </ul>"},{"location":"terms/function-calling/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Audit exposed tools against the safeguards described and document approval paths.</li> <li>Test hand-offs with human reviewers to confirm the safety expectations captured here are met.</li> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/function-calling/#short-definition","title":"Short definition","text":"<p>LLM capability that lets prompts invoke predefined functions and return structured arguments.</p>"},{"location":"terms/function-calling/#long-definition","title":"Long definition","text":"<p>Function calling bridges natural language prompts and executable tools. Developers describe approved functions\u2014such as retrieval queries, database lookups, or workflow automations\u2014in a schema the model can reason about. During inference the model decides whether to call a function, fills the arguments with structured JSON, and uses the tool's response to continue the conversation. Product teams rely on function calling to chain RAG pipelines, trigger actions, or enforce form validation while keeping humans in the loop for sensitive steps. Robust implementations include guardrails on which functions are exposed, argument validation, and logging so security and compliance teams can audit usage. When paired with agent frameworks, function calling becomes the backbone for orchestrating multi-step tasks.</p>"},{"location":"terms/function-calling/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Function calling lets assistants take reliable actions by routing model suggestions into vetted business APIs.</li> <li>Engineer: Expose idempotent, well-typed functions, validate arguments, and loop tool responses back into the conversation state.</li> </ul>"},{"location":"terms/function-calling/#examples","title":"Examples","text":"<p>Do - Start with read-only functions and add write access only after logging and approval workflows exist.</p> <p>Don't - Expose unrestricted shell commands or production secrets as callable functions.</p>"},{"location":"terms/function-calling/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, transparency</li> <li>Risk notes: Unrestricted function catalogs can let models trigger unsafe actions without audit trails.</li> </ul>"},{"location":"terms/function-calling/#relationships","title":"Relationships","text":"<ul> <li>Broader: tool-use</li> <li>Related: agentic ai, system prompt, retrieval-augmented generation, guardrails</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'function calling'.</p>"},{"location":"terms/function-calling/#citations","title":"Citations","text":"<ul> <li>Mistral Docs \u2013 Function Calling</li> <li>GitHub \u2013 OpenAI Cookbook: How to call functions with chat models</li> <li>LangChain Documentation Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/function-calling.yml</code></p>"},{"location":"terms/generalization/","title":"generalization","text":""},{"location":"terms/generalization/#generalization","title":"generalization","text":"<p>Aliases: generalisation, out-of-sample performance Categories: Foundations Roles: Product &amp; Program Managers, Engineering &amp; Platform, Data Science &amp; Research Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/generalization/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> </ul>"},{"location":"terms/generalization/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Add this concept to onboarding materials so teammates share a common baseline.</li> <li>Link supporting research or documentation in your internal wiki for deeper study.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/generalization/#short-definition","title":"Short definition","text":"<p>Model's ability to sustain performance on unseen data rather than memorising the training set.</p>"},{"location":"terms/generalization/#long-definition","title":"Long definition","text":"<p>Generalization determines whether a model delivers value once it leaves the lab. A system that overfits will look strong on training data but collapse when it meets new markets, languages, or customer behaviours. Engineering teams manage generalization by pairing representative train/validation/test splits with regularization, augmentation, and drift monitoring. Product and policy partners translate generalization risk into business and compliance exposure: degraded accuracy erodes customer trust, introduces bias, and can violate contractual or regulatory commitments. Healthy generalization comes from both technical controls and operational hygiene: clear loss functions, reproducible experiments, and post-deployment telemetry that surfaces when reality shifts away from the training distribution.</p>"},{"location":"terms/generalization/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Generalization shows whether the model keeps its promises once it meets real customers and scenarios.</li> <li>Engineer: Track performance on held-out and in-production slices, tune regularization, and trigger retraining when drift appears.</li> </ul>"},{"location":"terms/generalization/#examples","title":"Examples","text":"<p>Do - Hold back a realistic test set and monitor post-launch telemetry to confirm generalization. - Log distribution shifts and retrain when key features move away from training baselines.</p> <p>Don't - Ship models based solely on training metrics or synthetic benchmarks. - Ignore subgroup breakdowns that reveal poor generalization for protected or high-value cohorts.</p>"},{"location":"terms/generalization/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, robustness</li> <li>Risk notes: Poor generalization is a leading indicator of bias, drift, and safety incidents, so governance reviews demand evidence of monitoring.</li> </ul>"},{"location":"terms/generalization/#relationships","title":"Relationships","text":"<ul> <li>Broader: evaluation</li> <li>Related: bias-variance tradeoff, cross-validation, regularization, model drift</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'generalization'.</p>"},{"location":"terms/generalization/#citations","title":"Citations","text":"<ul> <li>Deep Learning (MIT Press)</li> <li>NIST AI Risk Management Framework</li> <li>Hugging Face Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/generalization.yml</code></p>"},{"location":"terms/generative-ai/","title":"generative ai","text":""},{"location":"terms/generative-ai/#generative-ai","title":"generative ai","text":"<p>Aliases: genai, generative artificial intelligence Categories: Foundations Roles: Communications &amp; Enablement, Policy &amp; Risk, Product &amp; Program Managers Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/generative-ai/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/generative-ai/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Add this concept to onboarding materials so teammates share a common baseline.</li> <li>Link supporting research or documentation in your internal wiki for deeper study.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/generative-ai/#short-definition","title":"Short definition","text":"<p>Family of models that produce new content\u2014text, images, code\u2014rather than only making predictions.</p>"},{"location":"terms/generative-ai/#long-definition","title":"Long definition","text":"<p>Generative AI refers to systems that create novel outputs such as text, images, audio, or code by learning patterns from large training corpora. Foundation models like GPT, diffusion models, and multimodal architectures power these experiences by predicting the next token or pixel based on context. Organizations adopt generative AI for drafting, summarization, design, simulation, and synthetic data generation. The technology amplifies creativity and productivity but introduces unique risks, including hallucinations, intellectual property exposure, and safety concerns. Product leaders evaluate generative AI opportunities alongside guardrails, retrieval augmentation, and review workflows to keep outputs trustworthy. Engineers maintain the pipelines that tokenize data, manage prompts, deploy models, and monitor usage at scale. Governance teams coordinate policies covering responsible use, content moderation, privacy, and transparency. Understanding generative AI provides the context for more specialized concepts in this glossary, from attention mechanisms to red teaming.</p>"},{"location":"terms/generative-ai/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Generative AI is the class of systems that draft content on demand, automating creative and analytical tasks.</li> <li>Engineer: Models that learn probability distributions over complex data and sample from them to produce new artifacts.</li> </ul>"},{"location":"terms/generative-ai/#examples","title":"Examples","text":"<p>Do - Pair generative AI deployments with clear disclosure and feedback channels for users.</p> <p>Don't - Launch generative features without documenting data sources, evaluation, and risk mitigations.</p>"},{"location":"terms/generative-ai/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, risk_management</li> <li>Risk notes: Uncontrolled generative systems can leak sensitive data, create misleading content, or breach intellectual property.</li> </ul>"},{"location":"terms/generative-ai/#relationships","title":"Relationships","text":"<ul> <li>Broader: artificial intelligence</li> <li>Narrower: retrieval-augmented generation, hallucination, decoding</li> <li>Related: attention, prompt engineering, guardrails</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'generative ai'.</p>"},{"location":"terms/generative-ai/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/generative-ai.yml</code></p>"},{"location":"terms/gradient-descent/","title":"gradient descent","text":""},{"location":"terms/gradient-descent/#gradient-descent","title":"gradient descent","text":"<p>Aliases: steepest descent, batch gradient descent Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/gradient-descent/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/gradient-descent/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Add this concept to onboarding materials so teammates share a common baseline.</li> <li>Link supporting research or documentation in your internal wiki for deeper study.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/gradient-descent/#short-definition","title":"Short definition","text":"<p>Iterative optimization algorithm that updates model parameters in the direction of the negative gradient to minimize a loss function.</p>"},{"location":"terms/gradient-descent/#long-definition","title":"Long definition","text":"<p>Gradient descent is the workhorse optimization routine behind many machine learning models. The algorithm measures how the loss function changes with respect to each parameter, then nudges those parameters in the direction that most quickly reduces the loss. Step size is controlled by the learning rate, and the procedure repeats until convergence or an early stopping rule triggers. Product teams see the effects of gradient descent in training curves and quality improvements, engineers focus on stability and runtime characteristics, and data scientists tune batch size, momentum, and learning rate schedules to balance accuracy with compute budget. Understanding gradient descent is critical when diagnosing training instability, bias amplification, or regressions after feature changes because it exposes how the model is navigating its optimization landscape.</p>"},{"location":"terms/gradient-descent/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Gradient descent is the routine that steadily adjusts model knobs until errors shrink.</li> <li>Engineer: Compute parameter updates using grad_theta L(theta); tune learning rate, batching, and momentum to ensure stable convergence.</li> </ul>"},{"location":"terms/gradient-descent/#examples","title":"Examples","text":"<p>Do - Monitor training and validation loss together to catch divergence early. - Scale learning rates with batch size when moving between GPU configurations.</p> <p>Don't - Ignore exploding gradients without adding clipping or schedule adjustments. - Assume a single learning rate works across every feature or dataset refresh.</p>"},{"location":"terms/gradient-descent/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, reliability</li> <li>Risk notes: Poorly tuned optimization can encode bias or destabilize production models, so training controls and reviews are essential.</li> </ul>"},{"location":"terms/gradient-descent/#relationships","title":"Relationships","text":"<ul> <li>Related: loss function, regularization, fine-tuning</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'gradient descent'.</p>"},{"location":"terms/gradient-descent/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>DeepLearning.AI Resources</li> <li>Wikipedia \u2013 Gradient Descent</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/gradient-descent.yml</code></p>"},{"location":"terms/greedy-decoding/","title":"greedy decoding","text":""},{"location":"terms/greedy-decoding/#greedy-decoding","title":"greedy decoding","text":"<p>Aliases: argmax decoding Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/greedy-decoding/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/greedy-decoding/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/greedy-decoding/#short-definition","title":"Short definition","text":"<p>Strategy that selects the highest-probability token at each step, producing deterministic outputs.</p>"},{"location":"terms/greedy-decoding/#long-definition","title":"Long definition","text":"<p>Greedy decoding generates text by repeatedly choosing the token with the maximum probability, skipping any sampling. The approach is deterministic, fast, and easy to reason about, which makes it attractive for scenarios that demand consistency or auditability. However, it can lead to repetitive phrasing, premature endings, or failure to explore alternative but valid continuations. Product teams deploy greedy decoding for transactional tasks such as structured responses, deterministic workflows, or template filling, where creativity is less important than reliability. Engineers monitor for mode collapse and may add techniques like repetition penalties or suffix constraints to mitigate degenerate loops. Governance teams value greedy decoding because it simplifies compliance reviews and reproducibility: the same prompt always yields the same answer. Nonetheless, the lack of variation can hide blind spots if evaluations rely solely on argmax outputs, so organizations often complement greedy decoding tests with sampling-based stress cases.</p>"},{"location":"terms/greedy-decoding/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Greedy decoding delivers the same answer every time, prioritizing consistency over creativity.</li> <li>Engineer: Iteratively pick the argmax token from the softmax distribution, allowing deterministic, low-latency generation.</li> </ul>"},{"location":"terms/greedy-decoding/#examples","title":"Examples","text":"<p>Do - Use greedy decoding for policy disclosures where text must match approved language.</p> <p>Don't - Rely on greedy decoding alone when testing for harmful edge cases that require sampling diversity.</p>"},{"location":"terms/greedy-decoding/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, accountability</li> <li>Risk notes: Deterministic decoding simplifies audits but can mask untested behaviors that appear only with sampling.</li> </ul>"},{"location":"terms/greedy-decoding/#relationships","title":"Relationships","text":"<ul> <li>Broader: decoding</li> <li>Related: top-k sampling, top-p sampling, temperature</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'greedy decoding'.</p>"},{"location":"terms/greedy-decoding/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/greedy-decoding.yml</code></p>"},{"location":"terms/guardrail-policy/","title":"guardrail policy","text":""},{"location":"terms/guardrail-policy/#guardrail-policy","title":"guardrail policy","text":"<p>Aliases: guardrail playbook, safety policy prompt Categories: Governance &amp; Risk Roles: Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust, Engineering &amp; Platform Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/guardrail-policy/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/guardrail-policy/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/guardrail-policy/#short-definition","title":"Short definition","text":"<p>Documented rules and prompts that define allowed, blocked, and escalated behaviors for AI systems.</p>"},{"location":"terms/guardrail-policy/#long-definition","title":"Long definition","text":"<p>A guardrail policy spells out how an AI system should respond to risky scenarios, combining policy prompts, tool restrictions, monitoring hooks, and escalation paths. It anchors the guardrails implemented in code to clear governance expectations so product, safety, and engineering teams act in sync. The policy enumerates prohibited content, required disclosures, human handoff triggers, and review cadences, and maps each rule to enforcement controls in the stack. Security leaders reference it when auditing access, while product owners ensure the policy keeps user experience coherent. Without this shared artifact, teams risk shipping fragmented mitigations that fail investigative audits or leave gaps between policy intent and agent behavior.</p>"},{"location":"terms/guardrail-policy/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Use the guardrail policy to prove you have enforceable boundaries before scaling sensitive features.</li> <li>Engineer: Translate each rule into prompts, filters, and monitoring alerts so violations are caught automatically.</li> </ul>"},{"location":"terms/guardrail-policy/#examples","title":"Examples","text":"<p>Do - Version-control policy prompts alongside code so deployments document why changes were made. - Define human approval checkpoints for financial or legal actions triggered by the agent.</p> <p>Don't - Rely on ad-hoc prompt tweaks without peer review or governance sign-off. - Ship new tools without mapping them to prohibited-use clauses in the policy.</p>"},{"location":"terms/guardrail-policy/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: governance, risk_management, accountability</li> <li>Risk notes: Missing or outdated guardrail policies make it impossible to demonstrate control effectiveness during audits.</li> </ul>"},{"location":"terms/guardrail-policy/#relationships","title":"Relationships","text":"<ul> <li>Broader: responsible ai, safety spec</li> <li>Narrower: escalation policy, system prompt</li> <li>Related: guardrails, ai incident response, red teaming</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'guardrail policy'.</p>"},{"location":"terms/guardrail-policy/#citations","title":"Citations","text":"<ul> <li>Constitutional AI (Anthropic)</li> <li>Microsoft \u2013 Content Filter &amp; Prompt Shields</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/guardrail-policy.yml</code></p>"},{"location":"terms/guardrails/","title":"guardrails","text":""},{"location":"terms/guardrails/#guardrails","title":"guardrails","text":"<p>Aliases: safety guardrails, policy guardrails Categories: Governance &amp; Risk Roles: Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/guardrails/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/guardrails/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/guardrails/#short-definition","title":"Short definition","text":"<p>Controls that constrain model behavior to comply with safety, legal, or brand requirements.</p>"},{"location":"terms/guardrails/#long-definition","title":"Long definition","text":"<p>Guardrails combine policy, technical, and operational measures designed to keep AI systems within acceptable behavior. They can include pre- and post-processing filters, policy-informed prompts, classifier ensembles, or moderation APIs that block disallowed content before it reaches end users. Effective guardrail programs coordinate with legal and risk teams to encode organizational standards and regulatory obligations. Engineers integrate guardrails into the request pipeline, monitor their performance, and log interventions for audit trails. Product managers review guardrail coverage to understand trade-offs between user experience and safety friction. Governance stakeholders treat guardrails as living controls that require change management, testing, and documentation to demonstrate due diligence. When guardrails fail or drift, the resulting incidents can expose organizations to legal liability, reputational damage, or regulatory penalties.</p>"},{"location":"terms/guardrails/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Guardrails are the checks that keep the AI from saying or doing things that would put the company at risk.</li> <li>Engineer: Policy-aligned filters, prompts, and classifiers embedded in the inference stack to block or reshape unsafe outputs.</li> </ul>"},{"location":"terms/guardrails/#examples","title":"Examples","text":"<p>Do - Test guardrail coverage with red-team prompts whenever the base model or system prompt changes.</p> <p>Don't - Rely on a single moderation classifier without monitoring precision and recall across scenarios.</p>"},{"location":"terms/guardrails/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, privacy, robustness</li> <li>Risk notes: Outdated guardrails can miss harmful outputs or inadvertently censor legitimate content, creating compliance gaps.</li> </ul>"},{"location":"terms/guardrails/#relationships","title":"Relationships","text":"<ul> <li>Broader: responsible AI</li> <li>Narrower: output filtering, safety prompt</li> <li>Related: system prompt, temperature, red teaming</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'guardrails'.</p>"},{"location":"terms/guardrails/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Salesforce \u2013 Building AI Guardrails</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/guardrails.yml</code></p>"},{"location":"terms/hallucination/","title":"hallucination","text":""},{"location":"terms/hallucination/#hallucination","title":"hallucination","text":"<p>Aliases: AI hallucination, confabulation Categories: LLM Core, Governance &amp; Risk Roles: Communications &amp; Enablement, Data Science &amp; Research, Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/hallucination/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/hallucination/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/hallucination/#short-definition","title":"Short definition","text":"<p>When an AI model presents fabricated or unsupported information as fact.</p>"},{"location":"terms/hallucination/#long-definition","title":"Long definition","text":"<p>Hallucination describes the tendency of generative models to deliver content that sounds plausible but is either factually incorrect, logically inconsistent, or entirely invented. The phenomenon stems from the probabilistic way large language models predict the next token based on training data patterns rather than grounded knowledge of the world. It can occur when prompts lack sufficient context, when the model has not seen relevant examples during training, or when decoding strategies over-index on fluency instead of accuracy. Product teams experience hallucination as broken user trust, while engineers may notice it during evaluation as high lexical overlap paired with low factual precision. Mitigations range from retrieval augmentation and prompt constraints to post-generation fact checking, human review, and model fine-tuning on verified corpora. Organizations must treat hallucination as both a quality and a risk management issue, particularly in regulated or safety-critical workflows.</p>"},{"location":"terms/hallucination/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Signals that the model is making things up, which erodes user trust and can trigger compliance issues.</li> <li>Engineer: Indicates the model sampled a high-probability sequence lacking factual grounding; investigate context, decoding, and eval signals.</li> </ul>"},{"location":"terms/hallucination/#examples","title":"Examples","text":"<p>Do - Log hallucination incidents and route high-severity cases to human review for remediation. - Use retrieval augmentation or tool grounding to supply verifiable context before generation.</p> <p>Don't - Deploy long-form responses without monitoring factual accuracy or adding disclaimers. - Assume higher model size alone will eliminate hallucination without evaluation improvements.</p>"},{"location":"terms/hallucination/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accuracy, transparency, validity</li> <li>Risk notes: Unaddressed hallucinations can produce misleading outputs that violate accuracy commitments and create legal exposure.</li> </ul>"},{"location":"terms/hallucination/#relationships","title":"Relationships","text":"<ul> <li>Broader: generative AI</li> <li>Narrower: factual hallucination, formal hallucination</li> <li>Related: retrieval-augmented generation, guardrails, evaluation</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'hallucination'.</p>"},{"location":"terms/hallucination/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>NIST AI RMF Glossary</li> <li>Google ML Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/hallucination.yml</code></p>"},{"location":"terms/human-handoff/","title":"human handoff","text":""},{"location":"terms/human-handoff/#human-handoff","title":"human handoff","text":"<p>Aliases: agent-to-human handoff, human-in-the-loop handoff Categories: Agents &amp; Tooling Roles: Product &amp; Program Managers, Communications &amp; Enablement, Engineering &amp; Platform, Policy &amp; Risk Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/human-handoff/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> </ul>"},{"location":"terms/human-handoff/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Audit exposed tools against the safeguards described and document approval paths.</li> <li>Test hand-offs with human reviewers to confirm the safety expectations captured here are met.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/human-handoff/#short-definition","title":"Short definition","text":"<p>Moment when an AI workflow transfers control to a human for review or action.</p>"},{"location":"terms/human-handoff/#long-definition","title":"Long definition","text":"<p>A human handoff occurs when an AI agent or automation pauses and routes context to a person for judgment, approval, or next steps. Effective handoffs bundle conversation history, risk signals, and recommended actions so humans can respond quickly. Product and support teams design the experience, engineering ensures state is preserved across channels, and policy leaders define which scenarios must escalate to people. Handoffs should capture audit trails, notify owners, and allow the human to resume the AI-assisted workflow. Poorly designed handoffs create dead ends, slow response times, or leave users confused about who's in control.</p>"},{"location":"terms/human-handoff/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Guarantee there is a clear path to human help in sensitive journeys to maintain trust.</li> <li>Engineer: Package context, risk scores, and next best actions so reviewers can resolve the handoff quickly.</li> </ul>"},{"location":"terms/human-handoff/#examples","title":"Examples","text":"<p>Do - Route escalations to a staffed queue with SLAs and full conversation summaries. - Allow humans to annotate the outcome so the agent can learn from future cases.</p> <p>Don't - Drop the user into a blank chat with no explanation of what the agent attempted. - Overlook accessibility needs when transferring to human support channels.</p>"},{"location":"terms/human-handoff/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: governance, monitoring, risk_management</li> <li>Risk notes: Weak handoffs undermine accountability and can leave high-risk cases unresolved.</li> </ul>"},{"location":"terms/human-handoff/#relationships","title":"Relationships","text":"<ul> <li>Broader: agent executor, escalation policy</li> <li>Related: guardrail policy, ai incident response, impact mitigation plan</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'human handoff'.</p>"},{"location":"terms/human-handoff/#citations","title":"Citations","text":"<ul> <li>LangChain Glossary \u2013 Human-in-the-loop</li> <li>Microsoft \u2013 Prompt Engineering Guidance</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/human-handoff.yml</code></p>"},{"location":"terms/impact-mitigation-plan/","title":"impact mitigation plan","text":""},{"location":"terms/impact-mitigation-plan/#impact-mitigation-plan","title":"impact mitigation plan","text":"<p>Aliases: mitigation roadmap, risk remediation plan Categories: Governance &amp; Risk Roles: Policy &amp; Risk, Product &amp; Program Managers, Legal &amp; Compliance, Engineering &amp; Platform Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/impact-mitigation-plan/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/impact-mitigation-plan/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/impact-mitigation-plan/#short-definition","title":"Short definition","text":"<p>Action plan that tracks risks, mitigations, owners, and timelines for an AI deployment.</p>"},{"location":"terms/impact-mitigation-plan/#long-definition","title":"Long definition","text":"<p>An impact mitigation plan documents the risks identified during assessments, the mitigation steps the organization will take, and the owners accountable for each action. It connects findings from algorithmic impact assessments, red-team exercises, and audits to specific deadlines, metrics, and escalation triggers. Policy leads maintain the plan to satisfy regulatory expectations, product teams map mitigations to launch gates, and engineering tracks the technical work needed to close gaps. When kept current, the plan provides a single source of truth for audits and post-incident reviews. When neglected, risks stay unaddressed and stakeholders lose visibility into whether controls are actually landing.</p>"},{"location":"terms/impact-mitigation-plan/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Use the mitigation plan to monitor whether high-severity risks are closing before committing to launch dates.</li> <li>Engineer: Link each mitigation to backlog tickets and telemetry so progress stays measurable and auditable.</li> </ul>"},{"location":"terms/impact-mitigation-plan/#examples","title":"Examples","text":"<p>Do - Record status updates and evidence links for every mitigation before major launch reviews. - Flag blocked mitigations with escalation paths and decision deadlines for leadership.</p> <p>Don't - Let mitigation owners slip without documenting rationale or alternate controls. - Track mitigations in private spreadsheets that governance teams cannot access.</p>"},{"location":"terms/impact-mitigation-plan/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: risk_management, accountability, monitoring</li> <li>Risk notes: Missing mitigation follow-through leaves organizations exposed to repeat incidents and regulatory penalties.</li> </ul>"},{"location":"terms/impact-mitigation-plan/#relationships","title":"Relationships","text":"<ul> <li>Broader: algorithmic impact assessment, model governance</li> <li>Related: ai assurance, transparency report, risk register</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'impact mitigation plan'.</p>"},{"location":"terms/impact-mitigation-plan/#citations","title":"Citations","text":"<ul> <li>NIST \u2013 AI Risk Management Framework</li> <li>Wikipedia \u2013 Impact Assessment</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/impact-mitigation-plan.yml</code></p>"},{"location":"terms/incident-taxonomy/","title":"incident taxonomy","text":""},{"location":"terms/incident-taxonomy/#incident-taxonomy","title":"incident taxonomy","text":"<p>Aliases: incident classification, risk taxonomy Categories: Operations &amp; Monitoring Roles: Policy &amp; Risk, Security &amp; Trust, Product &amp; Program Managers, Engineering &amp; Platform Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/incident-taxonomy/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/incident-taxonomy/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Instrument dashboards or alerts that reflect the metrics highlighted in this definition.</li> <li>Update incident response or on-call runbooks with the glossary's do/don't scenarios.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/incident-taxonomy/#short-definition","title":"Short definition","text":"<p>Standardized categories used to tag, analyze, and report AI incidents consistently.</p>"},{"location":"terms/incident-taxonomy/#long-definition","title":"Long definition","text":"<p>An incident taxonomy defines the categories and severity levels used when logging AI failures or near misses. It enables consistent reporting across teams, trend analysis, and regulatory submissions. The taxonomy should map to policy commitments (e.g., privacy, bias, safety), specify severity criteria, and align with incident response playbooks. Policy teams maintain the taxonomy, engineering and support staff apply it during postmortems, and executives use the data to prioritize mitigations. Without a shared taxonomy, incident data becomes noisy, making it difficult to learn from patterns or demonstrate accountability to regulators.</p>"},{"location":"terms/incident-taxonomy/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Mandate taxonomy usage so incident trends inform roadmap and governance decisions.</li> <li>Engineer: Tag incidents accurately and link them to the taxonomy when filing postmortems.</li> </ul>"},{"location":"terms/incident-taxonomy/#examples","title":"Examples","text":"<p>Do - Review taxonomy definitions annually with policy and security stakeholders. - Tie mitigation backlog items to taxonomy categories for tracking.</p> <p>Don't - Let teams create ad hoc categories that fragment reporting. - Ignore low-severity categories; they often reveal emerging risks.</p>"},{"location":"terms/incident-taxonomy/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: governance, monitoring, accountability</li> <li>Risk notes: Inconsistent classification hides systemic issues and complicates regulatory disclosure.</li> </ul>"},{"location":"terms/incident-taxonomy/#relationships","title":"Relationships","text":"<ul> <li>Broader: ai incident response</li> <li>Related: risk register, impact mitigation plan, transparency report</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'incident taxonomy'.</p>"},{"location":"terms/incident-taxonomy/#citations","title":"Citations","text":"<ul> <li>CISA \u2013 AI Cybersecurity Playbook</li> <li>AI Incident Database</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/incident-taxonomy.yml</code></p>"},{"location":"terms/instruction-tuning/","title":"instruction tuning","text":""},{"location":"terms/instruction-tuning/#instruction-tuning","title":"instruction tuning","text":"<p>Aliases: instruction fine-tuning, supervised fine-tuning Categories: Optimization &amp; Efficiency Roles: Engineering &amp; Platform, Data Science &amp; Research, Product &amp; Program Managers, Policy &amp; Risk Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/instruction-tuning/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> </ul>"},{"location":"terms/instruction-tuning/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Record before-and-after performance metrics when applying this optimisation technique.</li> <li>Document trade-offs for product and policy partners using the glossary's language.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/instruction-tuning/#short-definition","title":"Short definition","text":"<p>Supervised training that teaches models to follow natural-language instructions using curated examples.</p>"},{"location":"terms/instruction-tuning/#long-definition","title":"Long definition","text":"<p>Instruction tuning (often called supervised fine-tuning) adapts a pretrained model on datasets of prompts and ideal responses so it learns to follow instructions. Annotators craft demonstrations that reflect desired tone, safety, and reasoning patterns. Engineers run fine-tuning jobs, data scientists manage dataset quality, and policy teams ensure instructions encode governance requirements. The process is typically the first step before RLHF or other alignment work. Poorly curated instruction data can introduce bias, regress safety, or drift from product expectations, so teams monitor evaluation metrics and refresh the dataset as policies evolve.</p>"},{"location":"terms/instruction-tuning/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Budget for recurring instruction tuning to keep models aligned with evolving strategy.</li> <li>Engineer: Version datasets, hyperparameters, and evaluation checkpoints for every tuning run.</li> </ul>"},{"location":"terms/instruction-tuning/#examples","title":"Examples","text":"<p>Do - Label edge cases and refusal scenarios so the model learns safety boundaries. - Combine instruction data with role-based tone guidelines for different audiences.</p> <p>Don't - Assume open-source instruction datasets cover your policy requirements. - Overwrite the base model without comparing against prior alignment baselines.</p>"},{"location":"terms/instruction-tuning/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: risk_management, measurement, accountability</li> <li>Risk notes: Uncontrolled tuning can undo prior safety work and confuse governance owners.</li> </ul>"},{"location":"terms/instruction-tuning/#relationships","title":"Relationships","text":"<ul> <li>Broader: fine-tuning</li> <li>Related: reinforcement learning from human feedback, reward model, robust prompting</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'instruction tuning'.</p>"},{"location":"terms/instruction-tuning/#citations","title":"Citations","text":"<ul> <li>OpenAI \u2013 InstructGPT</li> <li>Stanford \u2013 Self-Instruct</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/instruction-tuning.yml</code></p>"},{"location":"terms/jailbreak-prompt/","title":"jailbreak prompt","text":""},{"location":"terms/jailbreak-prompt/#jailbreak-prompt","title":"jailbreak prompt","text":"<p>Aliases: prompt jailbreak, guardrail bypass Categories: Governance &amp; Risk Roles: Security &amp; Trust, Product &amp; Program Managers, Engineering &amp; Platform Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/jailbreak-prompt/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/jailbreak-prompt/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/jailbreak-prompt/#short-definition","title":"Short definition","text":"<p>Crafted input that persuades a model to ignore safety instructions and produce disallowed responses.</p>"},{"location":"terms/jailbreak-prompt/#long-definition","title":"Long definition","text":"<p>A jailbreak prompt is a deliberate attempt to bypass an AI system's guardrails. Attackers wrap malicious requests in roleplay, obfuscation, or multi-step instructions so the model forgets or ignores its policy. Jailbreaks often pair with prompt injection to extract secrets, generate harmful content, or trigger unauthorized tool calls. Defenders maintain libraries of known jailbreak patterns, run continuous red-teaming, and enforce layered mitigations (robust prompts, classifiers, tool restrictions). Product teams communicate refusals transparently, while governance partners document residual risks. Without jailbreak monitoring, safety regressions can go unnoticed until customers or regulators report incidents.</p>"},{"location":"terms/jailbreak-prompt/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Treat jailbreaks as a measurable security risk with owners, metrics, and incident response plans.</li> <li>Engineer: Automate regression tests for known jailbreak patterns and update defenses when new variants appear.</li> </ul>"},{"location":"terms/jailbreak-prompt/#examples","title":"Examples","text":"<p>Do - Maintain a catalog of blocked prompts and feed them into the evaluation harness. - Pair jailbreak detection with human review for high-severity categories.</p> <p>Don't - Assume a single policy prompt will withstand evolving jailbreak tactics. - Silently filter outputs without notifying security or policy teams.</p>"},{"location":"terms/jailbreak-prompt/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: security, monitoring, risk_management</li> <li>Risk notes: Unmitigated jailbreaks can surface banned content, expose data, or erode user trust.</li> </ul>"},{"location":"terms/jailbreak-prompt/#relationships","title":"Relationships","text":"<ul> <li>Broader: prompt injection</li> <li>Related: robust prompting, red teaming, guardrails</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'jailbreak prompt'.</p>"},{"location":"terms/jailbreak-prompt/#citations","title":"Citations","text":"<ul> <li>arXiv \u2013 Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study</li> <li>Microsoft \u2013 Content Filter &amp; Prompt Shields</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/jailbreak-prompt.yml</code></p>"},{"location":"terms/knowledge-distillation/","title":"knowledge distillation","text":""},{"location":"terms/knowledge-distillation/#knowledge-distillation","title":"knowledge distillation","text":"<p>Aliases: distillation, teacher-student training Categories: Optimization &amp; Efficiency Roles: Data Science &amp; Research, Engineering &amp; Platform Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/knowledge-distillation/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/knowledge-distillation/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Record before-and-after performance metrics when applying this optimisation technique.</li> <li>Document trade-offs for product and policy partners using the glossary's language.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/knowledge-distillation/#short-definition","title":"Short definition","text":"<p>Technique that trains a smaller student model to mimic a larger teacher model\u2019s behavior.</p>"},{"location":"terms/knowledge-distillation/#long-definition","title":"Long definition","text":"<p>Knowledge distillation transfers capabilities from a high-capacity teacher model to a more efficient student model by training the student on the teacher\u2019s softened output distributions or generated examples. The approach preserves much of the teacher\u2019s accuracy while reducing parameter count, latency, and cost\u2014making it popular for edge deployments and inference scaling. Distillation complements other efficiency strategies such as quantization or pruning, enabling organizations to meet budget constraints without abandoning model quality. Engineers configure temperature, loss weighting, and dataset selection to ensure the student captures critical behaviors, sometimes blending hard labels with soft targets. Governance teams review distillation pipelines to confirm the teacher and student share licensing compatibility and that synthetic data generation respects privacy obligations. Because distillation can copy undesirable biases along with strengths, evaluations must confirm that risk mitigations remain effective in the distilled model.</p>"},{"location":"terms/knowledge-distillation/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Distillation keeps model quality high while shrinking the model to run faster and cheaper.</li> <li>Engineer: Train a student network using teacher logits or generated traces so it approximates the teacher\u2019s function with fewer parameters.</li> </ul>"},{"location":"terms/knowledge-distillation/#examples","title":"Examples","text":"<p>Do - Document which teacher checkpoints and datasets produced each distilled release.</p> <p>Don't - Distill sensitive capabilities without reassessing safety performance on the student model.</p>"},{"location":"terms/knowledge-distillation/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: efficiency, validity</li> <li>Risk notes: Students inherit teacher biases; missing evaluations can hide regressions introduced during compression.</li> </ul>"},{"location":"terms/knowledge-distillation/#relationships","title":"Relationships","text":"<ul> <li>Broader: model optimization</li> <li>Related: quantization, low-rank adaptation, evaluation</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'knowledge distillation'.</p>"},{"location":"terms/knowledge-distillation/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/knowledge-distillation.yml</code></p>"},{"location":"terms/kv-cache/","title":"kv cache","text":""},{"location":"terms/kv-cache/#kv-cache","title":"kv cache","text":"<p>Aliases: key-value cache, attention cache Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/kv-cache/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/kv-cache/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/kv-cache/#short-definition","title":"Short definition","text":"<p>Stored attention keys and values reused across decoding steps to speed sequential generation.</p>"},{"location":"terms/kv-cache/#long-definition","title":"Long definition","text":"<p>The KV cache holds intermediate key and value tensors produced during transformer attention so they can be reused on subsequent tokens without recomputing earlier layers. During autoregressive decoding, each new token only needs to attend to prior states; caching those states significantly reduces latency and compute, especially for long prompts or streaming responses. Production systems manage KV cache sizes carefully because they grow with context length, consuming memory on GPUs and influencing batch throughput. Engineers optimize cache eviction policies, quantization, or paged memory formats to balance cost and responsiveness. Governance and reliability teams monitor KV cache behavior to ensure no residual data persists longer than intended, particularly when serving multi-tenant workloads where prompts may contain sensitive information. Documenting cache configuration is part of operational playbooks for diagnosing performance regressions.</p>"},{"location":"terms/kv-cache/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: The KV cache is the reuse trick that keeps responses snappy even when conversations get long.</li> <li>Engineer: Per-layer tensors of attention keys/values stored between decoding steps to avoid recomputing full sequence context.</li> </ul>"},{"location":"terms/kv-cache/#examples","title":"Examples","text":"<p>Do - Audit GPU memory usage with and without KV caching to plan capacity for long-context workloads.</p> <p>Don't - Share KV cache contents across tenants without isolation controls.</p>"},{"location":"terms/kv-cache/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: efficiency, privacy</li> <li>Risk notes: Improper cache management can leak residual user data or trigger out-of-memory failures.</li> </ul>"},{"location":"terms/kv-cache/#relationships","title":"Relationships","text":"<ul> <li>Broader: inference optimization</li> <li>Narrower: paged attention</li> <li>Related: attention, context window, quantization</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'kv cache'.</p>"},{"location":"terms/kv-cache/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/kv-cache.yml</code></p>"},{"location":"terms/log-probability/","title":"log probability","text":""},{"location":"terms/log-probability/#log-probability","title":"log probability","text":"<p>Aliases: logprob, token log probability Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/log-probability/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/log-probability/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/log-probability/#short-definition","title":"Short definition","text":"<p>Logarithm of a token\u2019s probability, used to inspect model confidence and guide decoding tweaks.</p>"},{"location":"terms/log-probability/#long-definition","title":"Long definition","text":"<p>Log probability represents the natural logarithm of a token\u2019s predicted probability during generation. Because probabilities multiply across sequence lengths, working in log space avoids numerical underflow and makes it easier to compare relative likelihoods. Tooling that surfaces token-level logprobs helps practitioners debug why a model chose specific words, identify low-confidence spans for post-processing, and implement rejection sampling. Product teams use logprob thresholds to trigger fallbacks, escalate to humans, or annotate responses with confidence indicators. Engineers rely on logprobs to enforce constraints such as toxicity caps, apply repetition penalties, or calibrate beam search scores. Governance stakeholders treat logprob telemetry as part of audit trails, correlating low-confidence regions with incidents or hallucinations. Capturing and analyzing log probabilities thus supports transparency requirements and informs mitigation strategies when behavior drifts over time.</p>"},{"location":"terms/log-probability/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Log probabilities surface how confident the model was about each word, which can trigger reviews when confidence drops.</li> <li>Engineer: Natural-log scores from the softmax output that accumulate across tokens; useful for diagnostics, penalties, and rejection sampling.</li> </ul>"},{"location":"terms/log-probability/#examples","title":"Examples","text":"<p>Do - Flag completions where average logprob falls below a threshold and route them for human review.</p> <p>Don't - Expose raw logprob values to end users without context or calibration.</p>"},{"location":"terms/log-probability/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, validity</li> <li>Risk notes: Missing logprob telemetry makes it harder to audit why unsafe or incorrect outputs were produced.</li> </ul>"},{"location":"terms/log-probability/#relationships","title":"Relationships","text":"<ul> <li>Broader: decoding</li> <li>Related: temperature, top-k sampling, evaluation</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'log probability'.</p>"},{"location":"terms/log-probability/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>NIST AI RMF Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/log-probability.yml</code></p>"},{"location":"terms/loss-function/","title":"loss function","text":""},{"location":"terms/loss-function/#loss-function","title":"loss function","text":"<p>Aliases: cost function, objective function Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers, Policy &amp; Risk Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/loss-function/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> </ul>"},{"location":"terms/loss-function/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Add this concept to onboarding materials so teammates share a common baseline.</li> <li>Link supporting research or documentation in your internal wiki for deeper study.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/loss-function/#short-definition","title":"Short definition","text":"<p>Mathematical rule that scores how far model predictions deviate from desired targets.</p>"},{"location":"terms/loss-function/#long-definition","title":"Long definition","text":"<p>A loss function translates model performance into a single scalar so optimisation algorithms know which direction to move. Classification systems rely on cross-entropy or focal loss to punish incorrect labels, while regression models often use mean squared error or Huber loss to balance sensitivity to outliers. Product and policy partners read loss definitions to understand which business or compliance outcomes the model optimises, and engineers adjust them when priorities shift from accuracy to calibration, fairness, or cost. Clear documentation of the loss function protects teams from accidental regressions, especially when a model serves multiple stakeholders who care about different metrics.</p>"},{"location":"terms/loss-function/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: The loss function is the scorecard that tells the training loop if changes made the model better or worse.</li> <li>Engineer: Define differentiable losses aligned with business metrics; monitor surrogate vs. offline KPI gaps and adjust weighting as needed.</li> </ul>"},{"location":"terms/loss-function/#examples","title":"Examples","text":"<p>Do - Document the exact loss, weighting, and class balancing strategy in model cards and runbooks. - Evaluate fairness metrics alongside the chosen loss before updating production thresholds.</p> <p>Don't - Swap in a new loss function without revalidating calibration and downstream KPIs. - Optimise purely for training loss when evaluation or business metrics tell a different story.</p>"},{"location":"terms/loss-function/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, transparency</li> <li>Risk notes: Misaligned losses can optimise for the wrong behaviour, creating governance or compliance gaps if not reviewed.</li> </ul>"},{"location":"terms/loss-function/#relationships","title":"Relationships","text":"<ul> <li>Broader: evaluation</li> <li>Related: gradient descent, regularization, bias-variance tradeoff</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'loss function'.</p>"},{"location":"terms/loss-function/#citations","title":"Citations","text":"<ul> <li>Google ML Crash Course</li> <li>Wikipedia \u2013 Loss Function</li> <li>Machine Learning Mastery</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/loss-function.yml</code></p>"},{"location":"terms/low-rank-adaptation/","title":"low-rank adaptation","text":""},{"location":"terms/low-rank-adaptation/#low-rank-adaptation","title":"low-rank adaptation","text":"<p>Aliases: LoRA, low rank fine-tuning Categories: Optimization &amp; Efficiency Roles: Data Science &amp; Research, Engineering &amp; Platform Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/low-rank-adaptation/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/low-rank-adaptation/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Record before-and-after performance metrics when applying this optimisation technique.</li> <li>Document trade-offs for product and policy partners using the glossary's language.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/low-rank-adaptation/#short-definition","title":"Short definition","text":"<p>Parameter-efficient fine-tuning that injects low-rank update matrices into transformer weights.</p>"},{"location":"terms/low-rank-adaptation/#long-definition","title":"Long definition","text":"<p>Low-rank adaptation (LoRA) fine-tunes large language models by learning compact update matrices rather than adjusting full weight tensors. The method freezes the original model parameters and trains additive low-rank factors that capture task-specific shifts, reducing memory usage and compute costs. LoRA adapters can be merged into the base model for deployment or stored separately to toggle behaviors per tenant. This approach enables organizations to customize models using modest hardware, accelerate experimentation, and share adapters without distributing full proprietary checkpoints. Engineers manage rank choices, scaling factors, and target layers to balance quality with efficiency. Governance teams evaluate LoRA artifacts like traditional model versions, reviewing data provenance, licensing, and security implications. Because adapters can encode sensitive capabilities, access control and documentation remain essential\u2014particularly when multiple teams contribute adapters to a shared serving stack.</p>"},{"location":"terms/low-rank-adaptation/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: LoRA lets teams customize giant models cheaply by training small plug-ins instead of retraining everything.</li> <li>Engineer: Freeze base weights, insert trainable low-rank matrices into attention or feed-forward layers, and fine-tune with minimal VRAM.</li> </ul>"},{"location":"terms/low-rank-adaptation/#examples","title":"Examples","text":"<p>Do - Track which datasets and objectives produced each LoRA adapter before sharing it across teams.</p> <p>Don't - Merge third-party adapters into production models without license verification.</p>"},{"location":"terms/low-rank-adaptation/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: efficiency, accountability</li> <li>Risk notes: Unvetted adapters can override safety tuning or introduce licensed data without traceability.</li> </ul>"},{"location":"terms/low-rank-adaptation/#relationships","title":"Relationships","text":"<ul> <li>Broader: fine-tuning</li> <li>Related: quantization, distillation, guardrails</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'low-rank adaptation'.</p>"},{"location":"terms/low-rank-adaptation/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>NIST AI RMF Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/low-rank-adaptation.yml</code></p>"},{"location":"terms/memory-strategy/","title":"memory strategy","text":""},{"location":"terms/memory-strategy/#memory-strategy","title":"memory strategy","text":"<p>Aliases: agent memory strategy, memory policy Categories: Agents &amp; Tooling Roles: Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/memory-strategy/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/memory-strategy/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Audit exposed tools against the safeguards described and document approval paths.</li> <li>Test hand-offs with human reviewers to confirm the safety expectations captured here are met.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/memory-strategy/#short-definition","title":"Short definition","text":"<p>Deliberate approach for when an AI agent stores, retrieves, or forgets context across tasks.</p>"},{"location":"terms/memory-strategy/#long-definition","title":"Long definition","text":"<p>A memory strategy defines how an AI agent captures, retrieves, and expires contextual information while executing tasks. It balances cost, latency, privacy, and safety considerations by deciding which events to persist, where to store them, and which signals trigger deletion. Engineers design memory policies to avoid stale or sensitive data leaking into future prompts, while product teams ensure the experience remains transparent to users. Common patterns include stateless execution, short-term scratchpads, vector memory with embeddings, and human-curated summaries. The right strategy depends on regulatory constraints, user consent, and the risk tolerance for hallucinations or runaway tool use. Without clear guardrails, agents can over-collect data, misapply outdated facts, or introduce compliance gaps.</p>"},{"location":"terms/memory-strategy/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Align on what information the agent keeps so you can set customer trust and compliance expectations.</li> <li>Engineer: Codify storage tiers, retention windows, and redaction rules before wiring memory into agent workflows.</li> </ul>"},{"location":"terms/memory-strategy/#examples","title":"Examples","text":"<p>Do - Expire vector memories that include personally identifiable information within 24 hours unless explicit consent exists. - Store only structured summaries of previous steps so downstream prompts stay within token and privacy budgets.</p> <p>Don't - Persist raw chat transcripts indefinitely without auditing for sensitive attributes. - Reuse memories from one customer tenant inside another tenant's session.</p>"},{"location":"terms/memory-strategy/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: governance, privacy, risk_management</li> <li>Risk notes: Poorly scoped memory increases the blast radius for privacy violations and policy noncompliance.</li> </ul>"},{"location":"terms/memory-strategy/#relationships","title":"Relationships","text":"<ul> <li>Broader: agentic ai, tool use</li> <li>Related: vector store, retrieval, data minimization</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'memory strategy'.</p>"},{"location":"terms/memory-strategy/#citations","title":"Citations","text":"<ul> <li>LangChain Documentation \u2013 Memory</li> <li>Constitutional AI (Anthropic)</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/memory-strategy.yml</code></p>"},{"location":"terms/mixture-of-experts/","title":"mixture of experts","text":""},{"location":"terms/mixture-of-experts/#mixture-of-experts","title":"mixture of experts","text":"<p>Aliases: moe, expert gating Categories: Optimization &amp; Efficiency Roles: Engineering &amp; Platform, Data Science &amp; Research, Product &amp; Program Managers Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/mixture-of-experts/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/mixture-of-experts/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Record before-and-after performance metrics when applying this optimisation technique.</li> <li>Document trade-offs for product and policy partners using the glossary's language.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/mixture-of-experts/#short-definition","title":"Short definition","text":"<p>Neural architecture that routes tokens to specialized submodels to scale capacity efficiently.</p>"},{"location":"terms/mixture-of-experts/#long-definition","title":"Long definition","text":"<p>Mixture-of-experts (MoE) models split a network into many expert submodels and use a gating function to decide which experts process each token. This allows models to increase parameter counts without running every computation for every input, improving efficiency and enabling specialization (e.g., code vs. dialogue experts). Engineers must manage load balancing, routing stability, and inference infrastructure that can handle sparse activation. Product teams care about how specialization affects consistency across use cases, and governance teams assess whether routing introduces bias or explainability challenges. Operationally, MoE systems demand robust logging to trace which experts contributed to an output for debugging and accountability.</p>"},{"location":"terms/mixture-of-experts/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: MoE architectures stretch compute budgets while boosting quality, but require investment in routing controls.</li> <li>Engineer: Monitor expert utilization, balance routing, and record which experts fired for auditability.</li> </ul>"},{"location":"terms/mixture-of-experts/#examples","title":"Examples","text":"<p>Do - Set guardrails for experts that handle sensitive domains like legal or medical content. - Instrument per-expert performance metrics to catch degradation early.</p> <p>Don't - Assume routing is fair without analyzing demographic or domain skew. - Deploy MoE models without redundancy plans for underperforming experts.</p>"},{"location":"terms/mixture-of-experts/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: monitoring, risk_management, transparency</li> <li>Risk notes: Opaque routing can hide failures or bias; sparse activation complicates reproducibility.</li> </ul>"},{"location":"terms/mixture-of-experts/#relationships","title":"Relationships","text":"<ul> <li>Broader: fine-tuning</li> <li>Related: knowledge distillation, model interpretability, evaluation</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'mixture of experts'.</p>"},{"location":"terms/mixture-of-experts/#citations","title":"Citations","text":"<ul> <li>Google Research \u2013 Switch Transformers</li> <li>Google Brain \u2013 Outrageously Large Neural Networks</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/mixture-of-experts.yml</code></p>"},{"location":"terms/ml-observability/","title":"ml observability","text":""},{"location":"terms/ml-observability/#ml-observability","title":"ml observability","text":"<p>Aliases: model observability, ai observability Categories: Operations &amp; Monitoring Roles: Engineering &amp; Platform, Policy &amp; Risk, Security &amp; Trust Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/ml-observability/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/ml-observability/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Instrument dashboards or alerts that reflect the metrics highlighted in this definition.</li> <li>Update incident response or on-call runbooks with the glossary's do/don't scenarios.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/ml-observability/#short-definition","title":"Short definition","text":"<p>Practices and tooling that surface model health through metrics, traces, and alerts across the lifecycle.</p>"},{"location":"terms/ml-observability/#long-definition","title":"Long definition","text":"<p>ML observability applies observability principles to machine learning systems, collecting signals that describe data quality, model behavior, infrastructure health, and user outcomes. The discipline unifies telemetry such as latency, throughput, drift metrics, guardrail triggers, and human feedback so teams can diagnose issues quickly. Robust observability stacks ingest logs from preprocessing, inference, retrieval, and post-processing stages, correlating them with experiment metadata and deployment versions. Product owners reference observability dashboards to understand adoption and satisfaction, while engineers rely on them to root-cause regressions and capacity incidents. Governance programs require observable pipelines to demonstrate compliance with monitoring expectations in frameworks like the NIST AI RMF. Without observability, organizations struggle to detect bias, hallucinations, or safety incidents before they impact users. Investing in standardized logging, alerting, and runbooks enables proactive triage and continuous improvement.</p>"},{"location":"terms/ml-observability/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: ML observability provides the dashboards and alerts that show whether AI systems remain healthy and trustworthy.</li> <li>Engineer: Collect metrics, logs, and traces across data, model, and infra layers; stitch them to deployments for debugging and compliance.</li> </ul>"},{"location":"terms/ml-observability/#examples","title":"Examples","text":"<p>Do - Correlate drift alerts with retrieval metrics to pinpoint whether failures stem from data or model changes.</p> <p>Don't - Disable guardrail logging due to cost; missing records breaks incident response and compliance.</p>"},{"location":"terms/ml-observability/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: monitoring, accountability</li> <li>Risk notes: Insufficient observability hides safety incidents and undermines regulatory reporting obligations.</li> </ul>"},{"location":"terms/ml-observability/#relationships","title":"Relationships","text":"<ul> <li>Broader: ml ops</li> <li>Related: model drift, guardrails, evaluation</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'ml observability'.</p>"},{"location":"terms/ml-observability/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Google ML Glossary</li> <li>Deepchecks \u2013 ML Monitoring Overview</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/ml-observability.yml</code></p>"},{"location":"terms/ml-ops/","title":"ml ops","text":""},{"location":"terms/ml-ops/#ml-ops","title":"ml ops","text":"<p>Aliases: mlops, machine learning operations Categories: Operations &amp; Monitoring Roles: Engineering &amp; Platform, Policy &amp; Risk, Security &amp; Trust Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/ml-ops/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/ml-ops/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Instrument dashboards or alerts that reflect the metrics highlighted in this definition.</li> <li>Update incident response or on-call runbooks with the glossary's do/don't scenarios.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/ml-ops/#short-definition","title":"Short definition","text":"<p>Operational discipline that manages ML models from experimentation through deployment and monitoring.</p>"},{"location":"terms/ml-ops/#long-definition","title":"Long definition","text":"<p>ML Ops adapts DevOps and data engineering practices to the lifecycle of machine learning systems. It encompasses experiment tracking, feature management, deployment automation, monitoring, incident response, and governance. Successful ML Ops programs unify teams across data science, engineering, and compliance so models move from prototype to production without manual heroics. Tooling includes model registries, CI/CD pipelines, automated evaluation gates, and observability platforms that watch for drift, data quality issues, and guardrail violations. Product organizations rely on ML Ops maturity to ship updates safely and respond quickly to incidents. Governance frameworks such as the NIST AI RMF expect documented ML Ops processes to demonstrate accountability, transparency, and continuous monitoring. Without disciplined ML Ops, models stagnate, degrade in accuracy, or fall out of compliance as the environment evolves.</p>"},{"location":"terms/ml-ops/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: ML Ops is the operating system that keeps production models reliable, compliant, and cost-efficient.</li> <li>Engineer: Integrate version control, CI/CD, registries, approvals, and monitoring to manage ML artifacts end to end.</li> </ul>"},{"location":"terms/ml-ops/#examples","title":"Examples","text":"<p>Do - Automate promotion from staging to production only after evaluation and guardrail checks pass.</p> <p>Don't - Deploy model updates outside the pipeline, bypassing logging and rollback controls.</p>"},{"location":"terms/ml-ops/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, monitoring</li> <li>Risk notes: Weak ML Ops processes increase the odds of untracked changes, bias reintroductions, and compliance failures.</li> </ul>"},{"location":"terms/ml-ops/#relationships","title":"Relationships","text":"<ul> <li>Broader: model governance</li> <li>Narrower: ml observability, model drift</li> <li>Related: fine-tuning, evaluation, guardrails</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'ml ops'.</p>"},{"location":"terms/ml-ops/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Google ML Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/ml-ops.yml</code></p>"},{"location":"terms/model-card/","title":"model card","text":""},{"location":"terms/model-card/#model-card","title":"model card","text":"<p>Aliases: model documentation, model datasheet Categories: Governance &amp; Risk, Operations &amp; Monitoring Roles: Policy &amp; Risk, Legal &amp; Compliance, Product &amp; Program Managers, Engineering &amp; Platform, Communications &amp; Enablement Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p> <p>Put it into practice</p> <p>Document changes using the Governance &amp; Risk section.</p>"},{"location":"terms/model-card/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> </ul>"},{"location":"terms/model-card/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Instrument dashboards or alerts that reflect the metrics highlighted in this definition.</li> <li>Update incident response or on-call runbooks with the glossary's do/don't scenarios.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/model-card/#short-definition","title":"Short definition","text":"<p>Standardized documentation describing a model\u2019s purpose, data, performance, and limitations.</p>"},{"location":"terms/model-card/#long-definition","title":"Long definition","text":"<p>A model card summarizes key information about an AI system, including intended use, training data sources, evaluation results (overall and across subgroups), known limitations, safety mitigations, and contact points for escalation. Model cards support transparency, reproducibility, and compliance by giving stakeholders a shared reference that travels with each release. Product managers and engineers co-author model cards during the development lifecycle, updating them after retraining, prompt changes, or mitigation work. Policy, legal, and communications teams rely on model cards to inform disclosures, customer messaging, and regulatory filings. Mature organizations treat model cards as living documents stored in version control, with automated checks verifying that new releases update relevant sections.</p>"},{"location":"terms/model-card/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: A model card is the product brief that explains what the AI is for, how it was built, and where it might misfire.</li> <li>Engineer: Document data lineage, evaluation metrics, ethical considerations, and change history; keep cards versioned alongside code.</li> </ul>"},{"location":"terms/model-card/#examples","title":"Examples","text":"<p>Do - Include subgroup metrics and fairness assessments in every model card. - Link to evaluation artifacts, incident history, and responsible AI approvals.</p> <p>Don't - Publish a model without a card\u2014reviewers and regulators will ask for it. - Let model cards go stale; update them whenever prompts or data shift.</p>"},{"location":"terms/model-card/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: documentation, accountability</li> <li>Risk notes: Missing or outdated model cards hinder audits and can delay approvals or trigger compliance findings.</li> </ul>"},{"location":"terms/model-card/#relationships","title":"Relationships","text":"<ul> <li>Broader: model governance</li> <li>Related: responsible ai, privacy impact assessment, evaluation</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'model card'.</p>"},{"location":"terms/model-card/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Google \u2013 Model Cards for Model Reporting</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/model-card.yml</code></p>"},{"location":"terms/model-drift/","title":"model drift","text":""},{"location":"terms/model-drift/#model-drift","title":"model drift","text":"<p>Aliases: distribution drift, concept drift Categories: Operations &amp; Monitoring, Governance &amp; Risk Roles: Communications &amp; Enablement, Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/model-drift/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/model-drift/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Instrument dashboards or alerts that reflect the metrics highlighted in this definition.</li> <li>Update incident response or on-call runbooks with the glossary's do/don't scenarios.</li> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/model-drift/#short-definition","title":"Short definition","text":"<p>Gradual mismatch between model assumptions and real-world data that degrades performance over time.</p>"},{"location":"terms/model-drift/#long-definition","title":"Long definition","text":"<p>Model drift arises when the data encountered in production no longer matches the distribution seen during training or evaluation. Changes in user behavior, regulations, adversarial inputs, or upstream systems can all erode accuracy and trustworthiness. Drift appears in multiple forms: data drift alters input characteristics, concept drift changes the meaning of target labels, and prior drift moves latent relationships within embeddings. Operations teams monitor dashboards for early signals using statistical tests, canary evaluations, and feedback loops. Product managers plan remediation playbooks that include prompt updates, retrieval refreshes, or new fine-tuning cycles. Governance frameworks require drift detection as part of continuous monitoring obligations, ensuring sensitive use cases receive timely reviews. Documenting drift incidents, response timelines, and residual risk assessments supports compliance and informs future model lifecycle decisions.</p>"},{"location":"terms/model-drift/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Model drift is the slow decay that happens when the world changes but the AI stays static.</li> <li>Engineer: Shift between training and production distributions; measured via statistical monitoring and addressed with retraining or prompt updates.</li> </ul>"},{"location":"terms/model-drift/#examples","title":"Examples","text":"<p>Do - Set drift thresholds and automated alerts tied to evaluation reruns for critical workflows.</p> <p>Don't - Ignore user feedback that signals degraded relevance or bias until after incidents occur.</p>"},{"location":"terms/model-drift/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: monitoring, validity</li> <li>Risk notes: Undetected drift can lead to policy violations, misaligned outputs, and regulatory non-compliance.</li> </ul>"},{"location":"terms/model-drift/#relationships","title":"Relationships","text":"<ul> <li>Broader: model governance</li> <li>Related: evaluation, guardrails, observability</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'model drift'.</p>"},{"location":"terms/model-drift/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Google ML Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/model-drift.yml</code></p>"},{"location":"terms/model-governance/","title":"model governance","text":""},{"location":"terms/model-governance/#model-governance","title":"model governance","text":"<p>Aliases: ai governance, ml governance Categories: Governance &amp; Risk Roles: Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/model-governance/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/model-governance/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/model-governance/#short-definition","title":"Short definition","text":"<p>Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.</p>"},{"location":"terms/model-governance/#long-definition","title":"Long definition","text":"<p>Model governance coordinates legal, risk, engineering, and product roles to oversee how AI systems are developed, deployed, and maintained. Core activities include documenting intended use, validating data sources, approving releases, monitoring for drift, and managing incidents. Governance frameworks\u2014such as the NIST AI RMF or ISO/IEC standards\u2014expect organizations to maintain traceability for models, prompts, datasets, and evaluation evidence. Committees or designated owners review changes, enforce segregation of duties, and ensure audits can reconstruct decisions. Product teams rely on governance guardrails to align with policy, while engineers integrate governance checkpoints into ML Ops pipelines. Without governance, AI initiatives accumulate technical debt, expose companies to regulatory penalties, and erode user trust. Mature governance balances innovation with accountability, enabling responsible scaling of AI capabilities.</p>"},{"location":"terms/model-governance/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Model governance keeps AI programs aligned with policy, compliance, and stakeholder expectations as they scale.</li> <li>Engineer: Define ownership, documentation, approvals, and monitoring requirements so every model change is auditable.</li> </ul>"},{"location":"terms/model-governance/#examples","title":"Examples","text":"<p>Do - Record decision logs and reviewers for each production model release.</p> <p>Don't - Deploy models without documenting purpose, controls, and evaluation results.</p>"},{"location":"terms/model-governance/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, risk_management</li> <li>Risk notes: Absent governance leads to untracked risks, inconsistent controls, and regulatory exposure.</li> </ul>"},{"location":"terms/model-governance/#relationships","title":"Relationships","text":"<ul> <li>Broader: responsible ai</li> <li>Narrower: ml ops, model drift, ai incident response</li> <li>Related: evaluation, alignment, guardrails</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'model governance'.</p>"},{"location":"terms/model-governance/#citations","title":"Citations","text":"<ul> <li>NIST AI Risk Management Framework</li> <li>IBM \u2013 What Is AI Governance?</li> <li>Microsoft \u2013 Responsible AI Principles</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/model-governance.yml</code></p>"},{"location":"terms/model-interpretability/","title":"model interpretability","text":""},{"location":"terms/model-interpretability/#model-interpretability","title":"model interpretability","text":"<p>Aliases: interpretability, explainability Categories: Governance &amp; Risk, Operations &amp; Monitoring Roles: Engineering &amp; Platform, Policy &amp; Risk, Legal &amp; Compliance, Product &amp; Program Managers Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p> <p>Put it into practice</p> <p>Sync with the Governance Dashboard to capture explanation plans.</p>"},{"location":"terms/model-interpretability/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/model-interpretability/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Instrument dashboards or alerts that reflect the metrics highlighted in this definition.</li> <li>Update incident response or on-call runbooks with the glossary's do/don't scenarios.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/model-interpretability/#short-definition","title":"Short definition","text":"<p>Ability to explain how a model arrives at its predictions in ways stakeholders understand.</p>"},{"location":"terms/model-interpretability/#long-definition","title":"Long definition","text":"<p>Model interpretability encompasses methods and practices that reveal why an AI system produced a particular output. Techniques range from local explanations (SHAP, LIME, token attribution) to global summaries (feature importance, surrogate models) and inherently interpretable architectures. Interpretability supports debugging, fairness audits, regulatory compliance, and customer trust. Engineering teams integrate explanation tooling into evaluation pipelines, while policy and legal stakeholders determine the level of transparency required for different products or jurisdictions. Model interpretability should be paired with documentation, human review, and responsible communication to avoid overstating confidence or exposing sensitive features.</p>"},{"location":"terms/model-interpretability/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Interpretability lets us open the black box so customers, regulators, and teams know why decisions were made.</li> <li>Engineer: Use techniques like SHAP, integrated gradients, or counterfactuals to attribute predictions; log results for audits and debugging.</li> </ul>"},{"location":"terms/model-interpretability/#examples","title":"Examples","text":"<p>Do - Provide dashboards that show top contributing features for high-risk decisions. - Validate explanations with subject-matter experts to ensure they make sense.</p> <p>Don't - Offer explanations that contradict model behavior or hide uncertainty. - Release interpretability tooling without access controls when sensitive features are involved.</p>"},{"location":"terms/model-interpretability/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, accountability</li> <li>Risk notes: Lack of interpretability undermines legal defensibility and trust; inaccurate explanations can mislead stakeholders.</li> </ul>"},{"location":"terms/model-interpretability/#relationships","title":"Relationships","text":"<ul> <li>Broader: responsible ai</li> <li>Related: model card, algorithmic bias, evaluation</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'model interpretability'.</p>"},{"location":"terms/model-interpretability/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Google ML Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/model-interpretability.yml</code></p>"},{"location":"terms/overfitting/","title":"overfitting","text":""},{"location":"terms/overfitting/#overfitting","title":"overfitting","text":"<p>Aliases: model overfitting, overtraining Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/overfitting/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/overfitting/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Add this concept to onboarding materials so teammates share a common baseline.</li> <li>Link supporting research or documentation in your internal wiki for deeper study.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/overfitting/#short-definition","title":"Short definition","text":"<p>When a model memorizes training data patterns so closely that it performs poorly on new samples.</p>"},{"location":"terms/overfitting/#long-definition","title":"Long definition","text":"<p>Overfitting occurs when a model adapts too precisely to idiosyncrasies and noise within the training data, sacrificing its ability to generalize to unseen examples. The model appears highly accurate during training but fails when exposed to validation or production inputs, leading to misleading metrics and degraded user experience. Overly complex architectures, insufficient regularization, and limited or unrepresentative datasets increase the risk. Teams detect overfitting by monitoring gaps between training and validation performance, examining learning curves, and evaluating on hold-out or cross-validation splits. Mitigation tactics include collecting more diverse data, applying regularization (dropout, weight decay), simplifying architectures, performing early stopping, or augmenting inputs. Product managers rely on overfitting diagnostics when planning rollouts, while engineers baseline mitigations before deploying models into regulated environments where failures can carry compliance impacts.</p>"},{"location":"terms/overfitting/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Overfitting is why a model can ace practice problems but stumble in front of real customers.</li> <li>Engineer: Large generalization gap between training and validation metrics caused by excessive capacity or insufficient regularization; diagnose via held-out evaluations and learning curves.</li> </ul>"},{"location":"terms/overfitting/#examples","title":"Examples","text":"<p>Do - Track validation performance for every experiment and stop training when it plateaus or declines. - Collect additional samples from underrepresented user journeys to improve generalization.</p> <p>Don't - Ship a model based solely on training accuracy without checking external benchmarks. - Ignore signs of drift that indicate the model has effectively overfit to outdated distributions.</p>"},{"location":"terms/overfitting/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, data_quality</li> <li>Risk notes: Overfit models produce inconsistent, biased outputs that can erode user trust and violate performance commitments.</li> </ul>"},{"location":"terms/overfitting/#relationships","title":"Relationships","text":"<ul> <li>Broader: model training</li> <li>Related: cross-validation, bias-variance tradeoff, evaluation</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'overfitting'.</p>"},{"location":"terms/overfitting/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/overfitting.yml</code></p>"},{"location":"terms/precision/","title":"precision","text":""},{"location":"terms/precision/#precision","title":"precision","text":"<p>Aliases: positive predictive value, ppv Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers, Policy &amp; Risk Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/precision/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> </ul>"},{"location":"terms/precision/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Add this concept to onboarding materials so teammates share a common baseline.</li> <li>Link supporting research or documentation in your internal wiki for deeper study.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/precision/#short-definition","title":"Short definition","text":"<p>Share of predicted positives that are actually correct for a given classifier.</p>"},{"location":"terms/precision/#long-definition","title":"Long definition","text":"<p>Precision measures how often a model\u2019s positive predictions are right. It is calculated as true positives divided by the sum of true positives and false positives. Precision is critical when false positives carry high costs\u2014such as flagging legitimate transactions as fraud or routing harmless content to safety reviewers. Product managers examine precision alongside recall to balance user trust and coverage, while engineers monitor precision across subgroups to detect drift and bias. Governance teams require precision reporting in regulated workflows where incorrect flags can create legal exposure. Precision is best interpreted in context with other metrics and operational goals, since maximizing it alone may reduce recall and harm users the model fails to detect.</p>"},{"location":"terms/precision/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Precision tells you how many of the alerts the AI raises are actually right.</li> <li>Engineer: TP / (TP + FP); inspect per-class and per-segment precision, and monitor trade-offs with recall and business cost.</li> </ul>"},{"location":"terms/precision/#examples","title":"Examples","text":"<p>Do - Track precision at the thresholds used in production, not just at 0.5. - Compare precision across demographics to uncover unfair false-positive rates.</p> <p>Don't - Optimize precision without checking the drop in recall. - Report a single precision value when different user segments behave differently.</p>"},{"location":"terms/precision/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, accountability</li> <li>Risk notes: Low precision increases false alarms, eroding trust and causing unnecessary interventions.</li> </ul>"},{"location":"terms/precision/#relationships","title":"Relationships","text":"<ul> <li>Broader: evaluation</li> <li>Related: recall, f1 score, confusion matrix</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'precision'.</p>"},{"location":"terms/precision/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/precision.yml</code></p>"},{"location":"terms/preference-dataset/","title":"preference dataset","text":""},{"location":"terms/preference-dataset/#preference-dataset","title":"preference dataset","text":"<p>Aliases: preference data, human feedback dataset Categories: LLM Core Roles: Data Science &amp; Research, Policy &amp; Risk, Product &amp; Program Managers, Engineering &amp; Platform Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/preference-dataset/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/preference-dataset/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/preference-dataset/#short-definition","title":"Short definition","text":"<p>Labeled comparisons of model outputs that capture which responses humans prefer.</p>"},{"location":"terms/preference-dataset/#long-definition","title":"Long definition","text":"<p>A preference dataset contains prompts, model outputs, and human annotations indicating which output better satisfies instructions or policies. Teams use it to train reward models or perform direct preference optimization. Gathering preference data requires clear labeling rubrics, diverse annotators, and safeguards for sensitive content. Policy teams define decision criteria, product teams ensure tone and UX requirements are reflected, and engineers manage secure tooling for annotation. Poorly managed preference data can leak personal information or encode unintended bias that later influences RLHF or reranking systems.</p>"},{"location":"terms/preference-dataset/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Protect preference data like any user research asset\u2014it shapes model behavior and compliance.</li> <li>Engineer: Track lineage from annotation tools to training pipelines and enforce access controls.</li> </ul>"},{"location":"terms/preference-dataset/#examples","title":"Examples","text":"<p>Do - Capture rationale alongside preferences so reviewers understand annotations. - Audit samples regularly for bias or policy drift.</p> <p>Don't - Mix production user data into preference datasets without consent. - Allow annotators to work without updated safety guidelines.</p>"},{"location":"terms/preference-dataset/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, risk_management, privacy</li> <li>Risk notes: Lax controls can leak sensitive prompts or entrench biased judgments into downstream models.</li> </ul>"},{"location":"terms/preference-dataset/#relationships","title":"Relationships","text":"<ul> <li>Broader: reinforcement learning from human feedback</li> <li>Related: reward model, instruction tuning, risk register</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'preference dataset'.</p>"},{"location":"terms/preference-dataset/#citations","title":"Citations","text":"<ul> <li>OpenAI \u2013 InstructGPT</li> <li>Microsoft \u2013 Responsible AI Principles</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/preference-dataset.yml</code></p>"},{"location":"terms/privacy-budget/","title":"privacy budget","text":""},{"location":"terms/privacy-budget/#privacy-budget","title":"privacy budget","text":"<p>Aliases: epsilon budget, differential privacy budget Categories: Governance &amp; Risk Roles: Data Science &amp; Research, Policy &amp; Risk, Legal &amp; Compliance, Engineering &amp; Platform Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/privacy-budget/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/privacy-budget/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/privacy-budget/#short-definition","title":"Short definition","text":"<p>Quantitative limit on how much privacy loss is allowed when applying differential privacy.</p>"},{"location":"terms/privacy-budget/#long-definition","title":"Long definition","text":"<p>A privacy budget sets the maximum cumulative privacy loss (often parameterized by epsilon) that an organization will tolerate when querying or training with differentially private mechanisms. Each query or training run consumes part of the budget, so teams must track usage and halt processing when the limit is reached. Policy and legal leaders define acceptable thresholds, data scientists monitor consumption, and engineers enforce the budget in pipelines. Without budget tracking, repeated queries can quietly degrade privacy guarantees beyond regulatory or ethical bounds.</p>"},{"location":"terms/privacy-budget/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Treat the privacy budget as a compliance control that requires monitoring and governance.</li> <li>Engineer: Instrument analytics and training jobs to decrement the budget automatically and alert owners.</li> </ul>"},{"location":"terms/privacy-budget/#examples","title":"Examples","text":"<p>Do - Document initial and remaining epsilon values alongside each release. - Aggregate similar queries to conserve budget and reduce risk.</p> <p>Don't - Ignore budget depletion warnings for critical user data. - Share raw event logs when the privacy budget reaches zero.</p>"},{"location":"terms/privacy-budget/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: privacy, risk_management, governance</li> <li>Risk notes: Overrunning the budget invalidates differential privacy claims and increases legal exposure.</li> </ul>"},{"location":"terms/privacy-budget/#relationships","title":"Relationships","text":"<ul> <li>Broader: differential privacy</li> <li>Related: data minimization, consent management, risk register</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'privacy budget'.</p>"},{"location":"terms/privacy-budget/#citations","title":"Citations","text":"<ul> <li>Harvard Privacy Tools \u2013 Differential Privacy</li> <li>Apple \u2013 Learning with Privacy Budget</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/privacy-budget.yml</code></p>"},{"location":"terms/privacy-impact-assessment/","title":"privacy impact assessment","text":""},{"location":"terms/privacy-impact-assessment/#privacy-impact-assessment","title":"privacy impact assessment","text":"<p>Aliases: pia, data protection impact assessment Categories: Governance &amp; Risk Roles: Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/privacy-impact-assessment/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/privacy-impact-assessment/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/privacy-impact-assessment/#short-definition","title":"Short definition","text":"<p>Structured review that evaluates how a system collects, uses, and safeguards personal data.</p>"},{"location":"terms/privacy-impact-assessment/#long-definition","title":"Long definition","text":"<p>A privacy impact assessment (PIA) identifies and mitigates privacy risks before launching or materially changing a product, dataset, or model. The process documents data sources, lawful bases, retention policies, third-party sharing, and safeguards such as encryption or differential privacy. PIAs often involve cross-functional stakeholders\u2014legal, security, engineering, product, and compliance\u2014to ensure controls meet regulatory requirements like GDPR, CCPA, or sector-specific rules. For AI systems, PIAs examine training data provenance, prompt logging, telemetry retention, and user disclosure obligations. Findings feed into risk registers, customer communications, and incident response plans. Conducting PIAs early reduces costly redesigns and strengthens trust with regulators, customers, and internal reviewers.</p>"},{"location":"terms/privacy-impact-assessment/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: A PIA is the privacy due diligence that keeps AI launches compliant and defensible.</li> <li>Engineer: Provide technical details on data flows, storage, and safeguards so legal can verify privacy controls.</li> </ul>"},{"location":"terms/privacy-impact-assessment/#examples","title":"Examples","text":"<p>Do - Trigger a PIA whenever new personal data sources or model capabilities are introduced. - Track mitigation tasks from the PIA in your product backlog until resolved.</p> <p>Don't - Treat PIAs as paperwork; revisit them after model updates or incidents. - Launch features that collect personal data without completing the assessment.</p>"},{"location":"terms/privacy-impact-assessment/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: privacy, documentation</li> <li>Risk notes: Skipping PIAs can violate legal obligations, resulting in fines or mandatory shutdowns.</li> </ul>"},{"location":"terms/privacy-impact-assessment/#relationships","title":"Relationships","text":"<ul> <li>Broader: model governance</li> <li>Related: differential privacy, incident response, guardrails</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'privacy impact assessment'.</p>"},{"location":"terms/privacy-impact-assessment/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Wikipedia \u2013 Data Protection Impact Assessment</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/privacy-impact-assessment.yml</code></p>"},{"location":"terms/privacy/","title":"privacy","text":""},{"location":"terms/privacy/#privacy","title":"privacy","text":"<p>Aliases: data privacy, information privacy Categories: Governance &amp; Risk Roles: Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/privacy/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/privacy/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/privacy/#short-definition","title":"Short definition","text":"<p>Principle of limiting data collection, use, and exposure to protect individuals\u2019 information.</p>"},{"location":"terms/privacy/#long-definition","title":"Long definition","text":"<p>Privacy in AI systems focuses on controlling how personal or sensitive data is collected, processed, and retained. Regulations such as GDPR, CCPA, and sector-specific laws establish legal obligations for transparency, consent, minimization, and user rights. In machine learning, privacy risks arise from training data, prompts, logs, and model outputs that may reveal personal information. Teams mitigate these risks through data minimization, access controls, anonymization, synthetic data, and privacy-enhancing technologies like differential privacy. Product leaders coordinate privacy reviews with legal counsel when launching new features, while engineers implement safeguards in data pipelines, storage, and logging. Governance programs track privacy impact assessments, retention schedules, and incident response plans. Maintaining rigorous privacy practices preserves user trust and avoids regulatory penalties, making it a foundational requirement for responsible AI deployments.</p>"},{"location":"terms/privacy/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Privacy keeps user data protected and ensures AI initiatives comply with laws and customer expectations.</li> <li>Engineer: Limit collection, apply technical controls, and document handling of personal data across training and inference workflows.</li> </ul>"},{"location":"terms/privacy/#examples","title":"Examples","text":"<p>Do - Run privacy impact assessments before ingesting new data sources for fine-tuning.</p> <p>Don't - Log raw prompts that contain personal identifiers without redaction or retention limits.</p>"},{"location":"terms/privacy/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: privacy, risk_management</li> <li>Risk notes: Weak privacy controls can expose personal data, triggering legal liability and loss of trust.</li> </ul>"},{"location":"terms/privacy/#relationships","title":"Relationships","text":"<ul> <li>Broader: responsible ai</li> <li>Related: guardrails, model governance, incident response</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'privacy'.</p>"},{"location":"terms/privacy/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>OECD \u2013 Privacy Guidelines</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/privacy.yml</code></p>"},{"location":"terms/prompt-engineering/","title":"prompt engineering","text":""},{"location":"terms/prompt-engineering/#prompt-engineering","title":"prompt engineering","text":"<p>Aliases: prompt design, prompt scripting Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p> <p>Put it into practice</p> <p>Dive into the Prompt Engineering Playbook for workflows and checklists.</p>"},{"location":"terms/prompt-engineering/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/prompt-engineering/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/prompt-engineering/#short-definition","title":"Short definition","text":"<p>Crafting and testing prompts to steer model behavior toward desired outcomes.</p>"},{"location":"terms/prompt-engineering/#long-definition","title":"Long definition","text":"<p>Prompt engineering involves designing the instructions, examples, and context provided to a language model so it produces reliable, policy-compliant responses. Practitioners experiment with phrasing, ordering, few-shot examples, and formatting cues to influence reasoning steps or output structure. The discipline has evolved from ad-hoc experimentation to a structured workflow that includes prompt libraries, evaluation harnesses, and version control. Prompt engineers collaborate with product, legal, and safety teams to encode business rules, tone, and disallowed behaviors, often in tandem with system prompts and guardrails. They also balance token budgets against performance, ensuring prompts fit within context windows while still providing the necessary guidance. Governance functions expect prompt changes to pass through review queues, since a seemingly small tweak can alter safety posture or introduce bias.</p>"},{"location":"terms/prompt-engineering/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Prompt engineering is how we teach the model to act like our product or policy experts.</li> <li>Engineer: Systematic design of prompt structure, exemplars, and metadata; evaluated via regression suites to manage behavior drift.</li> </ul>"},{"location":"terms/prompt-engineering/#examples","title":"Examples","text":"<p>Do - Version prompts and run automated evals before rolling them out to production flows.</p> <p>Don't - Copy prompts between domains without revisiting legal and safety requirements.</p>"},{"location":"terms/prompt-engineering/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, transparency</li> <li>Risk notes: Unreviewed prompt changes can bypass controls and result in unsafe or misleading outputs.</li> </ul>"},{"location":"terms/prompt-engineering/#relationships","title":"Relationships","text":"<ul> <li>Broader: human-in-the-loop</li> <li>Narrower: few-shot prompting, chain-of-thought prompting</li> <li>Related: system prompt, guardrails, evaluation</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'prompt engineering'.</p>"},{"location":"terms/prompt-engineering/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Learn Prompting \u2013 Introduction</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/prompt-engineering.yml</code></p>"},{"location":"terms/prompt-injection/","title":"prompt injection","text":""},{"location":"terms/prompt-injection/#prompt-injection","title":"prompt injection","text":"<p>Aliases: prompt attack, context hijacking Categories: Governance &amp; Risk Roles: Security &amp; Trust, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/prompt-injection/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/prompt-injection/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/prompt-injection/#short-definition","title":"Short definition","text":"<p>Attack that inserts malicious instructions into model inputs to override original prompts or policies.</p>"},{"location":"terms/prompt-injection/#long-definition","title":"Long definition","text":"<p>Prompt injection is an adversarial technique where attackers craft inputs that smuggle new instructions into a model's context. The injected text attempts to override safety policies, steal secrets, or trigger unintended actions, such as exfiltrating data from tools connected to the agent. The attack can appear in user text, retrieved documents, or API responses that the model reads. Security teams treat prompt injection like command injection: sanitizing inputs, isolating tools, and limiting model permissions. Product teams communicate when defensive refusals occur, and engineers add filters plus evaluation harnesses that test for jailbreaks. Without mitigations, attackers can bypass guardrails or weaponize the model to call sensitive tools.</p>"},{"location":"terms/prompt-injection/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Track prompt injection risks the same way you monitor phishing and social engineering threats.</li> <li>Engineer: Sanitize retrieved context, restrict tool scopes, and simulate attacks continuously.</li> </ul>"},{"location":"terms/prompt-injection/#examples","title":"Examples","text":"<p>Do - Strip or quarantine untrusted instructions before feeding documents into the agent. - Log and share injection attempts with the security incident response team.</p> <p>Don't - Allow the model to treat external data sources as authoritative without validation. - Expose long-lived secrets or admin tools to untrusted prompts.</p>"},{"location":"terms/prompt-injection/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: security, risk_management, monitoring</li> <li>Risk notes: Successful injections can leak data, perform unauthorized actions, or undermine governance commitments.</li> </ul>"},{"location":"terms/prompt-injection/#relationships","title":"Relationships","text":"<ul> <li>Broader: guardrails, robust prompting</li> <li>Related: jailbreak prompt, tool use, ai incident response</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'prompt injection'.</p>"},{"location":"terms/prompt-injection/#citations","title":"Citations","text":"<ul> <li>OWASP \u2013 LLM Top 10</li> <li>arXiv \u2013 Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection</li> <li>Prompt Injection Attacks and Defenses</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/prompt-injection.yml</code></p>"},{"location":"terms/quantization/","title":"quantization","text":""},{"location":"terms/quantization/#quantization","title":"quantization","text":"<p>Aliases: model quantization, weight quantization Categories: Optimization &amp; Efficiency Roles: Data Science &amp; Research, Engineering &amp; Platform Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/quantization/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/quantization/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Record before-and-after performance metrics when applying this optimisation technique.</li> <li>Document trade-offs for product and policy partners using the glossary's language.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/quantization/#short-definition","title":"Short definition","text":"<p>Technique that compresses model weights into lower-precision formats to shrink size and speed inference.</p>"},{"location":"terms/quantization/#long-definition","title":"Long definition","text":"<p>Quantization converts neural network parameters and activations from high-precision floating point representations (such as FP32) into lower bit-width formats (such as INT8 or INT4) to reduce memory footprint and accelerate inference. By mapping continuous values into a discrete set, quantization enables models to run on cost-sensitive hardware, deliver faster responses, and consume less energy, which is critical when deploying large language models at scale or on edge devices. Engineers choose between post-training quantization, which calibrates a frozen model on representative data, and quantization-aware training, which simulates low-precision behavior during fine-tuning to preserve accuracy. Careful evaluation is required to understand the trade-offs: aggressive quantization can introduce numerical instability, harm latency determinism, or amplify bias if calibration data under-represents certain groups. Successful programs pair quantization with monitoring, backstops such as higher-precision fallbacks, and documentation that makes these trade-offs explicit to stakeholders.</p>"},{"location":"terms/quantization/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: A cost-control lever: shrink model footprints so you can serve more traffic on existing hardware.</li> <li>Engineer: Apply per-tensor or per-channel scaling, choose symmetric/asymmetric schemes, and validate perplexity and latency post-quantization.</li> </ul>"},{"location":"terms/quantization/#examples","title":"Examples","text":"<p>Do - Benchmark accuracy and latency before and after quantization to document the trade-offs. - Use representative calibration datasets that include edge cases and demographic variation.</p> <p>Don't - Quantize safety-critical models without fallback paths or runtime monitoring. - Assume INT4 settings will work across architectures without profiling.</p>"},{"location":"terms/quantization/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: efficiency, robustness, documentation</li> <li>Risk notes: Quantization can reduce accuracy or shift error distribution; record evaluations and obtain stakeholder sign-off.</li> </ul>"},{"location":"terms/quantization/#relationships","title":"Relationships","text":"<ul> <li>Broader: model optimization</li> <li>Narrower: post-training quantization, quantization-aware training</li> <li>Related: compression, distillation, hardware acceleration</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'quantization'.</p>"},{"location":"terms/quantization/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/quantization.yml</code></p>"},{"location":"terms/recall/","title":"recall","text":""},{"location":"terms/recall/#recall","title":"recall","text":"<p>Aliases: sensitivity, true positive rate Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers, Policy &amp; Risk Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/recall/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> </ul>"},{"location":"terms/recall/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Add this concept to onboarding materials so teammates share a common baseline.</li> <li>Link supporting research or documentation in your internal wiki for deeper study.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/recall/#short-definition","title":"Short definition","text":"<p>Share of actual positives a model successfully identifies.</p>"},{"location":"terms/recall/#long-definition","title":"Long definition","text":"<p>Recall measures how well a model finds all relevant cases by dividing true positives by the sum of true positives and false negatives. High recall is essential when missing a positive case is costly or dangerous, such as failing to flag offensive content or overlooking a critical defect. When teams adjust thresholds, recall often moves in the opposite direction of precision, so decision-makers must weigh acceptable trade-offs. Engineers analyze recall per subgroup to ensure equitable performance, while policy and product teams set recall targets aligned with risk appetite and regulatory requirements. Recall is commonly monitored in tandem with precision, F1 score, and confusion matrices to understand overall effectiveness.</p>"},{"location":"terms/recall/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Recall is how many of the real problems the AI actually catches.</li> <li>Engineer: TP / (TP + FN); evaluate recall across classes and user cohorts, and tune thresholds or sampling to meet coverage goals.</li> </ul>"},{"location":"terms/recall/#examples","title":"Examples","text":"<p>Do - Set minimum recall thresholds for safety-critical detections. - Monitor recall daily to catch regressions caused by data drift.</p> <p>Don't - Maximize recall without reviewing the spike in false positives. - Report aggregate recall without subgroup breakdowns.</p>"},{"location":"terms/recall/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, accountability</li> <li>Risk notes: Poor recall leaves high-risk events undetected, undermining safety promises and compliance obligations.</li> </ul>"},{"location":"terms/recall/#relationships","title":"Relationships","text":"<ul> <li>Broader: evaluation</li> <li>Related: precision, f1 score, confusion matrix</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'recall'.</p>"},{"location":"terms/recall/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/recall.yml</code></p>"},{"location":"terms/red-teaming/","title":"red teaming","text":""},{"location":"terms/red-teaming/#red-teaming","title":"red teaming","text":"<p>Aliases: ai red teaming, adversarial testing Categories: Governance &amp; Risk, Operations &amp; Monitoring Roles: Communications &amp; Enablement, Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/red-teaming/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/red-teaming/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Instrument dashboards or alerts that reflect the metrics highlighted in this definition.</li> <li>Update incident response or on-call runbooks with the glossary's do/don't scenarios.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/red-teaming/#short-definition","title":"Short definition","text":"<p>Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.</p>"},{"location":"terms/red-teaming/#long-definition","title":"Long definition","text":"<p>Red teaming mobilizes interdisciplinary experts to craft adversarial prompts, scenarios, and data inputs that challenge an AI system\u2019s safeguards. The goal is to uncover failure modes\u2014such as unsafe content, confidential data leaks, or jailbreak exploits\u2014before attackers or end users discover them. Exercises blend automated probing, scripted attack playbooks, and human creativity. Findings feed into remediation plans for guardrails, prompts, training data, or escalation policies. Product leaders schedule recurring red-team cycles for high-risk surfaces, while engineers build tooling to log attempts, reproduce issues, and verify fixes. Governance teams treat red teaming as part of risk management, requiring documentation of scope, participants, severity ratings, and follow-up actions. In many jurisdictions, regulators expect evidence that red teaming has been performed for sensitive deployments, making it a core component of responsible AI programs.</p>"},{"location":"terms/red-teaming/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Red teaming is the pre-launch fire drill that exposes how the AI could fail or be abused.</li> <li>Engineer: Design adversarial prompts and automated probes, capture reproduction artifacts, and track mitigation work in the backlog.</li> </ul>"},{"location":"terms/red-teaming/#examples","title":"Examples","text":"<p>Do - Incorporate marginalized community perspectives when designing red-team scenarios.</p> <p>Don't - Close a red-team finding without documenting remediation owners and timelines.</p>"},{"location":"terms/red-teaming/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: risk_management, accountability</li> <li>Risk notes: Skipping red teaming leaves blind spots that can result in public incidents or regulatory enforcement.</li> </ul>"},{"location":"terms/red-teaming/#relationships","title":"Relationships","text":"<ul> <li>Broader: evaluation</li> <li>Related: guardrails, alignment, incident response</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'red teaming'.</p>"},{"location":"terms/red-teaming/#citations","title":"Citations","text":"<ul> <li>CISA \u2013 AI Security Incident Response Guidelines</li> <li>OpenAI \u2013 Red Teaming Network</li> <li>Wikipedia \u2013 Red Team</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/red-teaming.yml</code></p>"},{"location":"terms/regularization/","title":"regularization","text":""},{"location":"terms/regularization/#regularization","title":"regularization","text":"<p>Aliases: penalisation, weight decay Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/regularization/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/regularization/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Add this concept to onboarding materials so teammates share a common baseline.</li> <li>Link supporting research or documentation in your internal wiki for deeper study.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/regularization/#short-definition","title":"Short definition","text":"<p>Techniques that add penalties or constraints during training to reduce overfitting and improve generalisation.</p>"},{"location":"terms/regularization/#long-definition","title":"Long definition","text":"<p>Regularization tempers a model\u2019s tendency to memorise training data by introducing penalties or architectural constraints that favour simpler solutions. Common approaches include L1 and L2 weight penalties, dropout, early stopping, and data augmentation. Product teams see regularization as a lever for keeping performance stable after launch, while engineers adjust penalties to balance accuracy with latency and reproducibility. Governance and policy partners rely on documented regularization strategies to evidence that the organisation actively manages model complexity and drift risks. Without regularization, models often exhibit inflated offline metrics that fail to generalise to new markets, languages, or customer segments.</p>"},{"location":"terms/regularization/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Regularization is how we stop models from overfitting so they keep working on new data.</li> <li>Engineer: Apply penalties such as lambda * ||w||^2, dropout masks, or early stopping schedules to manage bias-variance trade-offs.</li> </ul>"},{"location":"terms/regularization/#examples","title":"Examples","text":"<p>Do - Log regularization settings alongside other hyperparameters for every experiment. - Review fairness and calibration metrics after tightening or relaxing penalties.</p> <p>Don't - Disable regularization in production experiments without compensating monitoring. - Rely on default penalty strengths when data distributions or architectures change.</p>"},{"location":"terms/regularization/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: robustness, governance</li> <li>Risk notes: Lack of regularization increases drift and fairness risks, triggering costly remediation when models are audited.</li> </ul>"},{"location":"terms/regularization/#relationships","title":"Relationships","text":"<ul> <li>Related: gradient descent, loss function, overfitting</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'regularization'.</p>"},{"location":"terms/regularization/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>DeepLearning.AI Resources</li> <li>Google ML Crash Course \u2013 Regularization</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/regularization.yml</code></p>"},{"location":"terms/reinforcement-learning-from-human-feedback/","title":"reinforcement learning from human feedback","text":""},{"location":"terms/reinforcement-learning-from-human-feedback/#reinforcement-learning-from-human-feedback","title":"reinforcement learning from human feedback","text":"<p>Aliases: rlhf, preference optimization Categories: LLM Core Roles: Engineering &amp; Platform, Data Science &amp; Research, Product &amp; Program Managers, Policy &amp; Risk Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/reinforcement-learning-from-human-feedback/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> </ul>"},{"location":"terms/reinforcement-learning-from-human-feedback/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/reinforcement-learning-from-human-feedback/#short-definition","title":"Short definition","text":"<p>Training approach that tunes a model using reward signals learned from human preference data.</p>"},{"location":"terms/reinforcement-learning-from-human-feedback/#long-definition","title":"Long definition","text":"<p>Reinforcement learning from human feedback (RLHF) trains a reward model on human preference comparisons, then uses that reward model to fine-tune a base language model with reinforcement learning. The process typically involves collecting prompts and multiple responses, asking human labelers which response better follows policy, training a reward model on those rankings, and running a reinforcement learning algorithm (often PPO) to optimize the model\u2019s behavior. RLHF aligns outputs with human expectations when explicit rules are difficult to encode. Product teams define the guidelines labelers follow, policy teams ensure safety requirements are reflected, and engineers monitor for reward hacking or regressions. RLHF requires continuous refreshes as policies evolve; stale preference data can drift from current standards.</p>"},{"location":"terms/reinforcement-learning-from-human-feedback/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Budget for ongoing RLHF cycles so the model keeps pace with policy and product shifts.</li> <li>Engineer: Instrument reward signals, evaluation sets, and safety metrics to catch regressions early.</li> </ul>"},{"location":"terms/reinforcement-learning-from-human-feedback/#examples","title":"Examples","text":"<p>Do - Collect diverse preference data that captures edge cases and sensitive topics. - Audit reward model calibration to prevent exploiting annotation quirks.</p> <p>Don't - Assume one RLHF pass will keep a model aligned indefinitely. - Let annotator instructions diverge from published safety or product policies.</p>"},{"location":"terms/reinforcement-learning-from-human-feedback/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: risk_management, monitoring, transparency</li> <li>Risk notes: Poorly curated feedback can encode bias or create reward hacking that violates policy.</li> </ul>"},{"location":"terms/reinforcement-learning-from-human-feedback/#relationships","title":"Relationships","text":"<ul> <li>Broader: fine-tuning</li> <li>Related: reward model, alignment, robust prompting</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'reinforcement learning from human feedback'.</p>"},{"location":"terms/reinforcement-learning-from-human-feedback/#citations","title":"Citations","text":"<ul> <li>OpenAI \u2013 Learning from Human Preferences</li> <li>InstructGPT: Training Language Models to Follow Instructions</li> <li>Hugging Face \u2013 RLHF Guide</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/reinforcement-learning-from-human-feedback.yml</code></p>"},{"location":"terms/repetition-penalty/","title":"repetition penalty","text":""},{"location":"terms/repetition-penalty/#repetition-penalty","title":"repetition penalty","text":"<p>Aliases: anti-repetition penalty, token penalty Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p> <p>Put it into practice</p> <p>Use the Prompt Engineering Playbook to balance repetition controls.</p>"},{"location":"terms/repetition-penalty/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/repetition-penalty/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/repetition-penalty/#short-definition","title":"Short definition","text":"<p>Decoding adjustment that down-weights tokens already generated to reduce loops and repeated phrases.</p>"},{"location":"terms/repetition-penalty/#long-definition","title":"Long definition","text":"<p>A repetition penalty rescales token probabilities during decoding so words that have already appeared become less likely to repeat. Implementations typically divide or multiply logits by a penalty factor greater than one, discouraging the model from reusing identical phrases while preserving the rest of the distribution. Product teams enable repetition penalties to prevent user-facing chatbots from producing redundant or endless loops, particularly in multilingual or code-heavy contexts where repetition can spike. Engineers tune separate penalties for input prompts versus generated output, and combine them with stop sequences to honour formatting requirements. Governance stakeholders log penalty settings because altering them can invalidate safety and quality evaluations\u2014lowering the penalty risks repetitive harmful content, whereas an excessively high penalty may distort meaning or remove essential disclosures. Monitoring repetition metrics in production helps confirm the chosen value remains effective as models, prompts, or content domains evolve.</p>"},{"location":"terms/repetition-penalty/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Think of the repetition penalty as the guardrail that keeps conversations from getting stuck in loops.</li> <li>Engineer: Scale logits for previously used tokens by a factor (e.g., 1.1\u20131.2) before sampling to discourage repeats without breaking coherence.</li> </ul>"},{"location":"terms/repetition-penalty/#examples","title":"Examples","text":"<p>Do - Track repetition rate metrics alongside hallucination incidents after changing penalty values. - Differentiate penalties for system prompts versus user-visible responses.</p> <p>Don't - Set the penalty so high that critical disclaimers or citations are removed from answers. - Forget to document penalty changes when comparing evaluation runs.</p>"},{"location":"terms/repetition-penalty/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: robustness, transparency</li> <li>Risk notes: Aggressive penalties can alter validated answer formats; coordinate with policy and QA before rollout.</li> </ul>"},{"location":"terms/repetition-penalty/#relationships","title":"Relationships","text":"<ul> <li>Broader: decoding</li> <li>Related: temperature, top-p sampling, beam search</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'repetition penalty'.</p>"},{"location":"terms/repetition-penalty/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/repetition-penalty.yml</code></p>"},{"location":"terms/reranking/","title":"reranking","text":""},{"location":"terms/reranking/#reranking","title":"reranking","text":"<p>Aliases: re-ranking, second-stage ranking Categories: Retrieval &amp; RAG Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/reranking/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/reranking/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Validate retrieval quality using the evaluation guidance referenced in this entry.</li> <li>Ensure knowledge sources named here appear in your data governance inventory.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/reranking/#short-definition","title":"Short definition","text":"<p>Step that refines retrieval results using a more precise but slower scoring model.</p>"},{"location":"terms/reranking/#long-definition","title":"Long definition","text":"<p>Reranking applies a secondary model to an initial set of retrieved documents to reorder them according to task-specific relevance. The first retrieval pass prioritizes speed, often using sparse or dense similarity search to gather dozens of candidates. A reranker\u2014frequently a cross-encoder or lightweight generative model\u2014then evaluates each candidate alongside the query to produce fine-grained scores. This approach improves precision, supports citation quality in RAG, and helps guardrails by pushing irrelevant or risky passages down the list. Engineers tune rerankers by limiting candidate set sizes, caching scores, and measuring latency budgets. Governance considerations include documenting model provenance, ensuring the reranker does not encode discriminatory bias, and verifying that safety filters still apply after reordering. When reranking is well managed, it improves both factual accuracy and user trust without incurring the cost of fully generative evaluation for every document in the corpus.</p>"},{"location":"terms/reranking/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Reranking lets us double-check the top answers so users see the most reliable sources first.</li> <li>Engineer: Second-stage scorer\u2014often a cross-encoder\u2014reorders retrieved candidates to boost precision while respecting latency limits.</li> </ul>"},{"location":"terms/reranking/#examples","title":"Examples","text":"<p>Do - Measure precision@k with and without reranking to justify added latency.</p> <p>Don't - Deploy rerankers trained on outdated data without monitoring for regressions in safety filters.</p>"},{"location":"terms/reranking/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, transparency</li> <li>Risk notes: Biased rerankers can systematically down-rank protected viewpoints or surface outdated information.</li> </ul>"},{"location":"terms/reranking/#relationships","title":"Relationships","text":"<ul> <li>Broader: retrieval</li> <li>Narrower: cross-encoder reranking</li> <li>Related: retrieval-augmented generation, chunking, guardrails</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'reranking'.</p>"},{"location":"terms/reranking/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/reranking.yml</code></p>"},{"location":"terms/responsible-ai/","title":"responsible ai","text":""},{"location":"terms/responsible-ai/#responsible-ai","title":"responsible ai","text":"<p>Aliases: trustworthy ai, ethical ai Categories: Governance &amp; Risk Roles: Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/responsible-ai/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/responsible-ai/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/responsible-ai/#short-definition","title":"Short definition","text":"<p>Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.</p>"},{"location":"terms/responsible-ai/#long-definition","title":"Long definition","text":"<p>Responsible AI encompasses the policies, technical controls, and cultural norms that guide how AI is built and deployed. It integrates principles such as fairness, transparency, accountability, privacy, and security into each phase of the model lifecycle. Organizations operationalize responsible AI through governance committees, risk assessments, red teaming, documentation standards, and inclusive design processes. Engineering teams implement safeguards like guardrails, evaluation suites, and monitoring to enforce these principles. Product and legal leaders translate regulatory requirements and stakeholder expectations into practical guardrails and disclosures. Responsible AI is not a single project but an ongoing discipline that adapts as technology and regulations evolve. By grounding innovations in responsible AI, organizations increase user trust, reduce liability, and create sustainable value.</p>"},{"location":"terms/responsible-ai/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Responsible AI ensures innovation progresses with safeguards that protect people and the business.</li> <li>Engineer: Embed fairness, safety, privacy, and accountability into data, model, and deployment workflows.</li> </ul>"},{"location":"terms/responsible-ai/#examples","title":"Examples","text":"<p>Do - Include responsible AI reviews in the release checklist for every high-impact feature.</p> <p>Don't - Treat responsible AI as a post-launch audit instead of a lifecycle commitment.</p>"},{"location":"terms/responsible-ai/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: risk_management, accountability</li> <li>Risk notes: Ignoring responsible AI principles invites regulatory action, reputational harm, and inequitable outcomes.</li> </ul>"},{"location":"terms/responsible-ai/#relationships","title":"Relationships","text":"<ul> <li>Broader: artificial intelligence</li> <li>Narrower: model governance, alignment, guardrails</li> <li>Related: red teaming, evaluation, privacy</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'responsible ai'.</p>"},{"location":"terms/responsible-ai/#citations","title":"Citations","text":"<ul> <li>OECD \u2013 AI Principles</li> <li>IBM \u2013 AI Ethics</li> <li>White House \u2013 Blueprint for an AI Bill of Rights</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/responsible-ai.yml</code></p>"},{"location":"terms/retrieval-augmented-generation/","title":"retrieval-augmented generation","text":""},{"location":"terms/retrieval-augmented-generation/#retrieval-augmented-generation","title":"retrieval-augmented generation","text":"<p>Aliases: RAG, retrieval augmented generation Categories: Retrieval &amp; RAG Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p> <p>Put it into practice</p> <p>Consult the Category Explorer for end-to-end grounding guidance.</p>"},{"location":"terms/retrieval-augmented-generation/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/retrieval-augmented-generation/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Validate retrieval quality using the evaluation guidance referenced in this entry.</li> <li>Ensure knowledge sources named here appear in your data governance inventory.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/retrieval-augmented-generation/#short-definition","title":"Short definition","text":"<p>Workflow that grounds a generative model with retrieved context before producing output.</p>"},{"location":"terms/retrieval-augmented-generation/#long-definition","title":"Long definition","text":"<p>Retrieval-augmented generation (RAG) combines information retrieval and text generation so that a model answers questions using relevant, up-to-date context instead of only relying on patterns memorized during pretraining. A retrieval component first decomposes the user query and searches a corpus\u2014via sparse, dense, or hybrid methods\u2014to surface the most relevant passages. Those passages are then packaged, often alongside metadata like source citations, and injected into the model prompt or context window prior to decoding. The approach reduces hallucinations, improves factual accuracy, and enables small models to compete with larger ones when grounded knowledge is more important than general fluency. Successful RAG systems invest in document chunking, embedding quality, reranking, and evaluation pipelines that monitor both retrieval recall and answer precision. Governance teams value RAG because it enables transparent sourcing and can encode organizational policies in the retrieval layer, but they also monitor for drift, stale content, and privacy leaks when sensitive documents are indexed.</p>"},{"location":"terms/retrieval-augmented-generation/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Treat it as a guardrail: the system fetches trusted documents before the model speaks, so answers stay on-policy.</li> <li>Engineer: Pipeline = query rewriting, retriever (BM25, DPR, hybrid), reranker, and generator with retrieved context appended to the prompt.</li> </ul>"},{"location":"terms/retrieval-augmented-generation/#examples","title":"Examples","text":"<p>Do - Version and monitor the document index so you can trace outputs back to specific sources. - Evaluate retrieval recall and answer accuracy separately to isolate failure modes.</p> <p>Don't - Assume RAG removes the need for model fine-tuning or ongoing evaluation. - Index sensitive records without access controls or data minimization.</p>"},{"location":"terms/retrieval-augmented-generation/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, data_quality, documentation</li> <li>Risk notes: Requires controls for stale data, source governance, and privacy when indexing internal corpora.</li> </ul>"},{"location":"terms/retrieval-augmented-generation/#relationships","title":"Relationships","text":"<ul> <li>Broader: grounded generation</li> <li>Narrower: hybrid RAG, agentic RAG</li> <li>Related: retrieval, chunking, reranking, hallucination</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'retrieval-augmented generation'.</p>"},{"location":"terms/retrieval-augmented-generation/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/retrieval-augmented-generation.yml</code></p>"},{"location":"terms/retrieval/","title":"retrieval","text":""},{"location":"terms/retrieval/#retrieval","title":"retrieval","text":"<p>Aliases: information retrieval, retriever Categories: Retrieval &amp; RAG Roles: Communications &amp; Enablement, Data Science &amp; Research, Product &amp; Program Managers, Security &amp; Trust, Engineering &amp; Platform, Policy &amp; Risk Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/retrieval/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> </ul>"},{"location":"terms/retrieval/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Validate retrieval quality using the evaluation guidance referenced in this entry.</li> <li>Ensure knowledge sources named here appear in your data governance inventory.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/retrieval/#short-definition","title":"Short definition","text":"<p>Process of selecting relevant documents or vectors from a corpus in response to a query.</p>"},{"location":"terms/retrieval/#long-definition","title":"Long definition","text":"<p>Retrieval refers to the algorithms and infrastructure that locate the most relevant pieces of information when a user submits a query. Classical approaches use lexical signals like TF-IDF and BM25, while modern systems incorporate dense vector similarity, hybrid combinations, and reranking models tailored to the domain. Retrieval quality underpins search, question answering, and retrieval-augmented generation workflows: without high recall and precision, downstream models hallucinate or return stale policy guidance. Engineers design retrievers by choosing chunking strategies, embedding models, index types, and freshness policies. They also monitor latency, relevance metrics, and guardrails such as filtering sensitive content. Governance teams treat retrieval pipelines as data processors, verifying access controls, audit logging, and compliance with data minimization requirements. Strong retrieval discipline enables traceability by linking generated outputs back to cited sources.</p>"},{"location":"terms/retrieval/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Retrieval is the step that fetches trusted facts before the AI speaks, keeping answers grounded.</li> <li>Engineer: Implements query encoding, similarity search, and ranking to surface high-relevance passages for downstream consumers.</li> </ul>"},{"location":"terms/retrieval/#examples","title":"Examples","text":"<p>Do - Track recall@k and precision metrics separately to diagnose retrieval bottlenecks.</p> <p>Don't - Index sensitive documents without permissioning or automated redaction.</p>"},{"location":"terms/retrieval/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: data_quality, transparency</li> <li>Risk notes: Weak retrieval controls can expose restricted records or feed outdated content into regulated workflows.</li> </ul>"},{"location":"terms/retrieval/#relationships","title":"Relationships","text":"<ul> <li>Broader: search</li> <li>Narrower: dense retrieval, lexical retrieval</li> <li>Related: retrieval-augmented generation, vector store, reranking</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'retrieval'.</p>"},{"location":"terms/retrieval/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>NIST AI RMF Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/retrieval.yml</code></p>"},{"location":"terms/reward-model/","title":"reward model","text":""},{"location":"terms/reward-model/#reward-model","title":"reward model","text":"<p>Aliases: preference model, policy reward model Categories: LLM Core Roles: Engineering &amp; Platform, Data Science &amp; Research, Policy &amp; Risk Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/reward-model/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> </ul>"},{"location":"terms/reward-model/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/reward-model/#short-definition","title":"Short definition","text":"<p>Model trained on human preferences that scores AI responses for alignment or quality.</p>"},{"location":"terms/reward-model/#long-definition","title":"Long definition","text":"<p>A reward model predicts how well a generated response aligns with human preferences or policy guidelines. Teams train it on comparison data where annotators choose the better of two or more outputs. The reward model then guides reinforcement learning, ranking, or rejection sampling to steer the underlying AI system. Because it encodes policy judgments, the model must be audited for bias, saturation, and overfitting. Engineers monitor reward drift, policy teams review labeling instructions, and data scientists retrain the model as new behaviors emerge. If the reward model is misaligned or gamed, the downstream system can optimize for the wrong incentives.</p>"},{"location":"terms/reward-model/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Treat reward models as governance artifacts\u2014they translate policy into scalable automation.</li> <li>Engineer: Version datasets, metrics, and calibration checks so the reward model stays trustworthy.</li> </ul>"},{"location":"terms/reward-model/#examples","title":"Examples","text":"<p>Do - Validate reward scores against a holdout of human-reviewed examples. - Document known blind spots and include them in launch gates.</p> <p>Don't - Deploy reward models without monitoring for reward hacking. - Train on unverified or biased annotations.</p>"},{"location":"terms/reward-model/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: measurement, risk_management, accountability</li> <li>Risk notes: Reward models encode subjective policy choices; poor oversight leads to misaligned optimization.</li> </ul>"},{"location":"terms/reward-model/#relationships","title":"Relationships","text":"<ul> <li>Broader: reinforcement learning from human feedback</li> <li>Related: alignment, evaluation harness, safety spec</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'reward model'.</p>"},{"location":"terms/reward-model/#citations","title":"Citations","text":"<ul> <li>OpenAI \u2013 InstructGPT Paper</li> <li>DeepMind \u2013 RLHF and Reward Modelling</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/reward-model.yml</code></p>"},{"location":"terms/risk-register/","title":"risk register","text":""},{"location":"terms/risk-register/#risk-register","title":"risk register","text":"<p>Aliases: risk log, risk inventory Categories: Governance &amp; Risk Roles: Policy &amp; Risk, Product &amp; Program Managers, Legal &amp; Compliance, Engineering &amp; Platform Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/risk-register/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/risk-register/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/risk-register/#short-definition","title":"Short definition","text":"<p>Central list of identified AI risks, their owners, mitigations, and review status.</p>"},{"location":"terms/risk-register/#long-definition","title":"Long definition","text":"<p>A risk register tracks every material risk an AI system poses, including potential harms, likelihood, impact, and planned mitigations. It links each item to an accountable owner, current status, and evidence such as evaluation results or assurance case references. Policy teams maintain the register for governance reviews, product owners prioritize fixes, engineering updates mitigation progress, and legal teams map entries to regulatory obligations. The register should be version-controlled and reviewed regularly; letting it go stale leaves leadership blind to emerging exposures.</p>"},{"location":"terms/risk-register/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Use the risk register as a single source of truth when approving launches or investments.</li> <li>Engineer: Update mitigation status promptly so policy partners can judge readiness accurately.</li> </ul>"},{"location":"terms/risk-register/#examples","title":"Examples","text":"<p>Do - Tag each risk with severity, detection method, and next review date. - Link mitigations to evaluation artifacts and assurance case claims.</p> <p>Don't - Hide unresolved risks in private notes or disparate tools. - Close a risk without capturing verification evidence.</p>"},{"location":"terms/risk-register/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: risk_management, accountability, governance</li> <li>Risk notes: Without a maintained register, teams miss systemic issues and fail audits.</li> </ul>"},{"location":"terms/risk-register/#relationships","title":"Relationships","text":"<ul> <li>Broader: model governance</li> <li>Related: assurance case, impact mitigation plan, transparency report</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'risk register'.</p>"},{"location":"terms/risk-register/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF \u2013 Risk Management Playbook</li> <li>Microsoft \u2013 Responsible AI Risk Controls</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/risk-register.yml</code></p>"},{"location":"terms/robust-prompting/","title":"robust prompting","text":""},{"location":"terms/robust-prompting/#robust-prompting","title":"robust prompting","text":"<p>Aliases: defensive prompting, resilient prompting Categories: LLM Core Roles: Engineering &amp; Platform, Product &amp; Program Managers, Security &amp; Trust Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/robust-prompting/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/robust-prompting/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/robust-prompting/#short-definition","title":"Short definition","text":"<p>Prompt design techniques that harden models against injections, ambiguity, and unsafe outputs.</p>"},{"location":"terms/robust-prompting/#long-definition","title":"Long definition","text":"<p>Robust prompting combines structured instructions, guard clauses, and response checks to make language models more resilient to adversarial or ambiguous inputs. Teams use it to constrain topics, refuse unsafe requests, and preserve critical context even when users try to override instructions. Techniques include layered system messages, explicit refusal criteria, verification prompts, and output formatting templates that make policy violations easier to detect. Engineers pair robust prompts with automated tests, while product and security stakeholders review language to ensure safety requirements and user empathy co-exist. Without defensive prompts, even well-governed systems can be steered into disallowed actions by crafted jailbreaks or accidental misuse.</p>"},{"location":"terms/robust-prompting/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Invest in robust prompts to reduce incident volume before adding expensive human moderation.</li> <li>Engineer: Version prompts, test for bypasses, and log refusals so you can tune protections proactively.</li> </ul>"},{"location":"terms/robust-prompting/#examples","title":"Examples","text":"<p>Do - Include explicit refusal language and escalation guidance when requests break policy. - Pair prompts with automated prompt-injection tests in CI.</p> <p>Don't - Rely on a single system message to enforce all safety requirements. - Ship prompt changes without regression testing for jailbreak coverage.</p>"},{"location":"terms/robust-prompting/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: risk_management, safety, monitoring</li> <li>Risk notes: Weak prompts increase exposure to jailbreaks, data leakage, and brand-damaging outputs.</li> </ul>"},{"location":"terms/robust-prompting/#relationships","title":"Relationships","text":"<ul> <li>Broader: prompt engineering</li> <li>Related: prompt injection, guardrail policy, self-critique loop</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'robust prompting'.</p>"},{"location":"terms/robust-prompting/#citations","title":"Citations","text":"<ul> <li>Prompting Guide \u2013 Defensive Prompting</li> <li>arXiv \u2013 InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/robust-prompting.yml</code></p>"},{"location":"terms/roc-auc/","title":"roc auc","text":""},{"location":"terms/roc-auc/#roc-auc","title":"roc auc","text":"<p>Aliases: area under the roc curve, roc area Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/roc-auc/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/roc-auc/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Add this concept to onboarding materials so teammates share a common baseline.</li> <li>Link supporting research or documentation in your internal wiki for deeper study.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/roc-auc/#short-definition","title":"Short definition","text":"<p>Metric summarizing binary classifier performance by measuring area under the ROC curve.</p>"},{"location":"terms/roc-auc/#long-definition","title":"Long definition","text":"<p>ROC AUC (Receiver Operating Characteristic Area Under the Curve) quantifies how well a binary classifier separates positive and negative classes across all possible decision thresholds. The ROC curve plots the true positive rate against the false positive rate; the area under that curve ranges from 0.5 (random) to 1.0 (perfect). Teams use ROC AUC when class distributions are imbalanced or when threshold selection will happen later in the product lifecycle. Engineers analyze the curve\u2019s shape to understand trade-offs between sensitivity and specificity, while product managers compare ROC AUC alongside business-focused metrics before launches. Governance and legal stakeholders review ROC AUC for fairness and regulatory reporting to ensure classifiers meet minimum performance standards before deployment.</p>"},{"location":"terms/roc-auc/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: ROC AUC shows how reliably a classifier can distinguish good from bad outcomes before you pick a final threshold.</li> <li>Engineer: Integrate true/false positive rates over thresholds; inspect curve segments to diagnose operating points and class imbalance.</li> </ul>"},{"location":"terms/roc-auc/#examples","title":"Examples","text":"<p>Do - Report ROC AUC alongside precision, recall, and confusion matrices for a holistic view. - Evaluate ROC AUC per subgroup to detect disparate impact before rollout.</p> <p>Don't - Rely solely on ROC AUC when decision thresholds are fixed and business costs are asymmetric. - Compare ROC AUC from different datasets without standardizing splits.</p>"},{"location":"terms/roc-auc/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, transparency</li> <li>Risk notes: Ignoring ROC AUC trends can hide degradation that leads to unfair or unsafe decisions in production.</li> </ul>"},{"location":"terms/roc-auc/#relationships","title":"Relationships","text":"<ul> <li>Broader: evaluation</li> <li>Related: cross-validation, bias-variance tradeoff, algorithmic bias</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'roc auc'.</p>"},{"location":"terms/roc-auc/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/roc-auc.yml</code></p>"},{"location":"terms/safety-classifier/","title":"safety classifier","text":""},{"location":"terms/safety-classifier/#safety-classifier","title":"safety classifier","text":"<p>Aliases: safety filter, policy classifier Categories: Governance &amp; Risk Roles: Security &amp; Trust, Policy &amp; Risk, Product &amp; Program Managers, Engineering &amp; Platform Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/safety-classifier/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/safety-classifier/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/safety-classifier/#short-definition","title":"Short definition","text":"<p>Model that detects policy-violating or risky content before or after generation.</p>"},{"location":"terms/safety-classifier/#long-definition","title":"Long definition","text":"<p>A safety classifier screens prompts and outputs to catch disallowed topics such as self-harm, extremism, or personal data. Classifiers can run pre-generation (blocking unsafe inputs), post-generation (filtering responses), or both. They complement prompt-based guardrails, providing a measurable signal that policies are enforced. Security and policy teams curate labeled datasets, engineering integrates the classifier into pipelines, and product owners tune messaging when content is blocked. Mature teams monitor precision/recall, recalibrate thresholds, and pair classifiers with human review for high-severity categories. Classifiers require ongoing evaluation to avoid overblocking legitimate use or missing novel abuse patterns.</p>"},{"location":"terms/safety-classifier/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Treat safety classifiers as a control with metrics, owners, and effectiveness reviews.</li> <li>Engineer: Log classifier scores, decisions, and overrides for auditing and incident analysis.</li> </ul>"},{"location":"terms/safety-classifier/#examples","title":"Examples","text":"<p>Do - Version classifier thresholds and align them with published policy requirements. - Run adversarial tests to catch bypasses introduced by new jailbreaks.</p> <p>Don't - Rely solely on classifiers without human review for high-risk categories. - Deploy classifiers trained on outdated policy definitions.</p>"},{"location":"terms/safety-classifier/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: monitoring, risk_management, security</li> <li>Risk notes: Poorly tuned classifiers erode trust\u2014either by letting harmful content through or by overblocking.</li> </ul>"},{"location":"terms/safety-classifier/#relationships","title":"Relationships","text":"<ul> <li>Broader: guardrails</li> <li>Related: jailbreak prompt, prompt injection, robust prompting</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'safety classifier'.</p>"},{"location":"terms/safety-classifier/#citations","title":"Citations","text":"<ul> <li>OpenAI \u2013 Moderation Tools</li> <li>Google Cloud \u2013 Content Safety Tools</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/safety-classifier.yml</code></p>"},{"location":"terms/safety-evaluation/","title":"safety evaluation","text":""},{"location":"terms/safety-evaluation/#safety-evaluation","title":"safety evaluation","text":"<p>Aliases: safety testing, safety assessment Categories: Governance &amp; Risk, Operations &amp; Monitoring Roles: Engineering &amp; Platform, Policy &amp; Risk, Product &amp; Program Managers, Communications &amp; Enablement Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/safety-evaluation/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> </ul>"},{"location":"terms/safety-evaluation/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Instrument dashboards or alerts that reflect the metrics highlighted in this definition.</li> <li>Update incident response or on-call runbooks with the glossary's do/don't scenarios.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/safety-evaluation/#short-definition","title":"Short definition","text":"<p>Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.</p>"},{"location":"terms/safety-evaluation/#long-definition","title":"Long definition","text":"<p>Safety evaluations probe AI systems for dangerous or disallowed behavior, complementing performance metrics with targeted abuse, bias, and compliance tests. Teams blend automated classifiers, curated prompt suites, and expert reviews to measure toxicity, misinformation, self-harm encouragement, and other high-risk outcomes. Results feed into guardrail tuning, incident response plans, and launch gate decisions. Engineering and policy teams collaborate on coverage, ensuring critical user journeys and demographic perspectives are represented. Communications and legal stakeholders review findings to shape disclosures and mitigation commitments. Safety evaluations are continuous: regressions can surface after prompt changes, model updates, or content shifts, so organizations schedule recurring runs and capture evidence for audits. Failing to operationalize safety evaluations at scale exposes products to public incidents, regulatory scrutiny, and erosion of user trust.</p>"},{"location":"terms/safety-evaluation/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Safety evaluation is the checkpoint that proves the AI won\u2019t violate our policies or harm users.</li> <li>Engineer: Execute targeted red-team suites, automated toxicity checks, and manual reviews; document thresholds, residual risk, and remediation plans.</li> </ul>"},{"location":"terms/safety-evaluation/#examples","title":"Examples","text":"<p>Do - Store evaluation artifacts alongside release notes for traceability. - Include marginalized community perspectives in the review panel and prompt set.</p> <p>Don't - Assume safety coverage automatically transfers when prompts or models change. - Treat a passing score as permanent\u2014schedule re-tests after meaningful updates.</p>"},{"location":"terms/safety-evaluation/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: risk_management, accountability</li> <li>Risk notes: Skipping or deferring safety evaluations invites policy breaches, public incidents, and enforcement actions.</li> </ul>"},{"location":"terms/safety-evaluation/#relationships","title":"Relationships","text":"<ul> <li>Broader: evaluation</li> <li>Related: red teaming, guardrails, incident response</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'safety evaluation'.</p>"},{"location":"terms/safety-evaluation/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>OpenAI \u2013 Safety Evaluations Overview</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/safety-evaluation.yml</code></p>"},{"location":"terms/safety-review-board/","title":"safety review board","text":""},{"location":"terms/safety-review-board/#safety-review-board","title":"safety review board","text":"<p>Aliases: ai safety council, responsible ai board Categories: Governance &amp; Risk Roles: Policy &amp; Risk, Legal &amp; Compliance, Security &amp; Trust, Product &amp; Program Managers Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/safety-review-board/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/safety-review-board/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/safety-review-board/#short-definition","title":"Short definition","text":"<p>Cross-functional committee that approves high-risk AI launches and monitors mitigations.</p>"},{"location":"terms/safety-review-board/#long-definition","title":"Long definition","text":"<p>A safety review board brings together policy, legal, security, product, and engineering leaders to evaluate AI systems before deployment. The board reviews risk registers, assurance cases, evaluation results, and mitigation plans, then issues approvals or action items. It also monitors incidents, tracks mitigation follow-through, and ensures governance policies evolve with capabilities. Effective boards maintain charters, meeting cadences, and decision logs that feed back into assurance cases and executive reporting. Without a formal board, decisions may be made piecemeal, leaving organizations exposed to fragmented oversight and inconsistent accountability.</p>"},{"location":"terms/safety-review-board/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Empower a safety board with decision rights so responsibility for high-risk launches is explicit.</li> <li>Engineer: Prepare evidence packages that address the board\u2019s checklist and document follow-ups.</li> </ul>"},{"location":"terms/safety-review-board/#examples","title":"Examples","text":"<p>Do - Publish a charter defining scope, membership, and escalation paths. - Record decisions and rationale to inform future reviews.</p> <p>Don't - Treat approvals as rubber stamps without requiring evidence of mitigations. - Leave the board uninformed about production incidents.</p>"},{"location":"terms/safety-review-board/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: governance, accountability, risk_management</li> <li>Risk notes: Absent governance bodies, high-severity decisions can bypass oversight and create liability.</li> </ul>"},{"location":"terms/safety-review-board/#relationships","title":"Relationships","text":"<ul> <li>Broader: model governance</li> <li>Related: assurance case, risk register, impact mitigation plan</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'safety review board'.</p>"},{"location":"terms/safety-review-board/#citations","title":"Citations","text":"<ul> <li>UK CDEI \u2013 Responsible AI Assurance Toolkit</li> <li>NIST AI RMF \u2013 Governance Function</li> <li>Partnership on AI \u2013 Safety-Critical AI Assurance</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/safety-review-board.yml</code></p>"},{"location":"terms/safety-spec/","title":"safety spec","text":""},{"location":"terms/safety-spec/#safety-spec","title":"safety spec","text":"<p>Aliases: safety specification, model safety policy Categories: Governance &amp; Risk, LLM Core Roles: Product &amp; Program Managers, Engineering &amp; Platform, Policy &amp; Risk, Security &amp; Trust, Communications &amp; Enablement Part of speech: <code>concept</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/safety-spec/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> </ul>"},{"location":"terms/safety-spec/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/safety-spec/#short-definition","title":"Short definition","text":"<p>Document that codifies allowed, disallowed, and escalated behaviours for an AI system so teams can enforce safety and policy expectations.</p>"},{"location":"terms/safety-spec/#long-definition","title":"Long definition","text":"<p>A safety spec translates high-level AI principles into actionable guidance for a specific system. It enumerates desired behaviours, prohibited outputs, escalation paths, and fallback actions across scenarios such as self-harm, misinformation, sensitive topics, or abuse. Product and policy teams draft the intent; engineering and safety specialists convert it into prompts, classifiers, guardrails, and monitoring hooks; communications teams prepare messaging for when guardrails trigger. A good safety spec links each requirement to reference examples, evaluation suites, and human review steps, so updates can be tested before deployment. The document is version-controlled alongside prompts and model configs, and is revisited after incidents, red teaming, or regulatory changes. Without a safety spec, expectations stay tribal, handoffs break down, and teams cannot prove alignment with industry or statutory obligations.</p>"},{"location":"terms/safety-spec/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Shows how we operationalise safety promises with concrete rules, owners, and escalation paths.</li> <li>Engineer: Clarifies what the model must block or escalate and how to wire guardrails, logging, and overrides.</li> </ul>"},{"location":"terms/safety-spec/#examples","title":"Examples","text":"<p>Do - Version-control the safety spec and require sign-off from policy, legal, and engineering before shipping changes. - Attach evaluation prompts and datasets that must pass before releasing a new prompt or model checkpoint.</p> <p>Don't - Copy a generic policy without adding concrete prompts, thresholds, or handoff procedures. - Let the safety spec drift after incidents\u2014update the rules and tests while lessons are fresh.</p>"},{"location":"terms/safety-spec/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: risk_management, accountability, transparency</li> <li>Risk notes: Outdated or missing safety specs leave teams unable to enforce guardrails or defend decisions during audits.</li> </ul>"},{"location":"terms/safety-spec/#relationships","title":"Relationships","text":"<ul> <li>Broader: alignment, ai assurance</li> <li>Related: guardrails, red teaming, prompt engineering, ai incident response</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'safety spec'.</p>"},{"location":"terms/safety-spec/#citations","title":"Citations","text":"<ul> <li>Responsible AI Safety Framework</li> <li>Google AI \u2013 AI Principles</li> <li>Constitutional AI (Anthropic)</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/safety-spec.yml</code></p>"},{"location":"terms/self-consistency-decoding/","title":"self-consistency decoding","text":""},{"location":"terms/self-consistency-decoding/#self-consistency-decoding","title":"self-consistency decoding","text":"<p>Aliases: self-consistency, majority-vote reasoning Categories: LLM Core Roles: Engineering &amp; Platform, Data Science &amp; Research Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/self-consistency-decoding/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> </ul>"},{"location":"terms/self-consistency-decoding/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/self-consistency-decoding/#short-definition","title":"Short definition","text":"<p>Decoding strategy that samples multiple reasoning paths and aggregates the most consistent answer.</p>"},{"location":"terms/self-consistency-decoding/#long-definition","title":"Long definition","text":"<p>Self-consistency decoding runs a model several times with chain-of-thought prompts, then selects the answer that appears most frequently across the sampled reasoning paths. The approach boosts accuracy on reasoning tasks by reducing the impact of any single hallucinated chain. Engineers trade off latency and cost for higher reliability, while data scientists evaluate how many samples are needed to meaningfully improve accuracy. The technique pairs well with automated checkers that verify the final answer. Without careful tuning, self-consistency can reinforce common but incorrect answers or leak sensitive reasoning traces.</p>"},{"location":"terms/self-consistency-decoding/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Use self-consistency when critical decisions require higher reasoning confidence.</li> <li>Engineer: Balance sampling counts with latency budgets and audit reasoning traces for policy compliance.</li> </ul>"},{"location":"terms/self-consistency-decoding/#examples","title":"Examples","text":"<p>Do - Combine self-consistency with validators that check math or policy compliance. - Log sampled chains for debugging and evaluation.</p> <p>Don't - Assume majority vote guarantees correctness without external checks. - Leak sensitive instruction text in stored reasoning chains.</p>"},{"location":"terms/self-consistency-decoding/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: monitoring, risk_management</li> <li>Risk notes: Sampling multiple chains increases cost and potential exposure of sensitive intermediate reasoning.</li> </ul>"},{"location":"terms/self-consistency-decoding/#relationships","title":"Relationships","text":"<ul> <li>Broader: decoding</li> <li>Related: chain-of-thought prompting, robust prompting, evaluation harness</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'self-consistency decoding'.</p>"},{"location":"terms/self-consistency-decoding/#citations","title":"Citations","text":"<ul> <li>Google \u2013 Self-Consistency Improves Chain of Thought</li> <li>OpenAI \u2013 Better Language Models and Their Implications</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/self-consistency-decoding.yml</code></p>"},{"location":"terms/self-critique-loop/","title":"self-critique loop","text":""},{"location":"terms/self-critique-loop/#self-critique-loop","title":"self-critique loop","text":"<p>Aliases: self-reflection loop, critique-and-revise Categories: Agents &amp; Tooling Roles: Engineering &amp; Platform, Product &amp; Program Managers, Policy &amp; Risk Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/self-critique-loop/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> </ul>"},{"location":"terms/self-critique-loop/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Audit exposed tools against the safeguards described and document approval paths.</li> <li>Test hand-offs with human reviewers to confirm the safety expectations captured here are met.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/self-critique-loop/#short-definition","title":"Short definition","text":"<p>Pattern where a model reviews its own outputs, critiques them, and produces revisions before responding.</p>"},{"location":"terms/self-critique-loop/#long-definition","title":"Long definition","text":"<p>A self-critique loop introduces an internal review step into an AI workflow. After generating an initial answer, the model (or a paired model) evaluates the response against instructions, safety policies, or quality rubrics, then revises the output. Engineers use the loop to catch hallucinations or policy violations automatically, while product teams tune the number of critique iterations to balance latency and reliability. Governance partners supply critique prompts that focus on harm categories and compliance language. Without clear stop conditions, self-critique can increase cost or drift toward repetitive edits, so loops should cap iterations and escalate to humans when uncertainty remains high.</p>"},{"location":"terms/self-critique-loop/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Invest in self-critique loops to raise reliability without assigning every response to manual review.</li> <li>Engineer: Instrument critique prompts, scoring, and escalation thresholds to keep loops fast and auditable.</li> </ul>"},{"location":"terms/self-critique-loop/#examples","title":"Examples","text":"<p>Do - Log critique reasons and revision diffs so policy teams can audit how safety issues were resolved. - Limit the loop to two or three iterations before escalating to a human reviewer.</p> <p>Don't - Allow the loop to run indefinitely chasing marginal quality gains. - Rely solely on self-critique for high-severity categories without human oversight.</p>"},{"location":"terms/self-critique-loop/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: monitoring, risk_management, transparency</li> <li>Risk notes: Unbounded self-critique can mask uncertain decisions and increase compute cost without clear accountability.</li> </ul>"},{"location":"terms/self-critique-loop/#relationships","title":"Relationships","text":"<ul> <li>Broader: constitutional ai, guardrails</li> <li>Related: robust prompting, safety spec, ai incident response</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'self-critique loop'.</p>"},{"location":"terms/self-critique-loop/#citations","title":"Citations","text":"<ul> <li>Anthropic \u2013 Constitutional AI</li> <li>Self-Refine \u2013 Iterative Refinement with LLMs</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/self-critique-loop.yml</code></p>"},{"location":"terms/shadow-deployment/","title":"shadow deployment","text":""},{"location":"terms/shadow-deployment/#shadow-deployment","title":"shadow deployment","text":"<p>Aliases: shadow mode, silent launch Categories: Operations &amp; Monitoring Roles: Engineering &amp; Platform, Product &amp; Program Managers, Policy &amp; Risk, Security &amp; Trust Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/shadow-deployment/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/shadow-deployment/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Instrument dashboards or alerts that reflect the metrics highlighted in this definition.</li> <li>Update incident response or on-call runbooks with the glossary's do/don't scenarios.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/shadow-deployment/#short-definition","title":"Short definition","text":"<p>Deploying an AI system alongside the existing workflow without user impact to collect telemetry.</p>"},{"location":"terms/shadow-deployment/#long-definition","title":"Long definition","text":"<p>In a shadow deployment, an AI model runs in production but its outputs are not shown to end users or do not control the primary workflow. Teams compare shadow results against human decisions, measure safety metrics, and stress-test guardrails before full launch. Engineering sets up parallel pipelines, product owners define success criteria, and policy leaders review logs for compliance or bias. Shadow mode provides real-world data without exposing customers to unverified behavior, but only if teams actively review the telemetry and follow up on issues.</p>"},{"location":"terms/shadow-deployment/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Use shadow deployments to de-risk launches and inform go/no-go decisions with real data.</li> <li>Engineer: Instrument drift, safety, and latency metrics during shadow mode and share findings broadly.</li> </ul>"},{"location":"terms/shadow-deployment/#examples","title":"Examples","text":"<p>Do - Run shadow mode until evaluation thresholds and mitigation backlog items are cleared. - Log disagreements between the AI and human decisions for analysis.</p> <p>Don't - Let shadow deployments run without active monitoring or ownership. - Ignore privacy commitments when storing shadow outputs.</p>"},{"location":"terms/shadow-deployment/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: monitoring, risk_management, governance</li> <li>Risk notes: Shadow mode without review creates false confidence\u2014issues surface only after public launch.</li> </ul>"},{"location":"terms/shadow-deployment/#relationships","title":"Relationships","text":"<ul> <li>Broader: ml ops</li> <li>Related: evaluation harness, impact mitigation plan, risk register</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'shadow deployment'.</p>"},{"location":"terms/shadow-deployment/#citations","title":"Citations","text":"<ul> <li>Google SRE \u2013 Canary Releases and Shadowing</li> <li>AWS Well-Architected \u2013 Shadow Testing</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/shadow-deployment.yml</code></p>"},{"location":"terms/synthetic-data-evaluation/","title":"synthetic data evaluation","text":""},{"location":"terms/synthetic-data-evaluation/#synthetic-data-evaluation","title":"synthetic data evaluation","text":"<p>Aliases: synthetic data quality assessment, synthetic validation Categories: Operations &amp; Monitoring, Governance &amp; Risk Roles: Data Science &amp; Research, Engineering &amp; Platform, Policy &amp; Risk, Product &amp; Program Managers Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/synthetic-data-evaluation/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/synthetic-data-evaluation/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Instrument dashboards or alerts that reflect the metrics highlighted in this definition.</li> <li>Update incident response or on-call runbooks with the glossary's do/don't scenarios.</li> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/synthetic-data-evaluation/#short-definition","title":"Short definition","text":"<p>Process for measuring fidelity, utility, privacy, and bias of synthetic datasets before use.</p>"},{"location":"terms/synthetic-data-evaluation/#long-definition","title":"Long definition","text":"<p>Synthetic data evaluation verifies that generated datasets mirror the statistical properties of real data without leaking sensitive information. Teams measure fidelity (distribution similarity), utility (model performance when trained on synthetic data), privacy leakage (distance or matching tests), and bias (subgroup balance). Evaluations often combine automated metrics\u2014such as Wasserstein distance, nearest-neighbor tests, TSTR/TSNT\u2014and human or domain-expert reviews. Product and policy teams review results to decide whether synthetic data is acceptable for training, testing, or sharing. Documentation should capture evaluation methods, thresholds, and mitigation plans when issues arise.</p>"},{"location":"terms/synthetic-data-evaluation/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Synthetic data evaluation proves the generated data is both useful and safe before we rely on it.</li> <li>Engineer: Compute fidelity, utility, privacy, and bias metrics; compare against acceptance thresholds and log findings with dataset versions.</li> </ul>"},{"location":"terms/synthetic-data-evaluation/#examples","title":"Examples","text":"<p>Do - Use real-world benchmarks (TSTR) to confirm models trained on synthetic data generalize. - Validate privacy leakage with membership inference or nearest-neighbor tests.</p> <p>Don't - Assume synthetic data is safe without quantitative evaluation. - Ignore subgroup representation when assessing utility.</p>"},{"location":"terms/synthetic-data-evaluation/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: data_quality, privacy</li> <li>Risk notes: Poor evaluation can result in biased or leaky synthetic datasets reaching production or partners.</li> </ul>"},{"location":"terms/synthetic-data-evaluation/#relationships","title":"Relationships","text":"<ul> <li>Broader: synthetic data</li> <li>Related: differential privacy, fairness metrics, evaluation</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'synthetic data evaluation'.</p>"},{"location":"terms/synthetic-data-evaluation/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/synthetic-data-evaluation.yml</code></p>"},{"location":"terms/synthetic-data/","title":"synthetic data","text":""},{"location":"terms/synthetic-data/#synthetic-data","title":"synthetic data","text":"<p>Aliases: generated data, simulated data Categories: Governance &amp; Risk, Operations &amp; Monitoring Roles: Data Science &amp; Research, Engineering &amp; Platform, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/synthetic-data/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/synthetic-data/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Instrument dashboards or alerts that reflect the metrics highlighted in this definition.</li> <li>Update incident response or on-call runbooks with the glossary's do/don't scenarios.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/synthetic-data/#short-definition","title":"Short definition","text":"<p>Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.</p>"},{"location":"terms/synthetic-data/#long-definition","title":"Long definition","text":"<p>Synthetic data recreates statistical properties of real datasets without exposing exact records. Teams generate it with generative models, simulations, or rule-based scripts to augment scarce examples, balance demographic representation, or share data across boundaries. When used responsibly, synthetic data accelerates experimentation and protects privacy; when mismanaged, it can amplify biases, leak sensitive patterns, or give a false sense of security. Product and data science teams validate that synthetic datasets preserve signal relevant to their tasks, while policy and security partners evaluate whether the generation process meets governance requirements. Documentation should cover source data lineage, generation methods, and evaluation metrics such as fidelity, utility, and privacy leakage. Synthetic data is most effective when paired with differential privacy, guardrails, and robust monitoring that detects drift between simulated and real-world behavior.</p>"},{"location":"terms/synthetic-data/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Synthetic data lets teams test and train quickly without always touching sensitive customer records.</li> <li>Engineer: Generate data via simulations or generative models; validate fidelity, diversity, and privacy leakage before using it in pipelines.</li> </ul>"},{"location":"terms/synthetic-data/#examples","title":"Examples","text":"<p>Do - Track utility metrics comparing model performance on synthetic versus real validation sets. - Label synthetic datasets clearly so downstream teams understand provenance.</p> <p>Don't - Assume synthetic data automatically removes bias\u2014measure subgroup impacts explicitly. - Share synthetic datasets externally without privacy and legal review.</p>"},{"location":"terms/synthetic-data/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: privacy, data_quality</li> <li>Risk notes: Poorly generated data can encode discriminatory patterns or expose sensitive distributions; pair releases with privacy and bias assessments.</li> </ul>"},{"location":"terms/synthetic-data/#relationships","title":"Relationships","text":"<ul> <li>Broader: data preprocessing</li> <li>Related: differential privacy, model drift, evaluation</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'synthetic data'.</p>"},{"location":"terms/synthetic-data/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Google ML Glossary</li> <li>Google Cloud \u2013 What Is Synthetic Data?</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/synthetic-data.yml</code></p>"},{"location":"terms/system-prompt/","title":"system prompt","text":""},{"location":"terms/system-prompt/#system-prompt","title":"system prompt","text":"<p>Aliases: system instruction, base prompt Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p> <p>Put it into practice</p> <p>Review the Prompt Engineering Playbook before shipping updates.</p>"},{"location":"terms/system-prompt/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/system-prompt/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/system-prompt/#short-definition","title":"Short definition","text":"<p>Foundational instruction that sets role, tone, and guardrails for an AI assistant before user input.</p>"},{"location":"terms/system-prompt/#long-definition","title":"Long definition","text":"<p>A system prompt is the preamble sent to a conversational model before user messages to establish behavior, policies, and capabilities. It can describe persona, writing style, safety instructions, tool usage rules, or escalation paths. Because the system prompt is prepended to every conversation turn, it shapes how the model interprets later inputs and resolves conflicts between user requests and organizational policy. Product teams iterate on system prompts to balance friendliness with compliance, while engineers version and test them like code, running regression suites to detect unexpectedly permissive outputs. Governance reviewers treat system prompts as formal policy artifacts: they require approval, change tracking, and alignment with risk controls such as prohibited content lists. Poorly maintained system prompts can drift, accumulate contradictory clauses, or leak internal policies if exposed, so teams pair them with automated linting and secret scanning.</p>"},{"location":"terms/system-prompt/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Think of the system prompt as the playbook that keeps the assistant on-brand and on-policy before it ever talks to a user.</li> <li>Engineer: Immutable prefix in the prompt stack that defines instructions and tool contracts; version-controlled for audits and evals.</li> </ul>"},{"location":"terms/system-prompt/#examples","title":"Examples","text":"<p>Do - Store system prompts in source control and run evaluations whenever they change.</p> <p>Don't - Embed sensitive credentials or private policies in the system prompt without access controls.</p>"},{"location":"terms/system-prompt/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, accountability</li> <li>Risk notes: Unreviewed system prompts can encode unsafe behaviors or leak confidential policy guidance.</li> </ul>"},{"location":"terms/system-prompt/#relationships","title":"Relationships","text":"<ul> <li>Broader: prompt engineering</li> <li>Narrower: safety prompt</li> <li>Related: guardrails, temperature, tool use</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'system prompt'.</p>"},{"location":"terms/system-prompt/#citations","title":"Citations","text":"<ul> <li>Anthropic \u2013 System Prompts</li> <li>Microsoft \u2013 System Message Guidance</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/system-prompt.yml</code></p>"},{"location":"terms/target-variable/","title":"target variable","text":""},{"location":"terms/target-variable/#target-variable","title":"target variable","text":"<p>Aliases: label, response variable Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/target-variable/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/target-variable/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Add this concept to onboarding materials so teammates share a common baseline.</li> <li>Link supporting research or documentation in your internal wiki for deeper study.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/target-variable/#short-definition","title":"Short definition","text":"<p>Outcome the model is trained to predict, providing the signal for calculating loss.</p>"},{"location":"terms/target-variable/#long-definition","title":"Long definition","text":"<p>The target variable is the reference output used to evaluate model predictions during training and testing. In supervised learning it represents ground truth labels, such as fraud or not fraud, estimated price, or safety category. Accurate, consistent targets allow teams to compute loss functions, track metrics like precision and recall, and align models with business objectives. Product stakeholders define which real-world outcome matters most, while engineers and data scientists maintain labeling pipelines, quality checks, and access controls. Poorly governed targets undermine evaluation integrity and can encode bias, leading to misinformed decisions at launch.</p>"},{"location":"terms/target-variable/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: The target variable is the result we ask the model to get right so business decisions stay grounded.</li> <li>Engineer: Keep labeling guidelines versioned, audit quality, and map targets to downstream KPIs before training.</li> </ul>"},{"location":"terms/target-variable/#examples","title":"Examples","text":"<p>Do - Review labeling consistency across vendors before starting a new training run. - Document how targets map to customer outcomes in the model card.</p> <p>Don't - Mix labels from incompatible taxonomies without reconciliation. - Expose sensitive targets to broad teams without privacy controls.</p>"},{"location":"terms/target-variable/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, fairness</li> <li>Risk notes: Low-quality target variables hide discrimination, inflate metrics, and break compliance attestations.</li> </ul>"},{"location":"terms/target-variable/#relationships","title":"Relationships","text":"<ul> <li>Related: loss function, precision, recall, content moderation</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'target variable'.</p>"},{"location":"terms/target-variable/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>Partnership on AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/target-variable.yml</code></p>"},{"location":"terms/temperature/","title":"temperature","text":""},{"location":"terms/temperature/#temperature","title":"temperature","text":"<p>Aliases: sampling temperature, softmax temperature Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p> <p>Put it into practice</p> <p>Experiment with settings using the Prompt Engineering Playbook.</p>"},{"location":"terms/temperature/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/temperature/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/temperature/#short-definition","title":"Short definition","text":"<p>Decoding parameter that controls how random or deterministic a model\u2019s outputs are.</p>"},{"location":"terms/temperature/#long-definition","title":"Long definition","text":"<p>Temperature adjusts the softness of the probability distribution used when sampling tokens during decoding. A low temperature sharpens the distribution so the model consistently chooses the highest-probability token, producing deterministic, conservative responses. Higher temperatures flatten the distribution, increasing the likelihood of picking lower-probability tokens that introduce creativity but also raise the risk of incoherence or policy violations. Product teams tune temperature per use case\u2014customer support bots typically prefer lower settings, while brainstorming assistants tolerate higher ones. Engineers may pair temperature with top-k or nucleus sampling to constrain randomness while preserving diversity. Governance reviewers monitor temperature configurations because overly permissive settings can bypass guardrails validated at default values, leading to unpredictable behavior in production. Documenting temperature choices and evaluating them against safety scenarios is therefore part of responsible deployment.</p>"},{"location":"terms/temperature/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Temperature is the creativity dial\u2014turn it down for consistency, up for variety.</li> <li>Engineer: Softmax scaling factor applied prior to sampling; inversely proportional to distribution sharpness and deterministic behavior.</li> </ul>"},{"location":"terms/temperature/#examples","title":"Examples","text":"<p>Do - Run evaluation suites at multiple temperature settings to quantify factual accuracy trade-offs.</p> <p>Don't - Ship higher temperatures to production without revisiting safety and compliance test results.</p>"},{"location":"terms/temperature/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: robustness, transparency</li> <li>Risk notes: High temperatures can invalidate prior safety assessments and increase hallucination incidents.</li> </ul>"},{"location":"terms/temperature/#relationships","title":"Relationships","text":"<ul> <li>Broader: decoding</li> <li>Narrower: temperature annealing</li> <li>Related: top-k sampling, top-p sampling, hallucination</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'temperature'.</p>"},{"location":"terms/temperature/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/temperature.yml</code></p>"},{"location":"terms/test-set/","title":"test set","text":""},{"location":"terms/test-set/#test-set","title":"test set","text":"<p>Aliases: test data, holdout set Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/test-set/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/test-set/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Add this concept to onboarding materials so teammates share a common baseline.</li> <li>Link supporting research or documentation in your internal wiki for deeper study.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/test-set/#short-definition","title":"Short definition","text":"<p>Final evaluation split reserved for measuring real-world performance after all model tuning is finished.</p>"},{"location":"terms/test-set/#long-definition","title":"Long definition","text":"<p>The test set acts as the last line of defense before deployment. Once training and validation are complete, teams use this untouched data slice to estimate how a model will behave in production. Because it reflects realistic scenarios and has never influenced tuning decisions, the test set provides credible evidence for governance reviews, model cards, and launch approvals. Product owners study test outcomes to understand business impact, while engineers compare historical test set runs to spot regressions or drift. Strict separation from training and validation data, coupled with version control and documented refresh policies, preserves trust in test results.</p>"},{"location":"terms/test-set/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: The test set is our final validation that the model keeps its promises before customers see it.</li> <li>Engineer: Protect the test set from reuse, track baseline metrics over time, and rotate it only with formal approvals.</li> </ul>"},{"location":"terms/test-set/#examples","title":"Examples","text":"<p>Do - Store multiple generations of test sets so you can benchmark new models against historical baselines. - Capture subgroup metrics and qualitative notes during every test run for audit trails.</p> <p>Don't - Peek at the test set to choose hyperparameters or feature engineering steps. - Reuse the same test set indefinitely without monitoring for stale scenarios.</p>"},{"location":"terms/test-set/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, accountability</li> <li>Risk notes: Compromised test sets hide regressions and undercut the evidence required for responsible AI approvals.</li> </ul>"},{"location":"terms/test-set/#relationships","title":"Relationships","text":"<ul> <li>Related: validation set, generalization, evaluation, model drift</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'test set'.</p>"},{"location":"terms/test-set/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>NIST AI Risk Management Framework</li> <li>DeepLearning.AI Resources</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/test-set.yml</code></p>"},{"location":"terms/token/","title":"token","text":""},{"location":"terms/token/#token","title":"token","text":"<p>Aliases: subword token, tokenized unit Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/token/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/token/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/token/#short-definition","title":"Short definition","text":"<p>Smallest unit of text a model processes after tokenization, such as a word fragment or character.</p>"},{"location":"terms/token/#long-definition","title":"Long definition","text":"<p>In language models a token is the discrete unit produced by a tokenizer before feeding data into the network. Depending on the tokenizer, a token might represent a whole word, a meaningful subword chunk, or even individual characters and punctuation. Models operate on token sequences rather than raw text, which makes token boundaries central to context window sizing, cost estimation, and latency planning. Tokens also drive how prompts and completions are billed in commercial APIs, where limits are expressed in tokens instead of characters. Engineers track token counts to avoid truncating prompts or spilling past context limits, while product teams translate token budgets into supported use cases and pricing. Understanding tokenization quirks helps explain why uncommon spellings, multilingual inputs, or code snippets can explode in length compared with natural language, affecting both accuracy and economics. Governance stakeholders rely on the deterministic nature of tokenization when auditing prompts and ensuring reproducibility across deployments.</p>"},{"location":"terms/token/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Think of tokens as the LEGO bricks that determine how long prompts can be and what they cost.</li> <li>Engineer: Tokenizer output units that set sequence length, influence embedding lookup, and bound context windows for inference/training.</li> </ul>"},{"location":"terms/token/#examples","title":"Examples","text":"<p>Do - Estimate prompt costs by counting tokens instead of characters before rolling out a pricing plan.</p> <p>Don't - Assume character length equals token length when enforcing guardrails or cost controls.</p>"},{"location":"terms/token/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, validity</li> <li>Risk notes: Incorrect token accounting can break context constraints, leading to truncated outputs and compliance failures.</li> </ul>"},{"location":"terms/token/#relationships","title":"Relationships","text":"<ul> <li>Broader: tokenization</li> <li>Related: context window, embedding, prompt engineering</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'token'.</p>"},{"location":"terms/token/#citations","title":"Citations","text":"<ul> <li>Wikipedia \u2013 Tokenization</li> <li>OpenAI API Reference</li> <li>Hugging Face \u2013 Tokenizers</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/token.yml</code></p>"},{"location":"terms/tool-use/","title":"tool use","text":""},{"location":"terms/tool-use/#tool-use","title":"tool use","text":"<p>Aliases: function calling, model tool invocation Categories: Agents &amp; Tooling Roles: Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/tool-use/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/tool-use/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Audit exposed tools against the safeguards described and document approval paths.</li> <li>Test hand-offs with human reviewers to confirm the safety expectations captured here are met.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/tool-use/#short-definition","title":"Short definition","text":"<p>Pattern where a model selects external tools or functions to handle parts of a task.</p>"},{"location":"terms/tool-use/#long-definition","title":"Long definition","text":"<p>Tool use occurs when a language model defers specific subtasks to external functions, APIs, or agents that it selects based on the conversation. Platforms expose structured schemas describing available tools\u2014such as search, retrieval, math, or workflow actions\u2014and the model predicts when and how to call them. This architecture extends model capabilities beyond text generation, supporting grounded answers, transactions, and integrations with enterprise systems. Engineers design tool contracts, manage authentication, and implement result formatting so outputs slot back into the conversation. Product owners document which tools are enabled per experience and define escalation paths when tools fail. Governance teams treat tool catalogs as controlled assets: they require access reviews, logging, and guardrails to prevent unauthorized actions or data leakage. When implemented responsibly, tool use increases accuracy and reduces hallucinations, but it also introduces new attack surfaces and compliance obligations.</p>"},{"location":"terms/tool-use/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Tool use lets the assistant call trusted services\u2014think search or ticketing\u2014to deliver accurate, actionable answers.</li> <li>Engineer: Expose structured tool specs, let the model emit JSON arguments, execute the function, and feed results back into the dialogue loop.</li> </ul>"},{"location":"terms/tool-use/#examples","title":"Examples","text":"<p>Do - Log every tool invocation with inputs and outputs for auditing and debugging.</p> <p>Don't - Expose production tools without rate limits or abuse monitoring.</p>"},{"location":"terms/tool-use/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, transparency</li> <li>Risk notes: Poorly governed tool catalogs can trigger unauthorized actions or leak sensitive data through function interfaces.</li> </ul>"},{"location":"terms/tool-use/#relationships","title":"Relationships","text":"<ul> <li>Broader: agentic ai</li> <li>Narrower: retrieval tool, calculator tool</li> <li>Related: retrieval-augmented generation, system prompt, guardrails</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'tool use'.</p>"},{"location":"terms/tool-use/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>Anthropic \u2013 Tool Use</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/tool-use.yml</code></p>"},{"location":"terms/top-k-sampling/","title":"top-k sampling","text":""},{"location":"terms/top-k-sampling/#top-k-sampling","title":"top-k sampling","text":"<p>Aliases: k-sampling, truncated sampling Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p> <p>Put it into practice</p> <p>See the Prompt Engineering Playbook for tuning tips.</p>"},{"location":"terms/top-k-sampling/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/top-k-sampling/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/top-k-sampling/#short-definition","title":"Short definition","text":"<p>Decoding method that samples from the k most probable next tokens to balance diversity and control.</p>"},{"location":"terms/top-k-sampling/#long-definition","title":"Long definition","text":"<p>Top-k sampling limits the candidate set during generation to the k tokens with the highest probabilities after applying the model\u2019s softmax distribution. By discarding the long tail of unlikely options, the technique keeps outputs coherent while preserving some variability relative to greedy decoding. Product teams use top-k to tune tone and creativity without inviting the unrestricted randomness of pure sampling. Engineers choose k values based on experimentation with validation prompts, often combining the method with temperature scaling or nucleus sampling to fine-tune randomness. Operationally, top-k is simple to implement and deterministic when paired with fixed seeds, which supports reproducibility requirements. Governance reviewers document chosen k values because they affect the likelihood of unsafe or off-policy responses; overly permissive settings can undo safety evaluations performed at lower k thresholds. As part of responsible deployment, teams monitor how k adjustments interact with guardrails, cost, and quality metrics across releases.</p>"},{"location":"terms/top-k-sampling/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: A knob that keeps the model creative but still focused on the best few answers.</li> <li>Engineer: Truncate the probability distribution to the highest-probability k tokens before sampling to manage diversity versus determinism.</li> </ul>"},{"location":"terms/top-k-sampling/#examples","title":"Examples","text":"<p>Do - Evaluate customer support flows at k=1, 10, and 40 to understand trade-offs in tone and accuracy.</p> <p>Don't - Increase k in production without rerunning safety and bias evaluations.</p>"},{"location":"terms/top-k-sampling/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: robustness, transparency</li> <li>Risk notes: Raising k expands behavioral variance and can expose users to unreviewed content patterns.</li> </ul>"},{"location":"terms/top-k-sampling/#relationships","title":"Relationships","text":"<ul> <li>Broader: decoding</li> <li>Related: temperature, top-p sampling, greedy decoding</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'top-k sampling'.</p>"},{"location":"terms/top-k-sampling/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/top-k-sampling.yml</code></p>"},{"location":"terms/top-p-sampling/","title":"top-p sampling","text":""},{"location":"terms/top-p-sampling/#top-p-sampling","title":"top-p sampling","text":"<p>Aliases: nucleus sampling, p-sampling Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p> <p>Put it into practice</p> <p>See the Prompt Engineering Playbook for tuning tips.</p>"},{"location":"terms/top-p-sampling/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/top-p-sampling/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.</li> <li>Share findings with enablement so downstream teams understand model implications.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/top-p-sampling/#short-definition","title":"Short definition","text":"<p>Decoding strategy that samples from the smallest set of tokens whose probabilities sum to p.</p>"},{"location":"terms/top-p-sampling/#long-definition","title":"Long definition","text":"<p>Top-p sampling, also called nucleus sampling, builds a dynamic shortlist of candidate tokens whose cumulative probability exceeds a threshold p. Instead of fixing the number of options, the method adapts to the shape of the probability distribution: sharply peaked distributions produce small candidate sets, while flatter distributions expand the pool. This flexibility makes top-p useful for preserving coherence in confident contexts while still allowing creative variations when the model is less certain. Engineers tune the p value in tandem with temperature to achieve the desired balance between determinism and variety. Product teams rely on the technique for experiences like storytelling, marketing copy, or brainstorming where monotone responses are undesirable. Governance teams record chosen p values alongside evaluation evidence, recognizing that higher thresholds can surface content not vetted during safety reviews. Monitoring how p interacts with policy classifiers and guardrails is an integral part of ongoing risk management.</p>"},{"location":"terms/top-p-sampling/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: A probability threshold that keeps responses varied without wandering too far off message.</li> <li>Engineer: Accumulate token probabilities until they exceed p, normalize, then sample\u2014yielding adaptive candidate sets per step.</li> </ul>"},{"location":"terms/top-p-sampling/#examples","title":"Examples","text":"<p>Do - Test critical workflows at p values of 0.7, 0.9, and 0.95 to document behavioral differences.</p> <p>Don't - Use high p thresholds for regulated communications without updated compliance sign-off.</p>"},{"location":"terms/top-p-sampling/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: robustness, transparency</li> <li>Risk notes: Large nucleus thresholds can introduce unvetted behaviors and may invalidate safety testing baselines.</li> </ul>"},{"location":"terms/top-p-sampling/#relationships","title":"Relationships","text":"<ul> <li>Broader: decoding</li> <li>Related: temperature, top-k sampling, greedy decoding</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'top-p sampling'.</p>"},{"location":"terms/top-p-sampling/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/top-p-sampling.yml</code></p>"},{"location":"terms/training-data/","title":"training data","text":""},{"location":"terms/training-data/#training-data","title":"training data","text":"<p>Aliases: training set, learning set Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/training-data/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/training-data/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Add this concept to onboarding materials so teammates share a common baseline.</li> <li>Link supporting research or documentation in your internal wiki for deeper study.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/training-data/#short-definition","title":"Short definition","text":"<p>Labeled examples the model learns from before it ever sees validation or test inputs.</p>"},{"location":"terms/training-data/#long-definition","title":"Long definition","text":"<p>Training data is the portion of the dataset used to fit model parameters. These examples expose the algorithm to the signals it should internalize, from class boundaries to feature interactions. High-quality training data mirrors the production environment, includes the right balance of cohorts, and has documented provenance so teams can trace issues. Engineers tune hyperparameters and optimization steps against this split, while product partners track which customer or policy scenarios are represented. Weak training data leads to brittle models that memorize quirks rather than patterns, increasing the odds of bias, regressions, and governance failures when the system is deployed.</p>"},{"location":"terms/training-data/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Training data is the fuel that teaches the model what \"good\" looks like before launch.</li> <li>Engineer: Curate representative, versioned examples with clear labels; document preprocessing and access controls.</li> </ul>"},{"location":"terms/training-data/#examples","title":"Examples","text":"<p>Do - Record data lineage and consent before moving samples into the training set. - Refresh training data when product usage, markets, or regulations shift.</p> <p>Don't - Mix evaluation samples back into training without resetting benchmark baselines. - Ignore gaps in protected classes or edge scenarios that appear in production.</p>"},{"location":"terms/training-data/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: data_quality, transparency</li> <li>Risk notes: Poorly governed training data creates hidden bias and auditing gaps that surface as fairness or compliance incidents.</li> </ul>"},{"location":"terms/training-data/#relationships","title":"Relationships","text":"<ul> <li>Related: data lineage, cross-validation, generalization, regularization</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'training data'.</p>"},{"location":"terms/training-data/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>NIST AI Risk Management Framework</li> <li>Wikipedia \u2013 Training, Validation, and Test Sets</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/training-data.yml</code></p>"},{"location":"terms/transparency-report/","title":"transparency report","text":""},{"location":"terms/transparency-report/#transparency-report","title":"transparency report","text":"<p>Aliases: algorithmic transparency report, safety disclosure report Categories: Governance &amp; Risk Roles: Policy &amp; Risk, Legal &amp; Compliance, Communications &amp; Enablement, Product &amp; Program Managers Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/transparency-report/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/transparency-report/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/transparency-report/#short-definition","title":"Short definition","text":"<p>Periodic disclosure that details how an AI system operates, what data it handles, and how risks are mitigated.</p>"},{"location":"terms/transparency-report/#long-definition","title":"Long definition","text":"<p>A transparency report is a structured publication that explains how an AI system was built, deployed, and governed. It typically documents training data provenance, safety mitigations, evaluation outcomes, incidents, and law-enforcement or policy interactions. Regulators increasingly require providers to publish these reports on a fixed cadence, while enterprises use them to demonstrate accountability to customers and civil society. Effective reports pair quantitative metrics with narrative context so stakeholders can understand both capabilities and limitations. Policy and legal teams coordinate the disclosure scope, communications tailors the narrative, and product owners provide operational detail. Without disciplined reporting, organizations struggle to prove compliance, respond to audits, or build trust with users.</p>"},{"location":"terms/transparency-report/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Use transparency reports to show regulators and customers that your AI program meets accountability expectations.</li> <li>Engineer: Treat the report as a contract: ensure logs, metrics, and documentation can back every disclosure you publish.</li> </ul>"},{"location":"terms/transparency-report/#examples","title":"Examples","text":"<p>Do - Publish red-team findings, mitigation steps, and open risks alongside capability highlights. - Describe data retention periods and consent mechanisms in plain language instead of legal shorthand.</p> <p>Don't - Release marketing copy that omits failure modes, known biases, or the limits of human oversight. - Bundle multiple system changes into one vague update that hides the timeline regulators expect.</p>"},{"location":"terms/transparency-report/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, accountability, governance</li> <li>Risk notes: Incomplete or inaccurate disclosures expose the organization to regulatory penalties and reputational damage.</li> </ul>"},{"location":"terms/transparency-report/#relationships","title":"Relationships","text":"<ul> <li>Broader: responsible ai, model governance</li> <li>Narrower: safety spec, risk register</li> <li>Related: algorithmic audit, ai assurance, privacy impact assessment</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'transparency report'.</p>"},{"location":"terms/transparency-report/#citations","title":"Citations","text":"<ul> <li>European Commission \u2013 DSA Transparency Database</li> <li>Wikipedia \u2013 Transparency Report</li> <li>University at Buffalo \u2013 AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/transparency-report.yml</code></p>"},{"location":"terms/validation-set/","title":"validation set","text":""},{"location":"terms/validation-set/#validation-set","title":"validation set","text":"<p>Aliases: validation data, dev set Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/validation-set/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/validation-set/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Add this concept to onboarding materials so teammates share a common baseline.</li> <li>Link supporting research or documentation in your internal wiki for deeper study.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/validation-set/#short-definition","title":"Short definition","text":"<p>Dataset slice used to tune hyperparameters and compare experiments without touching the test set.</p>"},{"location":"terms/validation-set/#long-definition","title":"Long definition","text":"<p>The validation set provides an unbiased checkpoint during model development. After fitting on the training data, teams gauge performance on this held-out split to choose architectures, hyperparameters, and stopping criteria. A disciplined validation strategy prevents information leakage into the final test set, offering a realistic signal of how design choices will play out in production. Product leaders rely on validation metrics to weigh accuracy against latency or cost trade-offs, while engineers track subgroup results to guard against bias. Clear documentation of validation protocol, refresh cadence, and governance sign-off keeps experiment logs reproducible and audit ready.</p>"},{"location":"terms/validation-set/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Validation data is the dress rehearsal that tells us if the model is ready before the final exam.</li> <li>Engineer: Keep validation slices immutable, monitor for drift, and reset them when feature space or labeling rules change.</li> </ul>"},{"location":"terms/validation-set/#examples","title":"Examples","text":"<p>Do - Log every hyperparameter trial that reads the validation set to maintain experiment lineage. - Audit validation performance across protected cohorts before promoting a model.</p> <p>Don't - Tune against the test set or stack multiple validations from overlapping data. - Treat validation metrics as final sign-off without human review or business context.</p>"},{"location":"terms/validation-set/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: robustness, accountability</li> <li>Risk notes: Mismanaged validation data masks overfitting and governance gaps, leading to unexpected failures after launch.</li> </ul>"},{"location":"terms/validation-set/#relationships","title":"Relationships","text":"<ul> <li>Broader: cross-validation</li> <li>Related: generalization, training data, regularization, evaluation</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'validation set'.</p>"},{"location":"terms/validation-set/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>NIST AI Risk Management Framework</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/validation-set.yml</code></p>"},{"location":"terms/vector-store/","title":"vector store","text":""},{"location":"terms/vector-store/#vector-store","title":"vector store","text":"<p>Aliases: vector database, embedding index Categories: Retrieval &amp; RAG Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun_phrase</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/vector-store/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/vector-store/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Validate retrieval quality using the evaluation guidance referenced in this entry.</li> <li>Ensure knowledge sources named here appear in your data governance inventory.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/vector-store/#short-definition","title":"Short definition","text":"<p>Database optimized to store embeddings and execute similarity search over vectors.</p>"},{"location":"terms/vector-store/#long-definition","title":"Long definition","text":"<p>A vector store persists embeddings alongside metadata and exposes similarity search primitives such as nearest-neighbor queries, filtering, and hybrid scoring. Unlike relational databases, vector stores optimize for high-dimensional distance computations, using structures like HNSW graphs, IVF lists, or product quantization to balance recall and latency. They are a foundational component of retrieval-augmented generation platforms because they resolve which document chunks should accompany a prompt. Operational teams monitor vector stores for index freshness, replication, and backup coverage, while engineers tune them for sharding strategies, dimensionality alignment with embedding models, and filterable metadata schemas. Governance teams oversee access controls and deletion workflows to guarantee that personal or regulated data can be removed promptly. Properly instrumented vector stores also log which records contributed to an answer, supporting audit trails and user-facing citations.</p>"},{"location":"terms/vector-store/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Think of a vector store as the searchable memory that lets AI find related knowledge quickly.</li> <li>Engineer: Specialized database offering ANN search over embedding vectors with metadata filters and lifecycle management.</li> </ul>"},{"location":"terms/vector-store/#examples","title":"Examples","text":"<p>Do - Align vector dimensionality and distance metrics with the embedding model before bulk ingestion.</p> <p>Don't - Leave vector indexes unversioned, making it impossible to trace which data powered a response.</p>"},{"location":"terms/vector-store/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, data_quality</li> <li>Risk notes: Lax access controls or deletion policies can expose sensitive embeddings or violate retention requirements.</li> </ul>"},{"location":"terms/vector-store/#relationships","title":"Relationships","text":"<ul> <li>Broader: retrieval infrastructure</li> <li>Narrower: approximate nearest neighbor index</li> <li>Related: embedding, retrieval, chunking</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'vector store'.</p>"},{"location":"terms/vector-store/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>NIST AI RMF Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/vector-store.yml</code></p>"},{"location":"terms/voice-cloning/","title":"voice cloning","text":""},{"location":"terms/voice-cloning/#voice-cloning","title":"voice cloning","text":"<p>Aliases: voice synthesis, speech cloning Categories: Foundations, Governance &amp; Risk Roles: Product &amp; Program Managers, Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Security &amp; Trust Part of speech: <code>process</code> Status: Approved (Last reviewed: 2025-09-29)</p>"},{"location":"terms/voice-cloning/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/voice-cloning/#practice-apply","title":"Practice &amp; apply","text":"<ul> <li>Add this concept to onboarding materials so teammates share a common baseline.</li> <li>Link supporting research or documentation in your internal wiki for deeper study.</li> <li>Map this term to the governance dashboard and record accountable owners in the backlog.</li> <li>Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.</li> <li>Share takeaways with the accountable roles listed above so actions land with the right owners.</li> </ul>"},{"location":"terms/voice-cloning/#short-definition","title":"Short definition","text":"<p>Technique that replicates a person\u2019s voice using generative models trained on audio samples.</p>"},{"location":"terms/voice-cloning/#long-definition","title":"Long definition","text":"<p>Voice cloning systems learn a speaker\u2019s vocal characteristics from recorded samples and synthesize new speech that mimics that voice. Modern approaches use encoder-decoder architectures, diffusion models, or transformer-based TTS pipelines conditioned on speaker embeddings. While voice cloning powers accessibility, localization, and creative tools, it also raises serious risks: impersonation, fraud, misinformation, and consent violations. Product teams must obtain clear user permissions and provide safeguards like watermarks or audible disclosures. Legal and policy teams assess compliance with biometric privacy laws and emerging deepfake regulations. Security groups monitor abuse signals and coordinate rapid takedowns when clones are misused. Transparency, usage logs, and red-team exercises are essential for trustworthy deployment.</p>"},{"location":"terms/voice-cloning/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Voice cloning lets you generate speech that sounds like a real person\u2014but it needs strict guardrails.</li> <li>Engineer: Extract speaker embeddings, condition neural TTS or diffusion decoders, and enforce consent, watermarking, and usage logs.</li> </ul>"},{"location":"terms/voice-cloning/#examples","title":"Examples","text":"<p>Do - Require opt-in consent and verification before training on a person\u2019s voice. - Embed inaudible watermarks and provide detection tools to partners.</p> <p>Don't - Release cloning features without a response plan for malicious use. - Store raw recordings longer than necessary or without encryption.</p>"},{"location":"terms/voice-cloning/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: risk_management, transparency</li> <li>Risk notes: Misuse of voice cloning can lead to fraud, reputational harm, and regulatory penalties; strict access controls and auditing are mandatory.</li> </ul>"},{"location":"terms/voice-cloning/#relationships","title":"Relationships","text":"<ul> <li>Broader: generative ai</li> <li>Related: synthetic data, content moderation, incident response</li> </ul> <p>Something missing?</p> <p>Suggest examples or clarifications via the term request intake and mention 'voice cloning'.</p>"},{"location":"terms/voice-cloning/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Wikipedia AI Glossary</li> <li>Microsoft \u2013 Responsible AI Guardrails</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/voice-cloning.yml</code></p>"}]}