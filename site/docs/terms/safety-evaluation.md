<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# safety evaluation

**Aliases:** safety testing, safety assessment
**Categories:** Governance & Risk, Operations & Monitoring
**Roles:** Engineering & Platform, Policy & Risk, Product & Program Managers, Communications & Enablement
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-09)

## Role takeaways
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Communications & Enablement:** Align messaging, FAQs, and enablement materials using this definition.

## Practice & apply
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Instrument dashboards or alerts that reflect the metrics highlighted in this definition.
- Update incident response or on-call runbooks with the glossary's do/don't scenarios.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.

## Long definition
Safety evaluations probe AI systems for dangerous or disallowed behavior, complementing performance metrics with targeted abuse, bias, and compliance tests. Teams blend automated classifiers, curated prompt suites, and expert reviews to measure toxicity, misinformation, self-harm encouragement, and other high-risk outcomes. Results feed into guardrail tuning, incident response plans, and launch gate decisions. Engineering and policy teams collaborate on coverage, ensuring critical user journeys and demographic perspectives are represented. Communications and legal stakeholders review findings to shape disclosures and mitigation commitments. Safety evaluations are continuous: regressions can surface after prompt changes, model updates, or content shifts, so organizations schedule recurring runs and capture evidence for audits. Failing to operationalize safety evaluations at scale exposes products to public incidents, regulatory scrutiny, and erosion of user trust.

## Audience perspectives
- **Exec:** Safety evaluation is the checkpoint that proves the AI won’t violate our policies or harm users.
- **Engineer:** Execute targeted red-team suites, automated toxicity checks, and manual reviews; document thresholds, residual risk, and remediation plans.

## Examples
**Do**
- Store evaluation artifacts alongside release notes for traceability.
- Include marginalized community perspectives in the review panel and prompt set.

**Don't**
- Assume safety coverage automatically transfers when prompts or models change.
- Treat a passing score as permanent—schedule re-tests after meaningful updates.

## Governance
- **NIST RMF tags:** risk_management, accountability
- **Risk notes:** Skipping or deferring safety evaluations invites policy breaches, public incidents, and enforcement actions.

## Relationships
- **Broader:** evaluation
- **Related:** red teaming, guardrails, incident response

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'safety evaluation'.

## Citations
- [NIST AI RMF Glossary](https://www.nist.gov/itl/ai-risk-management-framework)
- [OpenAI – Safety Evaluations Overview](https://learn.microsoft.com/en-us/security/ai-red-team/)
- [Stanford HAI Brief Definitions](https://hai.stanford.edu/news/brief-definitions)

_License: CC BY-SA 4.0_

_Source file: `data/terms/safety-evaluation.yml`_
