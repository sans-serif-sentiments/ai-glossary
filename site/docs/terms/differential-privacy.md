<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# differential privacy

**Aliases:** DP, epsilon-differential privacy
**Categories:** Governance & Risk
**Roles:** Engineering & Platform, Legal & Compliance, Policy & Risk, Security & Trust
**Part of speech:** `concept`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-10)

## Role takeaways
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Legal & Compliance:** Assess contractual and regulatory obligations tied to this term.
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Security & Trust:** Plan monitoring and abuse prevention scenarios influenced by this term.

## Practice & apply
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Mathematical framework that limits how much any single record influences published data or model outputs.

## Long definition
Differential privacy protects individuals in a dataset by adding calibrated noise to statistics or training procedures so the presence or absence of any one person becomes indistinguishable. The framework is governed by privacy budgets—epsilon and delta—which quantify acceptable leakage. In AI systems, teams apply differential privacy when releasing analytics, training embeddings, or sharing evaluation datasets. Engineers integrate mechanisms like DP-SGD, Laplace noise, or randomized response, tracking accumulated budgets across queries. Legal and policy partners evaluate whether privacy guarantees meet regulatory requirements, especially when models ingest sensitive or regulated data. Security teams monitor for side-channel attacks that could combine multiple noisy outputs to infer personal information. Differential privacy is not a silver bullet; product and research groups balance utility loss against risk mitigation, document assumptions, and communicate residual exposure to stakeholders.

## Audience perspectives
- **Exec:** Differential privacy lets you learn from user data while keeping any single person unidentifiable.
- **Engineer:** Bound information leakage by injecting calibrated noise; manage cumulative epsilon/delta budgets across analytics or training steps.

## Examples
**Do**
- Track privacy budgets in dashboards so analysts know when to stop issuing queries.
- Explain residual risk and utility trade-offs in launch documentation.

**Don't**
- Assume a single noisy release protects against repeated queries without monitoring budget depletion.
- Mix differentially private and non-private data exports without clear labeling.

## Governance
- **NIST RMF tags:** privacy, risk_management
- **Risk notes:** Incorrect epsilon or budget accounting can create a false sense of protection and trigger regulatory exposure.

## Relationships
- **Broader:** privacy
- **Narrower:** differentially private SGD
- **Related:** synthetic data, guardrails, model governance

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'differential privacy'.

## Citations
- [NIST AI RMF Glossary](https://www.nist.gov/itl/ai-risk-management-framework)
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Wikipedia AI Glossary](https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence)

_License: CC BY-SA 4.0_

_Source file: `data/terms/differential-privacy.yml`_
