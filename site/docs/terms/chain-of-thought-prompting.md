<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# chain-of-thought prompting

**Aliases:** cot prompting, step-by-step prompting
**Categories:** LLM Core
**Roles:** Engineering & Platform, Data Science & Research, Product & Program Managers
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-09)

## Role takeaways
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.
- Share findings with enablement so downstream teams understand model implications.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Prompting technique that asks models to reason through intermediate steps before giving a final answer.

## Long definition
Chain-of-thought prompting instructs a model to show its reasoning by generating step-by-step explanations. The extra reasoning tokens often improve accuracy on tasks like math, code, and logic by encouraging deliberate thinking. Engineers use chain-of-thought to debug and audit reasoning, while policy teams review steps for safety or bias. Product teams decide when to expose the chain to end users versus using it internally for verification. The technique increases latency and risk of leaking sensitive reasoning, so outputs must still be checked by evaluations or secondary models.

## Audience perspectives
- **Exec:** Apply chain-of-thought selectively where explainability or reasoning accuracy matters most.
- **Engineer:** Pair reasoning traces with validators to catch hallucinated logic or policy violations.

## Examples
**Do**
- Prompt models with `Let's reason step by step` for complex calculations.
- Filter intermediate reasoning before displaying responses to end users.

**Don't**
- Assume longer reasoning always means higher accuracy.
- Expose sensitive internal policies in reasoning traces.

## Governance
- **NIST RMF tags:** transparency, risk_management, monitoring
- **Risk notes:** Verbose reasoning can leak policy details or amplify biased logic if unchecked.

## Relationships
- **Broader:** prompt engineering
- **Related:** self-consistency decoding, robust prompting, evaluation

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'chain-of-thought prompting'.

## Citations
- [Google – Chain of Thought Prompting](https://arxiv.org/abs/2201.11903)
- [DeepMind – Improving Reasoning in Language Models](https://www.deepmind.com/blog)

_License: CC BY-SA 4.0_

_Source file: `data/terms/chain-of-thought-prompting.yml`_
