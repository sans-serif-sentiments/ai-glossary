<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# evaluation

**Aliases:** model evaluation, AI evaluation
**Categories:** Operations & Monitoring, Governance & Risk
**Roles:** Communications & Enablement, Engineering & Platform, Legal & Compliance, Policy & Risk, Product & Program Managers, Security & Trust
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-09)

## Role takeaways
- **Communications & Enablement:** Align messaging, FAQs, and enablement materials using this definition.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Legal & Compliance:** Assess contractual and regulatory obligations tied to this term.
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Security & Trust:** Plan monitoring and abuse prevention scenarios influenced by this term.

## Practice & apply
- Instrument dashboards or alerts that reflect the metrics highlighted in this definition.
- Update incident response or on-call runbooks with the glossary's do/don't scenarios.
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Systematic measurement of model performance, safety, and reliability using defined tests.

## Long definition
Evaluation is the disciplined practice of testing AI systems against quantitative and qualitative criteria before and after deployment. It extends beyond accuracy metrics to encompass robustness, bias detection, factual correctness, latency, and safety stress tests such as red teaming or jailbreak attempts. Teams build eval suites that blend automated metrics—like BLEU, accuracy@k, or toxicity scores—with human review checklists tailored to critical user journeys. Continuous evaluation supports regression detection when prompts, datasets, or infrastructure change. Governance frameworks treat evaluations as audit artifacts: they document assumptions, thresholds, and sign-offs required before promoting models to production. Mature programs integrate evaluation pipelines into CI/CD, enabling reproducibility and traceability. Without rigorous evaluation, organizations cannot credibly claim their models meet compliance obligations or user expectations.

## Audience perspectives
- **Exec:** Evaluation is our quality gate—it proves the AI delivers safe, reliable outcomes before we launch.
- **Engineer:** Automated and human-in-the-loop test harnesses measuring task metrics, robustness, bias, and safety across model releases.

## Examples
**Do**
- Run targeted red-team scenarios alongside quantitative metrics before shipping new prompts or fine-tuned models.

**Don't**
- Rely on a single aggregate score without examining subgroup performance or qualitative feedback.

## Governance
- **NIST RMF tags:** validity, accountability
- **Risk notes:** Skipping or weakening evaluations increases the likelihood of undetected harmful behaviors in production.

## Relationships
- **Broader:** model governance
- **Narrower:** safety evaluation, capability evaluation
- **Related:** guardrails, alignment, red teaming

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'evaluation'.

## Citations
- [NIST AI RMF Glossary](https://www.nist.gov/itl/ai-risk-management-framework)
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Stanford HAI Brief Definitions](https://hai.stanford.edu/news/brief-definitions)

_License: CC BY-SA 4.0_

_Source file: `data/terms/evaluation.yml`_
