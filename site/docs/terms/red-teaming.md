<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# red teaming

**Aliases:** ai red teaming, adversarial testing
**Categories:** Governance & Risk, Operations & Monitoring
**Roles:** Communications & Enablement, Engineering & Platform, Legal & Compliance, Policy & Risk, Product & Program Managers, Security & Trust
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-10)

## Role takeaways
- **Communications & Enablement:** Align messaging, FAQs, and enablement materials using this definition.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Legal & Compliance:** Assess contractual and regulatory obligations tied to this term.
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Security & Trust:** Plan monitoring and abuse prevention scenarios influenced by this term.

## Practice & apply
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Instrument dashboards or alerts that reflect the metrics highlighted in this definition.
- Update incident response or on-call runbooks with the glossary's do/don't scenarios.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.

## Long definition
Red teaming mobilizes interdisciplinary experts to craft adversarial prompts, scenarios, and data inputs that challenge an AI system’s safeguards. The goal is to uncover failure modes—such as unsafe content, confidential data leaks, or jailbreak exploits—before attackers or end users discover them. Exercises blend automated probing, scripted attack playbooks, and human creativity. Findings feed into remediation plans for guardrails, prompts, training data, or escalation policies. Product leaders schedule recurring red-team cycles for high-risk surfaces, while engineers build tooling to log attempts, reproduce issues, and verify fixes. Governance teams treat red teaming as part of risk management, requiring documentation of scope, participants, severity ratings, and follow-up actions. In many jurisdictions, regulators expect evidence that red teaming has been performed for sensitive deployments, making it a core component of responsible AI programs.

## Audience perspectives
- **Exec:** Red teaming is the pre-launch fire drill that exposes how the AI could fail or be abused.
- **Engineer:** Design adversarial prompts and automated probes, capture reproduction artifacts, and track mitigation work in the backlog.

## Examples
**Do**
- Incorporate marginalized community perspectives when designing red-team scenarios.

**Don't**
- Close a red-team finding without documenting remediation owners and timelines.

## Governance
- **NIST RMF tags:** risk_management, accountability
- **Risk notes:** Skipping red teaming leaves blind spots that can result in public incidents or regulatory enforcement.

## Relationships
- **Broader:** evaluation
- **Related:** guardrails, alignment, incident response

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'red teaming'.

## Citations
- [CISA – AI Security Incident Response Guidelines](https://www.cisa.gov/resources-tools/resources/ai-cybersecurity-collaboration-playbook)
- [OpenAI – Red Teaming Network](https://learn.microsoft.com/en-us/security/ai-red-team/)
- [Wikipedia – Red Team](https://en.wikipedia.org/wiki/Red_team)

_License: CC BY-SA 4.0_

_Source file: `data/terms/red-teaming.yml`_
