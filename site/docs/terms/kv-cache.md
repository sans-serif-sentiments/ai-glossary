<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# kv cache

**Aliases:** key-value cache, attention cache
**Categories:** LLM Core
**Roles:** Data Science & Research, Engineering & Platform, Product & Program Managers
**Part of speech:** `noun_phrase`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-10)

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.
- Share findings with enablement so downstream teams understand model implications.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Stored attention keys and values reused across decoding steps to speed sequential generation.

## Long definition
The KV cache holds intermediate key and value tensors produced during transformer attention so they can be reused on subsequent tokens without recomputing earlier layers. During autoregressive decoding, each new token only needs to attend to prior states; caching those states significantly reduces latency and compute, especially for long prompts or streaming responses. Production systems manage KV cache sizes carefully because they grow with context length, consuming memory on GPUs and influencing batch throughput. Engineers optimize cache eviction policies, quantization, or paged memory formats to balance cost and responsiveness. Governance and reliability teams monitor KV cache behavior to ensure no residual data persists longer than intended, particularly when serving multi-tenant workloads where prompts may contain sensitive information. Documenting cache configuration is part of operational playbooks for diagnosing performance regressions.

## Audience perspectives
- **Exec:** The KV cache is the reuse trick that keeps responses snappy even when conversations get long.
- **Engineer:** Per-layer tensors of attention keys/values stored between decoding steps to avoid recomputing full sequence context.

## Examples
**Do**
- Audit GPU memory usage with and without KV caching to plan capacity for long-context workloads.

**Don't**
- Share KV cache contents across tenants without isolation controls.

## Governance
- **NIST RMF tags:** efficiency, privacy
- **Risk notes:** Improper cache management can leak residual user data or trigger out-of-memory failures.

## Relationships
- **Broader:** inference optimization
- **Narrower:** paged attention
- **Related:** attention, context window, quantization

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'kv cache'.

## Citations
- [Hugging Face Glossary](https://huggingface.co/docs/transformers/en/glossary)
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Wikipedia AI Glossary](https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence)

_License: CC BY-SA 4.0_

_Source file: `data/terms/kv-cache.yml`_
