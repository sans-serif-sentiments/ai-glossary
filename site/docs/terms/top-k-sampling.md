<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# top-k sampling

**Aliases:** k-sampling, truncated sampling
**Categories:** LLM Core
**Roles:** Data Science & Research, Engineering & Platform, Product & Program Managers
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-10)

!!! tip "Put it into practice"
    See the [Prompt Engineering Playbook](../prompting.md) for tuning tips.

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.
- Share findings with enablement so downstream teams understand model implications.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Decoding method that samples from the k most probable next tokens to balance diversity and control.

## Long definition
Top-k sampling limits the candidate set during generation to the k tokens with the highest probabilities after applying the modelâ€™s softmax distribution. By discarding the long tail of unlikely options, the technique keeps outputs coherent while preserving some variability relative to greedy decoding. Product teams use top-k to tune tone and creativity without inviting the unrestricted randomness of pure sampling. Engineers choose k values based on experimentation with validation prompts, often combining the method with temperature scaling or nucleus sampling to fine-tune randomness. Operationally, top-k is simple to implement and deterministic when paired with fixed seeds, which supports reproducibility requirements. Governance reviewers document chosen k values because they affect the likelihood of unsafe or off-policy responses; overly permissive settings can undo safety evaluations performed at lower k thresholds. As part of responsible deployment, teams monitor how k adjustments interact with guardrails, cost, and quality metrics across releases.

## Audience perspectives
- **Exec:** A knob that keeps the model creative but still focused on the best few answers.
- **Engineer:** Truncate the probability distribution to the highest-probability k tokens before sampling to manage diversity versus determinism.

## Examples
**Do**
- Evaluate customer support flows at k=1, 10, and 40 to understand trade-offs in tone and accuracy.

**Don't**
- Increase k in production without rerunning safety and bias evaluations.

## Governance
- **NIST RMF tags:** robustness, transparency
- **Risk notes:** Raising k expands behavioral variance and can expose users to unreviewed content patterns.

## Relationships
- **Broader:** decoding
- **Related:** temperature, top-p sampling, greedy decoding

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'top-k sampling'.

## Citations
- [Hugging Face Glossary](https://huggingface.co/docs/transformers/en/glossary)
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Wikipedia AI Glossary](https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence)

_License: CC BY-SA 4.0_

_Source file: `data/terms/top-k-sampling.yml`_
