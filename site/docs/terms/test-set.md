<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# test set

**Aliases:** test data, holdout set
**Categories:** Foundations
**Roles:** Data Science & Research, Engineering & Platform, Product & Program Managers
**Part of speech:** `noun`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-09)

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Add this concept to onboarding materials so teammates share a common baseline.
- Link supporting research or documentation in your internal wiki for deeper study.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Final evaluation split reserved for measuring real-world performance after all model tuning is finished.

## Long definition
The test set acts as the last line of defense before deployment. Once training and validation are complete, teams use this untouched data slice to estimate how a model will behave in production. Because it reflects realistic scenarios and has never influenced tuning decisions, the test set provides credible evidence for governance reviews, model cards, and launch approvals. Product owners study test outcomes to understand business impact, while engineers compare historical test set runs to spot regressions or drift. Strict separation from training and validation data, coupled with version control and documented refresh policies, preserves trust in test results.

## Audience perspectives
- **Exec:** The test set is our final validation that the model keeps its promises before customers see it.
- **Engineer:** Protect the test set from reuse, track baseline metrics over time, and rotate it only with formal approvals.

## Examples
**Do**
- Store multiple generations of test sets so you can benchmark new models against historical baselines.
- Capture subgroup metrics and qualitative notes during every test run for audit trails.

**Don't**
- Peek at the test set to choose hyperparameters or feature engineering steps.
- Reuse the same test set indefinitely without monitoring for stale scenarios.

## Governance
- **NIST RMF tags:** validity, accountability
- **Risk notes:** Compromised test sets hide regressions and undercut the evidence required for responsible AI approvals.

## Relationships
- **Related:** validation set, generalization, evaluation, model drift

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'test set'.

## Citations
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary#test-set)
- [NIST AI Risk Management Framework](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf)
- [DeepLearning.AI Resources](https://www.deeplearning.ai/resources/)

_License: CC BY-SA 4.0_

_Source file: `data/terms/test-set.yml`_
