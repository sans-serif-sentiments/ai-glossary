<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# self-critique loop

**Aliases:** self-reflection loop, critique-and-revise
**Categories:** Agents & Tooling
**Roles:** Engineering & Platform, Product & Program Managers, Policy & Risk
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-09)

## Role takeaways
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Policy & Risk:** Map the definition to governance controls and review checklists.

## Practice & apply
- Audit exposed tools against the safeguards described and document approval paths.
- Test hand-offs with human reviewers to confirm the safety expectations captured here are met.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Pattern where a model reviews its own outputs, critiques them, and produces revisions before responding.

## Long definition
A self-critique loop introduces an internal review step into an AI workflow. After generating an initial answer, the model (or a paired model) evaluates the response against instructions, safety policies, or quality rubrics, then revises the output. Engineers use the loop to catch hallucinations or policy violations automatically, while product teams tune the number of critique iterations to balance latency and reliability. Governance partners supply critique prompts that focus on harm categories and compliance language. Without clear stop conditions, self-critique can increase cost or drift toward repetitive edits, so loops should cap iterations and escalate to humans when uncertainty remains high.

## Audience perspectives
- **Exec:** Invest in self-critique loops to raise reliability without assigning every response to manual review.
- **Engineer:** Instrument critique prompts, scoring, and escalation thresholds to keep loops fast and auditable.

## Examples
**Do**
- Log critique reasons and revision diffs so policy teams can audit how safety issues were resolved.
- Limit the loop to two or three iterations before escalating to a human reviewer.

**Don't**
- Allow the loop to run indefinitely chasing marginal quality gains.
- Rely solely on self-critique for high-severity categories without human oversight.

## Governance
- **NIST RMF tags:** monitoring, risk_management, transparency
- **Risk notes:** Unbounded self-critique can mask uncertain decisions and increase compute cost without clear accountability.

## Relationships
- **Broader:** constitutional ai, guardrails
- **Related:** robust prompting, safety spec, ai incident response

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'self-critique loop'.

## Citations
- [Anthropic – Constitutional AI](https://arxiv.org/abs/2212.08073)
- [Self-Refine – Iterative Refinement with LLMs](https://arxiv.org/abs/2303.11366)

_License: CC BY-SA 4.0_

_Source file: `data/terms/self-critique-loop.yml`_
