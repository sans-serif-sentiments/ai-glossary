<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# ml observability

**Aliases:** model observability, ai observability
**Categories:** Operations & Monitoring
**Roles:** Engineering & Platform, Policy & Risk, Security & Trust
**Part of speech:** `concept`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-09)

## Role takeaways
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Security & Trust:** Plan monitoring and abuse prevention scenarios influenced by this term.

## Practice & apply
- Instrument dashboards or alerts that reflect the metrics highlighted in this definition.
- Update incident response or on-call runbooks with the glossary's do/don't scenarios.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Practices and tooling that surface model health through metrics, traces, and alerts across the lifecycle.

## Long definition
ML observability applies observability principles to machine learning systems, collecting signals that describe data quality, model behavior, infrastructure health, and user outcomes. The discipline unifies telemetry such as latency, throughput, drift metrics, guardrail triggers, and human feedback so teams can diagnose issues quickly. Robust observability stacks ingest logs from preprocessing, inference, retrieval, and post-processing stages, correlating them with experiment metadata and deployment versions. Product owners reference observability dashboards to understand adoption and satisfaction, while engineers rely on them to root-cause regressions and capacity incidents. Governance programs require observable pipelines to demonstrate compliance with monitoring expectations in frameworks like the NIST AI RMF. Without observability, organizations struggle to detect bias, hallucinations, or safety incidents before they impact users. Investing in standardized logging, alerting, and runbooks enables proactive triage and continuous improvement.

## Audience perspectives
- **Exec:** ML observability provides the dashboards and alerts that show whether AI systems remain healthy and trustworthy.
- **Engineer:** Collect metrics, logs, and traces across data, model, and infra layers; stitch them to deployments for debugging and compliance.

## Examples
**Do**
- Correlate drift alerts with retrieval metrics to pinpoint whether failures stem from data or model changes.

**Don't**
- Disable guardrail logging due to cost; missing records breaks incident response and compliance.

## Governance
- **NIST RMF tags:** monitoring, accountability
- **Risk notes:** Insufficient observability hides safety incidents and undermines regulatory reporting obligations.

## Relationships
- **Broader:** ml ops
- **Related:** model drift, guardrails, evaluation

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'ml observability'.

## Citations
- [NIST AI RMF Glossary](https://www.nist.gov/itl/ai-risk-management-framework)
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Deepchecks â€“ ML Monitoring Overview](https://cloud.google.com/docs/generative-ai/glossary)

_License: CC BY-SA 4.0_

_Source file: `data/terms/ml-observability.yml`_
