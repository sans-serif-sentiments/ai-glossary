<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# low-rank adaptation

**Aliases:** LoRA, low rank fine-tuning
**Categories:** Optimization & Efficiency
**Roles:** Data Science & Research, Engineering & Platform
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-10)

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.

## Practice & apply
- Record before-and-after performance metrics when applying this optimisation technique.
- Document trade-offs for product and policy partners using the glossary's language.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Parameter-efficient fine-tuning that injects low-rank update matrices into transformer weights.

## Long definition
Low-rank adaptation (LoRA) fine-tunes large language models by learning compact update matrices rather than adjusting full weight tensors. The method freezes the original model parameters and trains additive low-rank factors that capture task-specific shifts, reducing memory usage and compute costs. LoRA adapters can be merged into the base model for deployment or stored separately to toggle behaviors per tenant. This approach enables organizations to customize models using modest hardware, accelerate experimentation, and share adapters without distributing full proprietary checkpoints. Engineers manage rank choices, scaling factors, and target layers to balance quality with efficiency. Governance teams evaluate LoRA artifacts like traditional model versions, reviewing data provenance, licensing, and security implications. Because adapters can encode sensitive capabilities, access control and documentation remain essentialâ€”particularly when multiple teams contribute adapters to a shared serving stack.

## Audience perspectives
- **Exec:** LoRA lets teams customize giant models cheaply by training small plug-ins instead of retraining everything.
- **Engineer:** Freeze base weights, insert trainable low-rank matrices into attention or feed-forward layers, and fine-tune with minimal VRAM.

## Examples
**Do**
- Track which datasets and objectives produced each LoRA adapter before sharing it across teams.

**Don't**
- Merge third-party adapters into production models without license verification.

## Governance
- **NIST RMF tags:** efficiency, accountability
- **Risk notes:** Unvetted adapters can override safety tuning or introduce licensed data without traceability.

## Relationships
- **Broader:** fine-tuning
- **Related:** quantization, distillation, guardrails

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'low-rank adaptation'.

## Citations
- [Hugging Face Glossary](https://huggingface.co/docs/transformers/en/glossary)
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [NIST AI RMF Glossary](https://www.nist.gov/itl/ai-risk-management-framework)

_License: CC BY-SA 4.0_

_Source file: `data/terms/low-rank-adaptation.yml`_
