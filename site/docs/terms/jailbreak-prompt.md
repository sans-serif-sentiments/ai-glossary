<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# jailbreak prompt

**Aliases:** prompt jailbreak, guardrail bypass
**Categories:** Governance & Risk
**Roles:** Security & Trust, Product & Program Managers, Engineering & Platform
**Part of speech:** `noun_phrase`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-09-29)

## Role takeaways
- **Security & Trust:** Plan monitoring and abuse prevention scenarios influenced by this term.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Engineering & Platform:** Document implementation requirements and operational caveats.

## Practice & apply
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Crafted input that persuades a model to ignore safety instructions and produce disallowed responses.

## Long definition
A jailbreak prompt is a deliberate attempt to bypass an AI system's guardrails. Attackers wrap malicious requests in roleplay, obfuscation, or multi-step instructions so the model forgets or ignores its policy. Jailbreaks often pair with prompt injection to extract secrets, generate harmful content, or trigger unauthorized tool calls. Defenders maintain libraries of known jailbreak patterns, run continuous red-teaming, and enforce layered mitigations (robust prompts, classifiers, tool restrictions). Product teams communicate refusals transparently, while governance partners document residual risks. Without jailbreak monitoring, safety regressions can go unnoticed until customers or regulators report incidents.

## Audience perspectives
- **Exec:** Treat jailbreaks as a measurable security risk with owners, metrics, and incident response plans.
- **Engineer:** Automate regression tests for known jailbreak patterns and update defenses when new variants appear.

## Examples
**Do**
- Maintain a catalog of blocked prompts and feed them into the evaluation harness.
- Pair jailbreak detection with human review for high-severity categories.

**Don't**
- Assume a single policy prompt will withstand evolving jailbreak tactics.
- Silently filter outputs without notifying security or policy teams.

## Governance
- **NIST RMF tags:** security, monitoring, risk_management
- **Risk notes:** Unmitigated jailbreaks can surface banned content, expose data, or erode user trust.

## Relationships
- **Broader:** prompt injection
- **Related:** robust prompting, red teaming, guardrails

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'jailbreak prompt'.

## Citations
- [arXiv – Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study](https://arxiv.org/abs/2305.13860)
- [Microsoft – Content Filter & Prompt Shields](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/content-filter-prompt-shields)

_License: CC BY-SA 4.0_

_Source file: `data/terms/jailbreak-prompt.yml`_
