<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# fairness metrics

**Aliases:** fairness measures, fairness evaluation
**Categories:** Governance & Risk, Operations & Monitoring
**Roles:** Policy & Risk, Legal & Compliance, Engineering & Platform, Product & Program Managers
**Part of speech:** `noun`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-10)

!!! tip "Put it into practice"
    Coordinate with the [Role Starter Packs](../roles.md#policy--risk) for governance actions.

## Role takeaways
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Legal & Compliance:** Assess contractual and regulatory obligations tied to this term.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Instrument dashboards or alerts that reflect the metrics highlighted in this definition.
- Update incident response or on-call runbooks with the glossary's do/don't scenarios.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Quantitative measures that evaluate whether model performance is equitable across groups.

## Long definition
Fairness metrics quantify disparities between demographic or contextual groups. Common measures include demographic parity, equalized odds, equal opportunity, predictive parity, and subgroup-specific precision/recall. Teams apply these metrics during model evaluation, post-deployment monitoring, and incident response to detect fairness regressions. Choosing the right metric depends on regulatory requirements and domain priorities; not all metrics can be optimized simultaneously. Policy and legal stakeholders define fairness thresholds, while engineers build pipelines that compute metrics for each release and in production dashboards. Findings feed into model cards, governance reviews, and mitigation plans.

## Audience perspectives
- **Exec:** Fairness metrics tell us whether different groups get comparable outcomes from the AI.
- **Engineer:** Compute metrics such as equalized odds, statistical parity difference, or subgroup ROC; monitor trends across releases and production.

## Examples
**Do**
- Align fairness thresholds with business and regulatory requirements.
- Track fairness metrics alongside traditional accuracy metrics in CI/CD.

**Don't**
- Rely on a single fairness metric without understanding trade-offs.
- Evaluate overall fairness without subgroup data or feedback from impacted communities.

## Governance
- **NIST RMF tags:** fairness, transparency
- **Risk notes:** Failure to monitor fairness metrics can lead to discrimination, legal liability, and loss of trust.

## Relationships
- **Broader:** algorithmic bias
- **Related:** model interpretability, evaluation, model card

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'fairness metrics'.

## Citations
- [NIST AI RMF Glossary](https://www.nist.gov/itl/ai-risk-management-framework)
- [Wikipedia â€“ Fairness in Machine Learning](https://en.wikipedia.org/wiki/Fairness_(machine_learning))
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)

_License: CC BY-SA 4.0_

_Source file: `data/terms/fairness-metrics.yml`_
