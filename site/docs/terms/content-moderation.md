<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# content moderation

**Aliases:** trust and safety, policy enforcement
**Categories:** Governance & Risk, Operations & Monitoring
**Roles:** Policy & Risk, Communications & Enablement, Product & Program Managers, Security & Trust, Engineering & Platform
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-09)

!!! tip "Put it into practice"
    Reference the [Governance Dashboard](../governance-dashboard.md) for monitoring obligations.

## Role takeaways
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Communications & Enablement:** Align messaging, FAQs, and enablement materials using this definition.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Security & Trust:** Plan monitoring and abuse prevention scenarios influenced by this term.
- **Engineering & Platform:** Document implementation requirements and operational caveats.

## Practice & apply
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Instrument dashboards or alerts that reflect the metrics highlighted in this definition.
- Update incident response or on-call runbooks with the glossary's do/don't scenarios.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Workflows and tools that review, filter, and act on user-generated content to enforce policy.

## Long definition
Content moderation combines automation, human review, and escalation procedures to detect policy violations such as hate speech, harassment, misinformation, or disallowed imagery. AI systems often provide the first layer of moderation by classifying or scoring content for human queues, requiring careful tuning of precision/recall trade-offs. Policy teams define enforcement rules, while communications and legal stakeholders handle appeals and transparency reports. Engineering teams maintain moderation pipelines, logging, and guardrails; security teams ensure abuse detection remains resilient. Effective moderation programs rely on measurement, red-teaming, and incident response to adapt to adversarial users and evolving regulations.

## Audience perspectives
- **Exec:** Content moderation protects users and the brand by keeping AI outputs and user posts within policy.
- **Engineer:** Blend classifiers, heuristic filters, and human review; monitor performance, appeals, and adversarial attempts.

## Examples
**Do**
- Audit moderation models for bias against protected groups.
- Publish user-facing guidelines and escalation paths.

**Don't**
- Rely solely on automation without human oversight for edge cases.
- Ignore feedback loops when policies change.

## Governance
- **NIST RMF tags:** risk_management, accountability
- **Risk notes:** Weak moderation exposes users to harm, invites regulatory fines, and erodes trust.

## Relationships
- **Broader:** guardrails
- **Related:** safety evaluation, incident response, algorithmic bias

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'content moderation'.

## Citations
- [European Commission â€“ Content Moderation Policy](https://digital-strategy.ec.europa.eu/en/policies/content-moderation)
- [NIST AI RMF Glossary](https://www.nist.gov/itl/ai-risk-management-framework)
- [Stanford HAI Brief Definitions](https://hai.stanford.edu/news/brief-definitions)

_License: CC BY-SA 4.0_

_Source file: `data/terms/content-moderation.yml`_
