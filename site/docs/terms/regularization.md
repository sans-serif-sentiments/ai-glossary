<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# regularization

**Aliases:** penalisation, weight decay
**Categories:** Foundations
**Roles:** Data Science & Research, Engineering & Platform, Product & Program Managers
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-09)

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Add this concept to onboarding materials so teammates share a common baseline.
- Link supporting research or documentation in your internal wiki for deeper study.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Techniques that add penalties or constraints during training to reduce overfitting and improve generalisation.

## Long definition
Regularization tempers a model’s tendency to memorise training data by introducing penalties or architectural constraints that favour simpler solutions. Common approaches include L1 and L2 weight penalties, dropout, early stopping, and data augmentation. Product teams see regularization as a lever for keeping performance stable after launch, while engineers adjust penalties to balance accuracy with latency and reproducibility. Governance and policy partners rely on documented regularization strategies to evidence that the organisation actively manages model complexity and drift risks. Without regularization, models often exhibit inflated offline metrics that fail to generalise to new markets, languages, or customer segments.

## Audience perspectives
- **Exec:** Regularization is how we stop models from overfitting so they keep working on new data.
- **Engineer:** Apply penalties such as lambda * ||w||^2, dropout masks, or early stopping schedules to manage bias-variance trade-offs.

## Examples
**Do**
- Log regularization settings alongside other hyperparameters for every experiment.
- Review fairness and calibration metrics after tightening or relaxing penalties.

**Don't**
- Disable regularization in production experiments without compensating monitoring.
- Rely on default penalty strengths when data distributions or architectures change.

## Governance
- **NIST RMF tags:** robustness, governance
- **Risk notes:** Lack of regularization increases drift and fairness risks, triggering costly remediation when models are audited.

## Relationships
- **Related:** gradient descent, loss function, overfitting

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'regularization'.

## Citations
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [DeepLearning.AI Resources](https://www.deeplearning.ai/resources/)
- [Google ML Crash Course – Regularization](https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/video-lecture)

_License: CC BY-SA 4.0_

_Source file: `data/terms/regularization.yml`_
