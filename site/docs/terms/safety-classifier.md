<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# safety classifier

**Aliases:** safety filter, policy classifier
**Categories:** Governance & Risk
**Roles:** Security & Trust, Policy & Risk, Product & Program Managers, Engineering & Platform
**Part of speech:** `noun_phrase`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-10)

## Role takeaways
- **Security & Trust:** Plan monitoring and abuse prevention scenarios influenced by this term.
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Engineering & Platform:** Document implementation requirements and operational caveats.

## Practice & apply
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Model that detects policy-violating or risky content before or after generation.

## Long definition
A safety classifier screens prompts and outputs to catch disallowed topics such as self-harm, extremism, or personal data. Classifiers can run pre-generation (blocking unsafe inputs), post-generation (filtering responses), or both. They complement prompt-based guardrails, providing a measurable signal that policies are enforced. Security and policy teams curate labeled datasets, engineering integrates the classifier into pipelines, and product owners tune messaging when content is blocked. Mature teams monitor precision/recall, recalibrate thresholds, and pair classifiers with human review for high-severity categories. Classifiers require ongoing evaluation to avoid overblocking legitimate use or missing novel abuse patterns.

## Audience perspectives
- **Exec:** Treat safety classifiers as a control with metrics, owners, and effectiveness reviews.
- **Engineer:** Log classifier scores, decisions, and overrides for auditing and incident analysis.

## Examples
**Do**
- Version classifier thresholds and align them with published policy requirements.
- Run adversarial tests to catch bypasses introduced by new jailbreaks.

**Don't**
- Rely solely on classifiers without human review for high-risk categories.
- Deploy classifiers trained on outdated policy definitions.

## Governance
- **NIST RMF tags:** monitoring, risk_management, security
- **Risk notes:** Poorly tuned classifiers erode trust—either by letting harmful content through or by overblocking.

## Relationships
- **Broader:** guardrails
- **Related:** jailbreak prompt, prompt injection, robust prompting

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'safety classifier'.

## Citations
- [OpenAI – Moderation Tools](https://learn.microsoft.com/en-us/azure/ai-foundry/responsible-ai/openai/overview)
- [Google Cloud – Content Safety Tools](https://cloud.google.com/blog/products/ai-machine-learning/responsible-ai-toolkits-content-safety)

_License: CC BY-SA 4.0_

_Source file: `data/terms/safety-classifier.yml`_
