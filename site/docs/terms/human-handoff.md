<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# human handoff

**Aliases:** agent-to-human handoff, human-in-the-loop handoff
**Categories:** Agents & Tooling
**Roles:** Product & Program Managers, Communications & Enablement, Engineering & Platform, Policy & Risk
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-09-29)

## Role takeaways
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Communications & Enablement:** Align messaging, FAQs, and enablement materials using this definition.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Policy & Risk:** Map the definition to governance controls and review checklists.

## Practice & apply
- Audit exposed tools against the safeguards described and document approval paths.
- Test hand-offs with human reviewers to confirm the safety expectations captured here are met.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Moment when an AI workflow transfers control to a human for review or action.

## Long definition
A human handoff occurs when an AI agent or automation pauses and routes context to a person for judgment, approval, or next steps. Effective handoffs bundle conversation history, risk signals, and recommended actions so humans can respond quickly. Product and support teams design the experience, engineering ensures state is preserved across channels, and policy leaders define which scenarios must escalate to people. Handoffs should capture audit trails, notify owners, and allow the human to resume the AI-assisted workflow. Poorly designed handoffs create dead ends, slow response times, or leave users confused about who's in control.

## Audience perspectives
- **Exec:** Guarantee there is a clear path to human help in sensitive journeys to maintain trust.
- **Engineer:** Package context, risk scores, and next best actions so reviewers can resolve the handoff quickly.

## Examples
**Do**
- Route escalations to a staffed queue with SLAs and full conversation summaries.
- Allow humans to annotate the outcome so the agent can learn from future cases.

**Don't**
- Drop the user into a blank chat with no explanation of what the agent attempted.
- Overlook accessibility needs when transferring to human support channels.

## Governance
- **NIST RMF tags:** governance, monitoring, risk_management
- **Risk notes:** Weak handoffs undermine accountability and can leave high-risk cases unresolved.

## Relationships
- **Broader:** agent executor, escalation policy
- **Related:** guardrail policy, ai incident response, impact mitigation plan

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'human handoff'.

## Citations
- [LangChain Glossary – Human-in-the-loop](https://api.python.langchain.com/en/latest/langchain/callbacks.html)
- [Microsoft – Prompt Engineering Guidance](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/prompt-engineering)

_License: CC BY-SA 4.0_

_Source file: `data/terms/human-handoff.yml`_
