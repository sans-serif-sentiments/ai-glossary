<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# guardrails

**Aliases:** safety guardrails, policy guardrails
**Categories:** Governance & Risk
**Roles:** Communications & Enablement, Legal & Compliance, Policy & Risk, Product & Program Managers
**Part of speech:** `noun`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-10)

## Role takeaways
- **Communications & Enablement:** Align messaging, FAQs, and enablement materials using this definition.
- **Legal & Compliance:** Assess contractual and regulatory obligations tied to this term.
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Controls that constrain model behavior to comply with safety, legal, or brand requirements.

## Long definition
Guardrails combine policy, technical, and operational measures designed to keep AI systems within acceptable behavior. They can include pre- and post-processing filters, policy-informed prompts, classifier ensembles, or moderation APIs that block disallowed content before it reaches end users. Effective guardrail programs coordinate with legal and risk teams to encode organizational standards and regulatory obligations. Engineers integrate guardrails into the request pipeline, monitor their performance, and log interventions for audit trails. Product managers review guardrail coverage to understand trade-offs between user experience and safety friction. Governance stakeholders treat guardrails as living controls that require change management, testing, and documentation to demonstrate due diligence. When guardrails fail or drift, the resulting incidents can expose organizations to legal liability, reputational damage, or regulatory penalties.

## Audience perspectives
- **Exec:** Guardrails are the checks that keep the AI from saying or doing things that would put the company at risk.
- **Engineer:** Policy-aligned filters, prompts, and classifiers embedded in the inference stack to block or reshape unsafe outputs.

## Examples
**Do**
- Test guardrail coverage with red-team prompts whenever the base model or system prompt changes.

**Don't**
- Rely on a single moderation classifier without monitoring precision and recall across scenarios.

## Governance
- **NIST RMF tags:** accountability, privacy, robustness
- **Risk notes:** Outdated guardrails can miss harmful outputs or inadvertently censor legitimate content, creating compliance gaps.

## Relationships
- **Broader:** responsible AI
- **Narrower:** output filtering, safety prompt
- **Related:** system prompt, temperature, red teaming

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'guardrails'.

## Citations
- [NIST AI RMF Glossary](https://www.nist.gov/itl/ai-risk-management-framework)
- [Salesforce â€“ Building AI Guardrails](https://www.ibm.com/think/topics/ai-guardrails)
- [Stanford HAI Brief Definitions](https://hai.stanford.edu/news/brief-definitions)

_License: CC BY-SA 4.0_

_Source file: `data/terms/guardrails.yml`_
