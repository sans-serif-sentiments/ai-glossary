<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# model interpretability

**Aliases:** interpretability, explainability
**Categories:** Governance & Risk, Operations & Monitoring
**Roles:** Engineering & Platform, Policy & Risk, Legal & Compliance, Product & Program Managers
**Part of speech:** `concept`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-09)

!!! tip "Put it into practice"
    Sync with the [Governance Dashboard](../governance-dashboard.md) to capture explanation plans.

## Role takeaways
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Legal & Compliance:** Assess contractual and regulatory obligations tied to this term.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Instrument dashboards or alerts that reflect the metrics highlighted in this definition.
- Update incident response or on-call runbooks with the glossary's do/don't scenarios.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Ability to explain how a model arrives at its predictions in ways stakeholders understand.

## Long definition
Model interpretability encompasses methods and practices that reveal why an AI system produced a particular output. Techniques range from local explanations (SHAP, LIME, token attribution) to global summaries (feature importance, surrogate models) and inherently interpretable architectures. Interpretability supports debugging, fairness audits, regulatory compliance, and customer trust. Engineering teams integrate explanation tooling into evaluation pipelines, while policy and legal stakeholders determine the level of transparency required for different products or jurisdictions. Model interpretability should be paired with documentation, human review, and responsible communication to avoid overstating confidence or exposing sensitive features.

## Audience perspectives
- **Exec:** Interpretability lets us open the black box so customers, regulators, and teams know why decisions were made.
- **Engineer:** Use techniques like SHAP, integrated gradients, or counterfactuals to attribute predictions; log results for audits and debugging.

## Examples
**Do**
- Provide dashboards that show top contributing features for high-risk decisions.
- Validate explanations with subject-matter experts to ensure they make sense.

**Don't**
- Offer explanations that contradict model behavior or hide uncertainty.
- Release interpretability tooling without access controls when sensitive features are involved.

## Governance
- **NIST RMF tags:** transparency, accountability
- **Risk notes:** Lack of interpretability undermines legal defensibility and trust; inaccurate explanations can mislead stakeholders.

## Relationships
- **Broader:** responsible ai
- **Related:** model card, algorithmic bias, evaluation

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'model interpretability'.

## Citations
- [NIST AI RMF Glossary](https://www.nist.gov/itl/ai-risk-management-framework)
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Stanford HAI Brief Definitions](https://hai.stanford.edu/news/brief-definitions)

_License: CC BY-SA 4.0_

_Source file: `data/terms/model-interpretability.yml`_
