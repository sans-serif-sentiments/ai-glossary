<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# attention

**Aliases:** attention mechanism, self-attention
**Categories:** LLM Core
**Roles:** Data Science & Research, Engineering & Platform, Product & Program Managers
**Part of speech:** `noun`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-09)

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.
- Share findings with enablement so downstream teams understand model implications.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Technique enabling models to weight input tokens differently when producing each output.

## Long definition
Attention assigns dynamic importance scores to tokens so a model can focus on the most relevant parts of the sequence when generating or interpreting outputs. In transformer architectures, self-attention computes query, key, and value projections that interact through scaled dot products, allowing every token to attend to every other token in the same layer. Multi-head attention repeats this operation across parallel subspaces, capturing nuanced relationships such as syntax, long-range dependencies, and positional context. The mechanism replaced recurrent networks for many language and vision tasks by enabling parallel processing and rich contextual reasoning. Engineers diagnose quality issues by inspecting attention patterns, tuning head counts, or constraining context windows to manage memory. Governance teams monitor attention configurations because they influence explainabilityâ€”saliency maps and attribution methods often rely on attention weights to justify model decisions in regulated settings.

## Audience perspectives
- **Exec:** Attention is how the model decides which words or pixels matter most before answering.
- **Engineer:** QKV projections with softmax-normalized weights that let each token aggregate information from the entire sequence.

## Examples
**Do**
- Profile attention head usage to identify redundant heads before applying pruning or distillation.

**Don't**
- Assume longer context windows automatically improve answers without verifying attention saturation and memory usage.

## Governance
- **NIST RMF tags:** transparency, robustness
- **Risk notes:** Opaque attention patterns can hinder explainability obligations in regulated workflows.

## Relationships
- **Broader:** transformer
- **Narrower:** cross-attention, multi-head attention
- **Related:** context window, kv cache, token

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'attention'.

## Citations
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Hugging Face Glossary](https://huggingface.co/docs/transformers/en/glossary)
- [Wikipedia AI Glossary](https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence)

_License: CC BY-SA 4.0_

_Source file: `data/terms/attention.yml`_
