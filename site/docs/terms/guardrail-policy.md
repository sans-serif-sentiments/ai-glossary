<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# guardrail policy

**Aliases:** guardrail playbook, safety policy prompt
**Categories:** Governance & Risk
**Roles:** Policy & Risk, Product & Program Managers, Security & Trust, Engineering & Platform
**Part of speech:** `noun_phrase`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-09)

## Role takeaways
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Security & Trust:** Plan monitoring and abuse prevention scenarios influenced by this term.
- **Engineering & Platform:** Document implementation requirements and operational caveats.

## Practice & apply
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Documented rules and prompts that define allowed, blocked, and escalated behaviors for AI systems.

## Long definition
A guardrail policy spells out how an AI system should respond to risky scenarios, combining policy prompts, tool restrictions, monitoring hooks, and escalation paths. It anchors the guardrails implemented in code to clear governance expectations so product, safety, and engineering teams act in sync. The policy enumerates prohibited content, required disclosures, human handoff triggers, and review cadences, and maps each rule to enforcement controls in the stack. Security leaders reference it when auditing access, while product owners ensure the policy keeps user experience coherent. Without this shared artifact, teams risk shipping fragmented mitigations that fail investigative audits or leave gaps between policy intent and agent behavior.

## Audience perspectives
- **Exec:** Use the guardrail policy to prove you have enforceable boundaries before scaling sensitive features.
- **Engineer:** Translate each rule into prompts, filters, and monitoring alerts so violations are caught automatically.

## Examples
**Do**
- Version-control policy prompts alongside code so deployments document why changes were made.
- Define human approval checkpoints for financial or legal actions triggered by the agent.

**Don't**
- Rely on ad-hoc prompt tweaks without peer review or governance sign-off.
- Ship new tools without mapping them to prohibited-use clauses in the policy.

## Governance
- **NIST RMF tags:** governance, risk_management, accountability
- **Risk notes:** Missing or outdated guardrail policies make it impossible to demonstrate control effectiveness during audits.

## Relationships
- **Broader:** responsible ai, safety spec
- **Narrower:** escalation policy, system prompt
- **Related:** guardrails, ai incident response, red teaming

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'guardrail policy'.

## Citations
- [Constitutional AI (Anthropic)](https://arxiv.org/abs/2302.12173)
- [Microsoft â€“ Content Filter & Prompt Shields](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/content-filter-prompt-shields)

_License: CC BY-SA 4.0_

_Source file: `data/terms/guardrail-policy.yml`_
