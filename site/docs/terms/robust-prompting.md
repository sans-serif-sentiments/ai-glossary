<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# robust prompting

**Aliases:** defensive prompting, resilient prompting
**Categories:** LLM Core
**Roles:** Engineering & Platform, Product & Program Managers, Security & Trust
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-09)

## Role takeaways
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Security & Trust:** Plan monitoring and abuse prevention scenarios influenced by this term.

## Practice & apply
- Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.
- Share findings with enablement so downstream teams understand model implications.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Prompt design techniques that harden models against injections, ambiguity, and unsafe outputs.

## Long definition
Robust prompting combines structured instructions, guard clauses, and response checks to make language models more resilient to adversarial or ambiguous inputs. Teams use it to constrain topics, refuse unsafe requests, and preserve critical context even when users try to override instructions. Techniques include layered system messages, explicit refusal criteria, verification prompts, and output formatting templates that make policy violations easier to detect. Engineers pair robust prompts with automated tests, while product and security stakeholders review language to ensure safety requirements and user empathy co-exist. Without defensive prompts, even well-governed systems can be steered into disallowed actions by crafted jailbreaks or accidental misuse.

## Audience perspectives
- **Exec:** Invest in robust prompts to reduce incident volume before adding expensive human moderation.
- **Engineer:** Version prompts, test for bypasses, and log refusals so you can tune protections proactively.

## Examples
**Do**
- Include explicit refusal language and escalation guidance when requests break policy.
- Pair prompts with automated prompt-injection tests in CI.

**Don't**
- Rely on a single system message to enforce all safety requirements.
- Ship prompt changes without regression testing for jailbreak coverage.

## Governance
- **NIST RMF tags:** risk_management, safety, monitoring
- **Risk notes:** Weak prompts increase exposure to jailbreaks, data leakage, and brand-damaging outputs.

## Relationships
- **Broader:** prompt engineering
- **Related:** prompt injection, guardrail policy, self-critique loop

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'robust prompting'.

## Citations
- [Prompting Guide – Defensive Prompting](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/content-filter-prompt-shields)
- [arXiv – InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models](https://arxiv.org/abs/2410.22770)

_License: CC BY-SA 4.0_

_Source file: `data/terms/robust-prompting.yml`_
