<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# instruction tuning

**Aliases:** instruction fine-tuning, supervised fine-tuning
**Categories:** Optimization & Efficiency
**Roles:** Engineering & Platform, Data Science & Research, Product & Program Managers, Policy & Risk
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-09)

## Role takeaways
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Policy & Risk:** Map the definition to governance controls and review checklists.

## Practice & apply
- Record before-and-after performance metrics when applying this optimisation technique.
- Document trade-offs for product and policy partners using the glossary's language.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Supervised training that teaches models to follow natural-language instructions using curated examples.

## Long definition
Instruction tuning (often called supervised fine-tuning) adapts a pretrained model on datasets of prompts and ideal responses so it learns to follow instructions. Annotators craft demonstrations that reflect desired tone, safety, and reasoning patterns. Engineers run fine-tuning jobs, data scientists manage dataset quality, and policy teams ensure instructions encode governance requirements. The process is typically the first step before RLHF or other alignment work. Poorly curated instruction data can introduce bias, regress safety, or drift from product expectations, so teams monitor evaluation metrics and refresh the dataset as policies evolve.

## Audience perspectives
- **Exec:** Budget for recurring instruction tuning to keep models aligned with evolving strategy.
- **Engineer:** Version datasets, hyperparameters, and evaluation checkpoints for every tuning run.

## Examples
**Do**
- Label edge cases and refusal scenarios so the model learns safety boundaries.
- Combine instruction data with role-based tone guidelines for different audiences.

**Don't**
- Assume open-source instruction datasets cover your policy requirements.
- Overwrite the base model without comparing against prior alignment baselines.

## Governance
- **NIST RMF tags:** risk_management, measurement, accountability
- **Risk notes:** Uncontrolled tuning can undo prior safety work and confuse governance owners.

## Relationships
- **Broader:** fine-tuning
- **Related:** reinforcement learning from human feedback, reward model, robust prompting

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'instruction tuning'.

## Citations
- [OpenAI – InstructGPT](https://arxiv.org/abs/2203.02155)
- [Stanford – Self-Instruct](https://arxiv.org/abs/2212.10560)

_License: CC BY-SA 4.0_

_Source file: `data/terms/instruction-tuning.yml`_
