<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# retrieval

**Aliases:** information retrieval, retriever
**Categories:** Retrieval & RAG
**Roles:** Communications & Enablement, Data Science & Research, Product & Program Managers, Security & Trust, Engineering & Platform, Policy & Risk
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-09)

## Role takeaways
- **Communications & Enablement:** Align messaging, FAQs, and enablement materials using this definition.
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Security & Trust:** Plan monitoring and abuse prevention scenarios influenced by this term.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Policy & Risk:** Map the definition to governance controls and review checklists.

## Practice & apply
- Validate retrieval quality using the evaluation guidance referenced in this entry.
- Ensure knowledge sources named here appear in your data governance inventory.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Process of selecting relevant documents or vectors from a corpus in response to a query.

## Long definition
Retrieval refers to the algorithms and infrastructure that locate the most relevant pieces of information when a user submits a query. Classical approaches use lexical signals like TF-IDF and BM25, while modern systems incorporate dense vector similarity, hybrid combinations, and reranking models tailored to the domain. Retrieval quality underpins search, question answering, and retrieval-augmented generation workflows: without high recall and precision, downstream models hallucinate or return stale policy guidance. Engineers design retrievers by choosing chunking strategies, embedding models, index types, and freshness policies. They also monitor latency, relevance metrics, and guardrails such as filtering sensitive content. Governance teams treat retrieval pipelines as data processors, verifying access controls, audit logging, and compliance with data minimization requirements. Strong retrieval discipline enables traceability by linking generated outputs back to cited sources.

## Audience perspectives
- **Exec:** Retrieval is the step that fetches trusted facts before the AI speaks, keeping answers grounded.
- **Engineer:** Implements query encoding, similarity search, and ranking to surface high-relevance passages for downstream consumers.

## Examples
**Do**
- Track recall@k and precision metrics separately to diagnose retrieval bottlenecks.

**Don't**
- Index sensitive documents without permissioning or automated redaction.

## Governance
- **NIST RMF tags:** data_quality, transparency
- **Risk notes:** Weak retrieval controls can expose restricted records or feed outdated content into regulated workflows.

## Relationships
- **Broader:** search
- **Narrower:** dense retrieval, lexical retrieval
- **Related:** retrieval-augmented generation, vector store, reranking

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'retrieval'.

## Citations
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Hugging Face Glossary](https://huggingface.co/docs/transformers/en/glossary)
- [NIST AI RMF Glossary](https://www.nist.gov/itl/ai-risk-management-framework)

_License: CC BY-SA 4.0_

_Source file: `data/terms/retrieval.yml`_
