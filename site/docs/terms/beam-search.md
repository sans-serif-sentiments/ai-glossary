<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# beam search

**Aliases:** beam decoding, multi-path decoding
**Categories:** LLM Core
**Roles:** Data Science & Research, Engineering & Platform, Product & Program Managers
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-10)

!!! tip "Put it into practice"
    Pair with the [Prompt Engineering Playbook](../prompting.md) when crafting deterministic flows.

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.
- Share findings with enablement so downstream teams understand model implications.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Deterministic decoding that keeps the top scoring sequences across multiple beams before selecting the final output.

## Long definition
Beam search expands several candidate continuations in parallel, keeping only the highest-scoring sequences at each step according to their cumulative log probabilities. By exploring multiple beams instead of a single greedy path, the method can uncover higher-quality completions and reduce the chance of getting stuck in locally optimal—but globally poor—answers. Teams tune beam width and length penalties to balance diversity against compute cost, because wider beams demand more memory and latency. Product workflows that require structured or citation-heavy responses often pair beam search with reranking or guardrails to ensure final selections remain compliant. Engineers also log intermediate beams to debug why a response was chosen and to audit near-miss alternatives. Governance stakeholders review beam configurations in high-stakes deployments to confirm that deterministic choices align with evaluation baselines and do not inadvertently suppress required disclosures.

## Audience perspectives
- **Exec:** Beam search is a quality knob—explore a few strong answer paths before picking the best one.
- **Engineer:** Maintain N top-scoring sequences using cumulative log probabilities; apply length penalties and reranking before selecting the final candidate.

## Examples
**Do**
- Monitor latency impact when increasing beam width beyond 4 to ensure SLAs still hold.
- Analyze discarded beams during incident reviews to understand alternative outputs.

**Don't**
- Ship beam search without configuring length penalties, which can bias toward verbose responses.
- Use the same beam width across all locales without measuring cost and accuracy trade-offs.

## Governance
- **NIST RMF tags:** robustness, transparency
- **Risk notes:** Misconfigured beams can surface repetitive or off-policy content; document parameters and align them with evaluation coverage.

## Relationships
- **Broader:** decoding
- **Related:** greedy decoding, top-k sampling, log probability

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'beam search'.

## Citations
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Hugging Face Glossary](https://huggingface.co/docs/transformers/en/glossary)
- [Wikipedia AI Glossary](https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence)

_License: CC BY-SA 4.0_

_Source file: `data/terms/beam-search.yml`_
