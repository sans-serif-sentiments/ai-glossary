<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# preference dataset

**Aliases:** preference data, human feedback dataset
**Categories:** LLM Core
**Roles:** Data Science & Research, Policy & Risk, Product & Program Managers, Engineering & Platform
**Part of speech:** `noun_phrase`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-10)

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Engineering & Platform:** Document implementation requirements and operational caveats.

## Practice & apply
- Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.
- Share findings with enablement so downstream teams understand model implications.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Labeled comparisons of model outputs that capture which responses humans prefer.

## Long definition
A preference dataset contains prompts, model outputs, and human annotations indicating which output better satisfies instructions or policies. Teams use it to train reward models or perform direct preference optimization. Gathering preference data requires clear labeling rubrics, diverse annotators, and safeguards for sensitive content. Policy teams define decision criteria, product teams ensure tone and UX requirements are reflected, and engineers manage secure tooling for annotation. Poorly managed preference data can leak personal information or encode unintended bias that later influences RLHF or reranking systems.

## Audience perspectives
- **Exec:** Protect preference data like any user research asset—it shapes model behavior and compliance.
- **Engineer:** Track lineage from annotation tools to training pipelines and enforce access controls.

## Examples
**Do**
- Capture rationale alongside preferences so reviewers understand annotations.
- Audit samples regularly for bias or policy drift.

**Don't**
- Mix production user data into preference datasets without consent.
- Allow annotators to work without updated safety guidelines.

## Governance
- **NIST RMF tags:** accountability, risk_management, privacy
- **Risk notes:** Lax controls can leak sensitive prompts or entrench biased judgments into downstream models.

## Relationships
- **Broader:** reinforcement learning from human feedback
- **Related:** reward model, instruction tuning, risk register

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'preference dataset'.

## Citations
- [OpenAI – InstructGPT](https://arxiv.org/abs/2203.02155)
- [Microsoft – Responsible AI Principles](https://www.microsoft.com/en-us/ai/principles-and-approach)

_License: CC BY-SA 4.0_

_Source file: `data/terms/preference-dataset.yml`_
