<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# alignment

**Aliases:** AI alignment, value alignment
**Categories:** Governance & Risk
**Roles:** Communications & Enablement, Legal & Compliance, Policy & Risk, Product & Program Managers
**Part of speech:** `concept`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-09)

## Role takeaways
- **Communications & Enablement:** Align messaging, FAQs, and enablement materials using this definition.
- **Legal & Compliance:** Assess contractual and regulatory obligations tied to this term.
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Making sure AI systems optimize for human values, policies, and intended outcomes.

## Long definition
Alignment is the multidisciplinary effort to design AI systems whose goals, behaviors, and outputs remain consistent with human intent and societal norms. It spans technical research—such as reward modeling, constitutional AI, interpretability, and adversarial training—and organizational governance, including policy frameworks, oversight committees, and escalation paths. Alignment work acknowledges that models learn from imperfect data and may pursue proxy objectives that conflict with human priorities. Product leaders use alignment roadmaps to decide which features require human-in-the-loop review, while engineers translate alignment goals into metrics, eval harnesses, and guardrails. Regulators and standards bodies, including NIST and ISO, emphasize alignment as part of trustworthy AI, requiring documentation of assumptions, residual risks, and impact mitigation strategies. Sustainable alignment programs treat it as an ongoing lifecycle activity rather than a one-time tuning exercise.

## Audience perspectives
- **Exec:** Alignment is how we make sure the AI keeps serving our mission and values as it evolves.
- **Engineer:** Research and governance toolkit ensuring loss functions, feedback loops, and guardrails drive behavior toward intended objectives.

## Examples
**Do**
- Document alignment hypotheses and track eval metrics tied to specific risk scenarios.

**Don't**
- Assume alignment is solved after one fine-tuning pass without continuous monitoring.

## Governance
- **NIST RMF tags:** accountability, transparency, validity
- **Risk notes:** Weak alignment programs allow models to pursue proxy goals that conflict with legal or ethical obligations.

## Relationships
- **Broader:** responsible AI
- **Narrower:** constitutional AI, reinforcement learning from human feedback
- **Related:** guardrails, evaluation, red teaming

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'alignment'.

## Citations
- [Wikipedia – AI Alignment](https://en.wikipedia.org/wiki/AI_alignment)
- [OpenAI – Learning from Human Preferences](https://arxiv.org/abs/1706.03741)
- [Microsoft – Responsible AI Principles](https://www.microsoft.com/en-us/ai/principles-and-approach)

_License: CC BY-SA 4.0_

_Source file: `data/terms/alignment.yml`_
