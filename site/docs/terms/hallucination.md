<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# hallucination

**Aliases:** AI hallucination, confabulation
**Categories:** LLM Core, Governance & Risk
**Roles:** Communications & Enablement, Data Science & Research, Engineering & Platform, Legal & Compliance, Policy & Risk, Product & Program Managers
**Part of speech:** `noun`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-09)

## Role takeaways
- **Communications & Enablement:** Align messaging, FAQs, and enablement materials using this definition.
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Legal & Compliance:** Assess contractual and regulatory obligations tied to this term.
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.
- Share findings with enablement so downstream teams understand model implications.
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
When an AI model presents fabricated or unsupported information as fact.

## Long definition
Hallucination describes the tendency of generative models to deliver content that sounds plausible but is either factually incorrect, logically inconsistent, or entirely invented. The phenomenon stems from the probabilistic way large language models predict the next token based on training data patterns rather than grounded knowledge of the world. It can occur when prompts lack sufficient context, when the model has not seen relevant examples during training, or when decoding strategies over-index on fluency instead of accuracy. Product teams experience hallucination as broken user trust, while engineers may notice it during evaluation as high lexical overlap paired with low factual precision. Mitigations range from retrieval augmentation and prompt constraints to post-generation fact checking, human review, and model fine-tuning on verified corpora. Organizations must treat hallucination as both a quality and a risk management issue, particularly in regulated or safety-critical workflows.

## Audience perspectives
- **Exec:** Signals that the model is making things up, which erodes user trust and can trigger compliance issues.
- **Engineer:** Indicates the model sampled a high-probability sequence lacking factual grounding; investigate context, decoding, and eval signals.

## Examples
**Do**
- Log hallucination incidents and route high-severity cases to human review for remediation.
- Use retrieval augmentation or tool grounding to supply verifiable context before generation.

**Don't**
- Deploy long-form responses without monitoring factual accuracy or adding disclaimers.
- Assume higher model size alone will eliminate hallucination without evaluation improvements.

## Governance
- **NIST RMF tags:** accuracy, transparency, validity
- **Risk notes:** Unaddressed hallucinations can produce misleading outputs that violate accuracy commitments and create legal exposure.

## Relationships
- **Broader:** generative AI
- **Narrower:** factual hallucination, formal hallucination
- **Related:** retrieval-augmented generation, guardrails, evaluation

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'hallucination'.

## Citations
- [Hugging Face Glossary](https://huggingface.co/docs/transformers/en/glossary)
- [NIST AI RMF Glossary](https://www.nist.gov/itl/ai-risk-management-framework)
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)

_License: CC BY-SA 4.0_

_Source file: `data/terms/hallucination.yml`_
