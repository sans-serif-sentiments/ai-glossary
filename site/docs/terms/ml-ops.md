<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# ml ops

**Aliases:** mlops, machine learning operations
**Categories:** Operations & Monitoring
**Roles:** Engineering & Platform, Policy & Risk, Security & Trust
**Part of speech:** `concept`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-10)

## Role takeaways
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Security & Trust:** Plan monitoring and abuse prevention scenarios influenced by this term.

## Practice & apply
- Instrument dashboards or alerts that reflect the metrics highlighted in this definition.
- Update incident response or on-call runbooks with the glossary's do/don't scenarios.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Operational discipline that manages ML models from experimentation through deployment and monitoring.

## Long definition
ML Ops adapts DevOps and data engineering practices to the lifecycle of machine learning systems. It encompasses experiment tracking, feature management, deployment automation, monitoring, incident response, and governance. Successful ML Ops programs unify teams across data science, engineering, and compliance so models move from prototype to production without manual heroics. Tooling includes model registries, CI/CD pipelines, automated evaluation gates, and observability platforms that watch for drift, data quality issues, and guardrail violations. Product organizations rely on ML Ops maturity to ship updates safely and respond quickly to incidents. Governance frameworks such as the NIST AI RMF expect documented ML Ops processes to demonstrate accountability, transparency, and continuous monitoring. Without disciplined ML Ops, models stagnate, degrade in accuracy, or fall out of compliance as the environment evolves.

## Audience perspectives
- **Exec:** ML Ops is the operating system that keeps production models reliable, compliant, and cost-efficient.
- **Engineer:** Integrate version control, CI/CD, registries, approvals, and monitoring to manage ML artifacts end to end.

## Examples
**Do**
- Automate promotion from staging to production only after evaluation and guardrail checks pass.

**Don't**
- Deploy model updates outside the pipeline, bypassing logging and rollback controls.

## Governance
- **NIST RMF tags:** accountability, monitoring
- **Risk notes:** Weak ML Ops processes increase the odds of untracked changes, bias reintroductions, and compliance failures.

## Relationships
- **Broader:** model governance
- **Narrower:** ml observability, model drift
- **Related:** fine-tuning, evaluation, guardrails

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'ml ops'.

## Citations
- [NIST AI RMF Glossary](https://www.nist.gov/itl/ai-risk-management-framework)
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Stanford HAI Brief Definitions](https://hai.stanford.edu/news/brief-definitions)

_License: CC BY-SA 4.0_

_Source file: `data/terms/ml-ops.yml`_
