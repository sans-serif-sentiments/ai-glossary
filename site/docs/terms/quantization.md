<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# quantization

**Aliases:** model quantization, weight quantization
**Categories:** Optimization & Efficiency
**Roles:** Data Science & Research, Engineering & Platform
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-10)

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.

## Practice & apply
- Record before-and-after performance metrics when applying this optimisation technique.
- Document trade-offs for product and policy partners using the glossary's language.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Technique that compresses model weights into lower-precision formats to shrink size and speed inference.

## Long definition
Quantization converts neural network parameters and activations from high-precision floating point representations (such as FP32) into lower bit-width formats (such as INT8 or INT4) to reduce memory footprint and accelerate inference. By mapping continuous values into a discrete set, quantization enables models to run on cost-sensitive hardware, deliver faster responses, and consume less energy, which is critical when deploying large language models at scale or on edge devices. Engineers choose between post-training quantization, which calibrates a frozen model on representative data, and quantization-aware training, which simulates low-precision behavior during fine-tuning to preserve accuracy. Careful evaluation is required to understand the trade-offs: aggressive quantization can introduce numerical instability, harm latency determinism, or amplify bias if calibration data under-represents certain groups. Successful programs pair quantization with monitoring, backstops such as higher-precision fallbacks, and documentation that makes these trade-offs explicit to stakeholders.

## Audience perspectives
- **Exec:** A cost-control lever: shrink model footprints so you can serve more traffic on existing hardware.
- **Engineer:** Apply per-tensor or per-channel scaling, choose symmetric/asymmetric schemes, and validate perplexity and latency post-quantization.

## Examples
**Do**
- Benchmark accuracy and latency before and after quantization to document the trade-offs.
- Use representative calibration datasets that include edge cases and demographic variation.

**Don't**
- Quantize safety-critical models without fallback paths or runtime monitoring.
- Assume INT4 settings will work across architectures without profiling.

## Governance
- **NIST RMF tags:** efficiency, robustness, documentation
- **Risk notes:** Quantization can reduce accuracy or shift error distribution; record evaluations and obtain stakeholder sign-off.

## Relationships
- **Broader:** model optimization
- **Narrower:** post-training quantization, quantization-aware training
- **Related:** compression, distillation, hardware acceleration

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'quantization'.

## Citations
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Hugging Face Glossary](https://huggingface.co/docs/transformers/en/glossary)
- [Stanford HAI Brief Definitions](https://hai.stanford.edu/news/brief-definitions)

_License: CC BY-SA 4.0_

_Source file: `data/terms/quantization.yml`_
