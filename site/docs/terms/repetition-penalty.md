<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# repetition penalty

**Aliases:** anti-repetition penalty, token penalty
**Categories:** LLM Core
**Roles:** Data Science & Research, Engineering & Platform, Product & Program Managers
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-10)

!!! tip "Put it into practice"
    Use the [Prompt Engineering Playbook](../prompting.md) to balance repetition controls.

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.
- Share findings with enablement so downstream teams understand model implications.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Decoding adjustment that down-weights tokens already generated to reduce loops and repeated phrases.

## Long definition
A repetition penalty rescales token probabilities during decoding so words that have already appeared become less likely to repeat. Implementations typically divide or multiply logits by a penalty factor greater than one, discouraging the model from reusing identical phrases while preserving the rest of the distribution. Product teams enable repetition penalties to prevent user-facing chatbots from producing redundant or endless loops, particularly in multilingual or code-heavy contexts where repetition can spike. Engineers tune separate penalties for input prompts versus generated output, and combine them with stop sequences to honour formatting requirements. Governance stakeholders log penalty settings because altering them can invalidate safety and quality evaluations—lowering the penalty risks repetitive harmful content, whereas an excessively high penalty may distort meaning or remove essential disclosures. Monitoring repetition metrics in production helps confirm the chosen value remains effective as models, prompts, or content domains evolve.

## Audience perspectives
- **Exec:** Think of the repetition penalty as the guardrail that keeps conversations from getting stuck in loops.
- **Engineer:** Scale logits for previously used tokens by a factor (e.g., 1.1–1.2) before sampling to discourage repeats without breaking coherence.

## Examples
**Do**
- Track repetition rate metrics alongside hallucination incidents after changing penalty values.
- Differentiate penalties for system prompts versus user-visible responses.

**Don't**
- Set the penalty so high that critical disclaimers or citations are removed from answers.
- Forget to document penalty changes when comparing evaluation runs.

## Governance
- **NIST RMF tags:** robustness, transparency
- **Risk notes:** Aggressive penalties can alter validated answer formats; coordinate with policy and QA before rollout.

## Relationships
- **Broader:** decoding
- **Related:** temperature, top-p sampling, beam search

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'repetition penalty'.

## Citations
- [Hugging Face Glossary](https://huggingface.co/docs/transformers/en/glossary)
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Stanford HAI Brief Definitions](https://hai.stanford.edu/news/brief-definitions)

_License: CC BY-SA 4.0_

_Source file: `data/terms/repetition-penalty.yml`_
