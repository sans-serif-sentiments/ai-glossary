<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# prompt injection

**Aliases:** prompt attack, context hijacking
**Categories:** Governance & Risk
**Roles:** Security & Trust, Engineering & Platform, Product & Program Managers
**Part of speech:** `noun_phrase`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-10-09)

## Role takeaways
- **Security & Trust:** Plan monitoring and abuse prevention scenarios influenced by this term.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Attack that inserts malicious instructions into model inputs to override original prompts or policies.

## Long definition
Prompt injection is an adversarial technique where attackers craft inputs that smuggle new instructions into a model's context. The injected text attempts to override safety policies, steal secrets, or trigger unintended actions, such as exfiltrating data from tools connected to the agent. The attack can appear in user text, retrieved documents, or API responses that the model reads. Security teams treat prompt injection like command injection: sanitizing inputs, isolating tools, and limiting model permissions. Product teams communicate when defensive refusals occur, and engineers add filters plus evaluation harnesses that test for jailbreaks. Without mitigations, attackers can bypass guardrails or weaponize the model to call sensitive tools.

## Audience perspectives
- **Exec:** Track prompt injection risks the same way you monitor phishing and social engineering threats.
- **Engineer:** Sanitize retrieved context, restrict tool scopes, and simulate attacks continuously.

## Examples
**Do**
- Strip or quarantine untrusted instructions before feeding documents into the agent.
- Log and share injection attempts with the security incident response team.

**Don't**
- Allow the model to treat external data sources as authoritative without validation.
- Expose long-lived secrets or admin tools to untrusted prompts.

## Governance
- **NIST RMF tags:** security, risk_management, monitoring
- **Risk notes:** Successful injections can leak data, perform unauthorized actions, or undermine governance commitments.

## Relationships
- **Broader:** guardrails, robust prompting
- **Related:** jailbreak prompt, tool use, ai incident response

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'prompt injection'.

## Citations
- [OWASP – LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [arXiv – Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/abs/2302.12173)
- [Prompt Injection Attacks and Defenses](https://arxiv.org/abs/2307.15043)

_License: CC BY-SA 4.0_

_Source file: `data/terms/prompt-injection.yml`_
