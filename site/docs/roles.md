<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# Role Starter Packs

Guidance for common stakeholder groups. Each pack includes actionable steps and key focus areas so teams can operationalize insights immediately.

## Product & Program Managers
Focus on user outcomes, feature scope, and launch readiness.

**Action plan**
- Bookmark the [Glossary Search](search.md) filtered to this role and review the top 5 unfamiliar terms.
- Schedule a sync with partner roles listed under each term to clarify ownership and open questions.
- Capture insights in your runbook or onboarding guide so future teammates ramp faster.

### Guided learning path
1. Skim the Governance & Risk category to learn which terms drive launch checklists.
2. Bookmark three model or prompt concepts that influence roadmap trade-offs.
3. Schedule a debrief with policy partners to align on escalation triggers.

### Practice checklist
- Review the glossary search filtered to product + governance and log two takeaways in your launch checklist.
- Pair with engineering to confirm which guardrails or prompts need updates before feature freeze.

### Focus areas
- Governance & Risk (38 terms)
- LLM Core (24 terms)
- Foundations (21 terms)
- Operations & Monitoring (16 terms)
- Agents & Tooling (8 terms)
- Retrieval & RAG (7 terms)
- Optimization & Efficiency (2 terms)

### Recommended terms
- [activation function](terms/activation-function.md) — Mathematical transformation applied to a neuron’s weighted sum that lets neural networks model nonlinear relationships and control signal range.
- [agent executor](terms/agent-executor.md) — Controller layer that schedules planning, tool calls, and stop conditions so an AI agent completes tasks safely.
- [agentic ai](terms/agentic-ai.md) — Systems that plan, act, and iterate with minimal human prompts by chaining model calls and tools.
- [ai assurance](terms/ai-assurance.md) — Discipline that produces evidence, controls, and attestations showing an AI system meets agreed safety, compliance, and performance thresholds.
- [ai circuit breaker](terms/ai-circuit-breaker.md) — Automated control that halts model responses or tool access when risk thresholds are exceeded.
- [ai incident response](terms/ai-incident-response.md) — Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.
- [algorithmic audit](terms/algorithmic-audit.md) — Independent review of an AI system’s data, design, and outcomes to verify compliance, fairness, and risk controls.
- [algorithmic bias](terms/algorithmic-bias.md) — Systematic unfairness in model outputs that disadvantages certain groups or outcomes.
- [algorithmic impact assessment](terms/algorithmic-impact-assessment.md) — Structured review that documents how an AI system may affect people, processes, and compliance obligations.
- [alignment](terms/alignment.md) — Making sure AI systems optimize for human values, policies, and intended outcomes.
- [attention](terms/attention.md) — Technique enabling models to weight input tokens differently when producing each output.
- [beam search](terms/beam-search.md) — Deterministic decoding that keeps the top scoring sequences across multiple beams before selecting the final output.
- [chain-of-thought prompting](terms/chain-of-thought-prompting.md) — Prompting technique that asks models to reason through intermediate steps before giving a final answer.
- [chunking](terms/chunking.md) — Splitting source documents into manageable pieces before indexing or feeding them to models.
- [clip](terms/clip.md) — Multimodal model that embeds images and text into a shared space using contrastive learning.
- [confusion matrix](terms/confusion-matrix.md) — Table that summarizes true/false positives and negatives to diagnose classification performance.
- [consent management](terms/consent-management.md) — Practices that capture, honor, and audit user permissions across AI features.
- [constitutional ai](terms/constitutional-ai.md) — Alignment approach where models critique and revise their own outputs against a written set of principles.
- [content moderation](terms/content-moderation.md) — Workflows and tools that review, filter, and act on user-generated content to enforce policy.
- [context window](terms/context-window.md) — Maximum number of tokens a model can consider at once during prompting or inference.
- [cross-validation](terms/cross-validation.md) — Evaluation technique that splits data into multiple folds to estimate model performance on unseen samples.
- [data minimization](terms/data-minimization.md) — Principle of collecting and retaining only the data necessary for a defined purpose.
- [data retention](terms/data-retention.md) — Policies defining how long data is stored, where it lives, and how it is deleted.
- [dataset card](terms/dataset-card.md) — Structured documentation describing a dataset’s purpose, composition, risks, and usage constraints.
- [decoding](terms/decoding.md) — Algorithms that turn model probability distributions into output tokens during generation.
- [diffusion model](terms/diffusion-model.md) — Generative model that iteratively denoises random noise to synthesize images, audio, or other data.
- [direct preference optimization](terms/direct-preference-optimization.md) — Alignment technique that fine-tunes models directly on preference data without training a separate reward model.
- [embedding](terms/embedding.md) — Dense numerical representation that captures semantic meaning of text, images, or other data.
- [escalation policy](terms/escalation-policy.md) — Playbook that defines when and how AI systems route control to human reviewers.
- [evaluation](terms/evaluation.md) — Systematic measurement of model performance, safety, and reliability using defined tests.
- [evaluation harness](terms/evaluation-harness.md) — Automated pipeline that replays tasks, scores outputs, and reports regressions for AI systems.
- [f1 score](terms/f1-score.md) — Harmonic mean of precision and recall, balancing false positives and false negatives.
- [fairness metrics](terms/fairness-metrics.md) — Quantitative measures that evaluate whether model performance is equitable across groups.
- [feature engineering](terms/feature-engineering.md) — Transforming raw data into model-ready features that improve signal, fairness, and maintainability.
- [function calling](terms/function-calling.md) — LLM capability that lets prompts invoke predefined functions and return structured arguments.
- [generalization](terms/generalization.md) — Model's ability to sustain performance on unseen data rather than memorising the training set.
- [generative ai](terms/generative-ai.md) — Family of models that produce new content—text, images, code—rather than only making predictions.
- [gradient descent](terms/gradient-descent.md) — Iterative optimization algorithm that updates model parameters in the direction of the negative gradient to minimize a loss function.
- [greedy decoding](terms/greedy-decoding.md) — Strategy that selects the highest-probability token at each step, producing deterministic outputs.
- [guardrail policy](terms/guardrail-policy.md) — Documented rules and prompts that define allowed, blocked, and escalated behaviors for AI systems.
- [guardrails](terms/guardrails.md) — Controls that constrain model behavior to comply with safety, legal, or brand requirements.
- [hallucination](terms/hallucination.md) — When an AI model presents fabricated or unsupported information as fact.
- [human handoff](terms/human-handoff.md) — Moment when an AI workflow transfers control to a human for review or action.
- [impact mitigation plan](terms/impact-mitigation-plan.md) — Action plan that tracks risks, mitigations, owners, and timelines for an AI deployment.
- [incident taxonomy](terms/incident-taxonomy.md) — Standardized categories used to tag, analyze, and report AI incidents consistently.
- [instruction tuning](terms/instruction-tuning.md) — Supervised training that teaches models to follow natural-language instructions using curated examples.
- [jailbreak prompt](terms/jailbreak-prompt.md) — Crafted input that persuades a model to ignore safety instructions and produce disallowed responses.
- [kv cache](terms/kv-cache.md) — Stored attention keys and values reused across decoding steps to speed sequential generation.
- [log probability](terms/log-probability.md) — Logarithm of a token’s probability, used to inspect model confidence and guide decoding tweaks.
- [loss function](terms/loss-function.md) — Mathematical rule that scores how far model predictions deviate from desired targets.
- [memory strategy](terms/memory-strategy.md) — Deliberate approach for when an AI agent stores, retrieves, or forgets context across tasks.
- [mixture of experts](terms/mixture-of-experts.md) — Neural architecture that routes tokens to specialized submodels to scale capacity efficiently.
- [model card](terms/model-card.md) — Standardized documentation describing a model’s purpose, data, performance, and limitations.
- [model drift](terms/model-drift.md) — Gradual mismatch between model assumptions and real-world data that degrades performance over time.
- [model governance](terms/model-governance.md) — Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.
- [model interpretability](terms/model-interpretability.md) — Ability to explain how a model arrives at its predictions in ways stakeholders understand.
- [overfitting](terms/overfitting.md) — When a model memorizes training data patterns so closely that it performs poorly on new samples.
- [planner executor pattern](terms/planner-executor-pattern.md) — Agent design where one component plans multi-step work and another executes steps while feeding results back for re-planning.
- [precision](terms/precision.md) — Share of predicted positives that are actually correct for a given classifier.
- [preference dataset](terms/preference-dataset.md) — Labeled comparisons of model outputs that capture which responses humans prefer.
- [privacy](terms/privacy.md) — Principle of limiting data collection, use, and exposure to protect individuals’ information.
- [privacy impact assessment](terms/privacy-impact-assessment.md) — Structured review that evaluates how a system collects, uses, and safeguards personal data.
- [prompt engineering](terms/prompt-engineering.md) — Crafting and testing prompts to steer model behavior toward desired outcomes.
- [prompt injection](terms/prompt-injection.md) — Attack that inserts malicious instructions into model inputs to override original prompts or policies.
- [prompt registry](terms/prompt-registry.md) — Central system that versions, reviews, and distributes approved prompts for LLM-powered experiences.
- [recall](terms/recall.md) — Share of actual positives a model successfully identifies.
- [red teaming](terms/red-teaming.md) — Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.
- [regularization](terms/regularization.md) — Techniques that add penalties or constraints during training to reduce overfitting and improve generalisation.
- [reinforcement learning from human feedback](terms/reinforcement-learning-from-human-feedback.md) — Training approach that tunes a model using reward signals learned from human preference data.
- [repetition penalty](terms/repetition-penalty.md) — Decoding adjustment that down-weights tokens already generated to reduce loops and repeated phrases.
- [reranking](terms/reranking.md) — Step that refines retrieval results using a more precise but slower scoring model.
- [responsible ai](terms/responsible-ai.md) — Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.
- [retrieval](terms/retrieval.md) — Process of selecting relevant documents or vectors from a corpus in response to a query.
- [retrieval-augmented generation](terms/retrieval-augmented-generation.md) — Workflow that grounds a generative model with retrieved context before producing output.
- [risk register](terms/risk-register.md) — Central list of identified AI risks, their owners, mitigations, and review status.
- [robust prompting](terms/robust-prompting.md) — Prompt design techniques that harden models against injections, ambiguity, and unsafe outputs.
- [roc auc](terms/roc-auc.md) — Metric summarizing binary classifier performance by measuring area under the ROC curve.
- [safety classifier](terms/safety-classifier.md) — Model that detects policy-violating or risky content before or after generation.
- [safety evaluation](terms/safety-evaluation.md) — Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.
- [safety review board](terms/safety-review-board.md) — Cross-functional committee that approves high-risk AI launches and monitors mitigations.
- [safety spec](terms/safety-spec.md) — Document that codifies allowed, disallowed, and escalated behaviours for an AI system so teams can enforce safety and policy expectations.
- [self-critique loop](terms/self-critique-loop.md) — Pattern where a model reviews its own outputs, critiques them, and produces revisions before responding.
- [shadow deployment](terms/shadow-deployment.md) — Deploying an AI system alongside the existing workflow without user impact to collect telemetry.
- [synthetic data](terms/synthetic-data.md) — Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.
- [synthetic data evaluation](terms/synthetic-data-evaluation.md) — Process for measuring fidelity, utility, privacy, and bias of synthetic datasets before use.
- [system prompt](terms/system-prompt.md) — Foundational instruction that sets role, tone, and guardrails for an AI assistant before user input.
- [target variable](terms/target-variable.md) — Outcome the model is trained to predict, providing the signal for calculating loss.
- [temperature](terms/temperature.md) — Decoding parameter that controls how random or deterministic a model’s outputs are.
- [test set](terms/test-set.md) — Final evaluation split reserved for measuring real-world performance after all model tuning is finished.
- [token](terms/token.md) — Smallest unit of text a model processes after tokenization, such as a word fragment or character.
- [tool use](terms/tool-use.md) — Pattern where a model selects external tools or functions to handle parts of a task.
- [top-k sampling](terms/top-k-sampling.md) — Decoding method that samples from the k most probable next tokens to balance diversity and control.
- [top-p sampling](terms/top-p-sampling.md) — Decoding strategy that samples from the smallest set of tokens whose probabilities sum to p.
- [training data](terms/training-data.md) — Labeled examples the model learns from before it ever sees validation or test inputs.
- [transparency report](terms/transparency-report.md) — Periodic disclosure that details how an AI system operates, what data it handles, and how risks are mitigated.
- [validation set](terms/validation-set.md) — Dataset slice used to tune hyperparameters and compare experiments without touching the test set.
- [vector store](terms/vector-store.md) — Database optimized to store embeddings and execute similarity search over vectors.
- [voice cloning](terms/voice-cloning.md) — Technique that replicates a person’s voice using generative models trained on audio samples.

## Engineering & Platform
Own model integration, infra, and technical debt.

**Action plan**
- Bookmark the [Glossary Search](search.md) filtered to this role and review the top 5 unfamiliar terms.
- Schedule a sync with partner roles listed under each term to clarify ownership and open questions.
- Capture insights in your runbook or onboarding guide so future teammates ramp faster.

### Guided learning path
1. Start with LLM Core mechanics to understand knobs that affect reliability.
2. Review Operations & Monitoring entries and note which metrics to add to dashboards.
3. Pair with policy leads on governance terms that require instrumentation support.

### Practice checklist
- Use the search filters (engineering + operations) and capture metrics to wire into observability dashboards.
- Document deployment actions in your runbook using examples referenced in the glossary.

### Focus areas
- Governance & Risk (29 terms)
- LLM Core (26 terms)
- Foundations (20 terms)
- Operations & Monitoring (19 terms)
- Agents & Tooling (8 terms)
- Retrieval & RAG (7 terms)
- Optimization & Efficiency (6 terms)

### Recommended terms
- [activation function](terms/activation-function.md) — Mathematical transformation applied to a neuron’s weighted sum that lets neural networks model nonlinear relationships and control signal range.
- [agent executor](terms/agent-executor.md) — Controller layer that schedules planning, tool calls, and stop conditions so an AI agent completes tasks safely.
- [agentic ai](terms/agentic-ai.md) — Systems that plan, act, and iterate with minimal human prompts by chaining model calls and tools.
- [ai assurance](terms/ai-assurance.md) — Discipline that produces evidence, controls, and attestations showing an AI system meets agreed safety, compliance, and performance thresholds.
- [ai circuit breaker](terms/ai-circuit-breaker.md) — Automated control that halts model responses or tool access when risk thresholds are exceeded.
- [ai incident response](terms/ai-incident-response.md) — Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.
- [algorithmic audit](terms/algorithmic-audit.md) — Independent review of an AI system’s data, design, and outcomes to verify compliance, fairness, and risk controls.
- [algorithmic impact assessment](terms/algorithmic-impact-assessment.md) — Structured review that documents how an AI system may affect people, processes, and compliance obligations.
- [assurance case](terms/assurance-case.md) — Structured argument that proves an AI system meets safety and compliance expectations.
- [attention](terms/attention.md) — Technique enabling models to weight input tokens differently when producing each output.
- [beam search](terms/beam-search.md) — Deterministic decoding that keeps the top scoring sequences across multiple beams before selecting the final output.
- [bias-variance tradeoff](terms/bias-variance-tradeoff.md) — Balance between underfitting and overfitting: low bias increases variance, while high bias lowers variance but misses patterns.
- [chain-of-thought prompting](terms/chain-of-thought-prompting.md) — Prompting technique that asks models to reason through intermediate steps before giving a final answer.
- [chunking](terms/chunking.md) — Splitting source documents into manageable pieces before indexing or feeding them to models.
- [clip](terms/clip.md) — Multimodal model that embeds images and text into a shared space using contrastive learning.
- [confusion matrix](terms/confusion-matrix.md) — Table that summarizes true/false positives and negatives to diagnose classification performance.
- [consent management](terms/consent-management.md) — Practices that capture, honor, and audit user permissions across AI features.
- [constitutional ai](terms/constitutional-ai.md) — Alignment approach where models critique and revise their own outputs against a written set of principles.
- [content moderation](terms/content-moderation.md) — Workflows and tools that review, filter, and act on user-generated content to enforce policy.
- [context window](terms/context-window.md) — Maximum number of tokens a model can consider at once during prompting or inference.
- [cross-validation](terms/cross-validation.md) — Evaluation technique that splits data into multiple folds to estimate model performance on unseen samples.
- [data lineage](terms/data-lineage.md) — Traceable record of how data moves, transforms, and is used across AI systems.
- [data redaction](terms/data-redaction.md) — Removal or masking of sensitive fields before data is stored, shared, or used for model training.
- [decoding](terms/decoding.md) — Algorithms that turn model probability distributions into output tokens during generation.
- [differential privacy](terms/differential-privacy.md) — Mathematical framework that limits how much any single record influences published data or model outputs.
- [diffusion model](terms/diffusion-model.md) — Generative model that iteratively denoises random noise to synthesize images, audio, or other data.
- [direct preference optimization](terms/direct-preference-optimization.md) — Alignment technique that fine-tunes models directly on preference data without training a separate reward model.
- [embedding](terms/embedding.md) — Dense numerical representation that captures semantic meaning of text, images, or other data.
- [escalation policy](terms/escalation-policy.md) — Playbook that defines when and how AI systems route control to human reviewers.
- [evaluation](terms/evaluation.md) — Systematic measurement of model performance, safety, and reliability using defined tests.
- [evaluation harness](terms/evaluation-harness.md) — Automated pipeline that replays tasks, scores outputs, and reports regressions for AI systems.
- [f1 score](terms/f1-score.md) — Harmonic mean of precision and recall, balancing false positives and false negatives.
- [fairness metrics](terms/fairness-metrics.md) — Quantitative measures that evaluate whether model performance is equitable across groups.
- [feature engineering](terms/feature-engineering.md) — Transforming raw data into model-ready features that improve signal, fairness, and maintainability.
- [fine-tuning](terms/fine-tuning.md) — Additional training that adapts a pretrained model to a specific task or domain.
- [function calling](terms/function-calling.md) — LLM capability that lets prompts invoke predefined functions and return structured arguments.
- [generalization](terms/generalization.md) — Model's ability to sustain performance on unseen data rather than memorising the training set.
- [gradient descent](terms/gradient-descent.md) — Iterative optimization algorithm that updates model parameters in the direction of the negative gradient to minimize a loss function.
- [greedy decoding](terms/greedy-decoding.md) — Strategy that selects the highest-probability token at each step, producing deterministic outputs.
- [guardrail policy](terms/guardrail-policy.md) — Documented rules and prompts that define allowed, blocked, and escalated behaviors for AI systems.
- [hallucination](terms/hallucination.md) — When an AI model presents fabricated or unsupported information as fact.
- [human handoff](terms/human-handoff.md) — Moment when an AI workflow transfers control to a human for review or action.
- [impact mitigation plan](terms/impact-mitigation-plan.md) — Action plan that tracks risks, mitigations, owners, and timelines for an AI deployment.
- [incident taxonomy](terms/incident-taxonomy.md) — Standardized categories used to tag, analyze, and report AI incidents consistently.
- [instruction tuning](terms/instruction-tuning.md) — Supervised training that teaches models to follow natural-language instructions using curated examples.
- [jailbreak prompt](terms/jailbreak-prompt.md) — Crafted input that persuades a model to ignore safety instructions and produce disallowed responses.
- [knowledge distillation](terms/knowledge-distillation.md) — Technique that trains a smaller student model to mimic a larger teacher model’s behavior.
- [kv cache](terms/kv-cache.md) — Stored attention keys and values reused across decoding steps to speed sequential generation.
- [log probability](terms/log-probability.md) — Logarithm of a token’s probability, used to inspect model confidence and guide decoding tweaks.
- [loss function](terms/loss-function.md) — Mathematical rule that scores how far model predictions deviate from desired targets.
- [low-rank adaptation](terms/low-rank-adaptation.md) — Parameter-efficient fine-tuning that injects low-rank update matrices into transformer weights.
- [memory strategy](terms/memory-strategy.md) — Deliberate approach for when an AI agent stores, retrieves, or forgets context across tasks.
- [mixture of experts](terms/mixture-of-experts.md) — Neural architecture that routes tokens to specialized submodels to scale capacity efficiently.
- [ml observability](terms/ml-observability.md) — Practices and tooling that surface model health through metrics, traces, and alerts across the lifecycle.
- [ml ops](terms/ml-ops.md) — Operational discipline that manages ML models from experimentation through deployment and monitoring.
- [model card](terms/model-card.md) — Standardized documentation describing a model’s purpose, data, performance, and limitations.
- [model drift](terms/model-drift.md) — Gradual mismatch between model assumptions and real-world data that degrades performance over time.
- [model interpretability](terms/model-interpretability.md) — Ability to explain how a model arrives at its predictions in ways stakeholders understand.
- [overfitting](terms/overfitting.md) — When a model memorizes training data patterns so closely that it performs poorly on new samples.
- [planner executor pattern](terms/planner-executor-pattern.md) — Agent design where one component plans multi-step work and another executes steps while feeding results back for re-planning.
- [precision](terms/precision.md) — Share of predicted positives that are actually correct for a given classifier.
- [preference dataset](terms/preference-dataset.md) — Labeled comparisons of model outputs that capture which responses humans prefer.
- [privacy budget](terms/privacy-budget.md) — Quantitative limit on how much privacy loss is allowed when applying differential privacy.
- [prompt engineering](terms/prompt-engineering.md) — Crafting and testing prompts to steer model behavior toward desired outcomes.
- [prompt injection](terms/prompt-injection.md) — Attack that inserts malicious instructions into model inputs to override original prompts or policies.
- [prompt registry](terms/prompt-registry.md) — Central system that versions, reviews, and distributes approved prompts for LLM-powered experiences.
- [quantization](terms/quantization.md) — Technique that compresses model weights into lower-precision formats to shrink size and speed inference.
- [recall](terms/recall.md) — Share of actual positives a model successfully identifies.
- [red teaming](terms/red-teaming.md) — Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.
- [regularization](terms/regularization.md) — Techniques that add penalties or constraints during training to reduce overfitting and improve generalisation.
- [reinforcement learning from human feedback](terms/reinforcement-learning-from-human-feedback.md) — Training approach that tunes a model using reward signals learned from human preference data.
- [repetition penalty](terms/repetition-penalty.md) — Decoding adjustment that down-weights tokens already generated to reduce loops and repeated phrases.
- [reranking](terms/reranking.md) — Step that refines retrieval results using a more precise but slower scoring model.
- [retrieval](terms/retrieval.md) — Process of selecting relevant documents or vectors from a corpus in response to a query.
- [retrieval-augmented generation](terms/retrieval-augmented-generation.md) — Workflow that grounds a generative model with retrieved context before producing output.
- [reward model](terms/reward-model.md) — Model trained on human preferences that scores AI responses for alignment or quality.
- [risk register](terms/risk-register.md) — Central list of identified AI risks, their owners, mitigations, and review status.
- [robust prompting](terms/robust-prompting.md) — Prompt design techniques that harden models against injections, ambiguity, and unsafe outputs.
- [roc auc](terms/roc-auc.md) — Metric summarizing binary classifier performance by measuring area under the ROC curve.
- [safety classifier](terms/safety-classifier.md) — Model that detects policy-violating or risky content before or after generation.
- [safety evaluation](terms/safety-evaluation.md) — Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.
- [safety spec](terms/safety-spec.md) — Document that codifies allowed, disallowed, and escalated behaviours for an AI system so teams can enforce safety and policy expectations.
- [self-consistency decoding](terms/self-consistency-decoding.md) — Decoding strategy that samples multiple reasoning paths and aggregates the most consistent answer.
- [self-critique loop](terms/self-critique-loop.md) — Pattern where a model reviews its own outputs, critiques them, and produces revisions before responding.
- [shadow deployment](terms/shadow-deployment.md) — Deploying an AI system alongside the existing workflow without user impact to collect telemetry.
- [synthetic data](terms/synthetic-data.md) — Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.
- [synthetic data evaluation](terms/synthetic-data-evaluation.md) — Process for measuring fidelity, utility, privacy, and bias of synthetic datasets before use.
- [system prompt](terms/system-prompt.md) — Foundational instruction that sets role, tone, and guardrails for an AI assistant before user input.
- [target variable](terms/target-variable.md) — Outcome the model is trained to predict, providing the signal for calculating loss.
- [temperature](terms/temperature.md) — Decoding parameter that controls how random or deterministic a model’s outputs are.
- [test set](terms/test-set.md) — Final evaluation split reserved for measuring real-world performance after all model tuning is finished.
- [token](terms/token.md) — Smallest unit of text a model processes after tokenization, such as a word fragment or character.
- [tool use](terms/tool-use.md) — Pattern where a model selects external tools or functions to handle parts of a task.
- [top-k sampling](terms/top-k-sampling.md) — Decoding method that samples from the k most probable next tokens to balance diversity and control.
- [top-p sampling](terms/top-p-sampling.md) — Decoding strategy that samples from the smallest set of tokens whose probabilities sum to p.
- [training data](terms/training-data.md) — Labeled examples the model learns from before it ever sees validation or test inputs.
- [validation set](terms/validation-set.md) — Dataset slice used to tune hyperparameters and compare experiments without touching the test set.
- [vector store](terms/vector-store.md) — Database optimized to store embeddings and execute similarity search over vectors.

## Data Science & Research
Drive experimentation, measurement, and model improvement.

**Action plan**
- Bookmark the [Glossary Search](search.md) filtered to this role and review the top 5 unfamiliar terms.
- Schedule a sync with partner roles listed under each term to clarify ownership and open questions.
- Capture insights in your runbook or onboarding guide so future teammates ramp faster.

### Guided learning path
1. Refresh foundational metrics (precision, recall, ROC AUC) to ensure evaluation coverage.
2. Study Optimization & Efficiency techniques to plan future experiments.
3. Document how governance-aligned metrics will be reported to stakeholders.

### Practice checklist
- Select one evaluation metric and one mitigation technique from the glossary for your next experiment brief.
- Record baseline measurements tied to the definitions before shipping changes.

### Focus areas
- LLM Core (23 terms)
- Foundations (20 terms)
- Retrieval & RAG (7 terms)
- Optimization & Efficiency (6 terms)
- Governance & Risk (5 terms)
- Operations & Monitoring (4 terms)
- Agents & Tooling (1 term)

### Recommended terms
- [activation function](terms/activation-function.md) — Mathematical transformation applied to a neuron’s weighted sum that lets neural networks model nonlinear relationships and control signal range.
- [attention](terms/attention.md) — Technique enabling models to weight input tokens differently when producing each output.
- [beam search](terms/beam-search.md) — Deterministic decoding that keeps the top scoring sequences across multiple beams before selecting the final output.
- [bias-variance tradeoff](terms/bias-variance-tradeoff.md) — Balance between underfitting and overfitting: low bias increases variance, while high bias lowers variance but misses patterns.
- [chain-of-thought prompting](terms/chain-of-thought-prompting.md) — Prompting technique that asks models to reason through intermediate steps before giving a final answer.
- [chunking](terms/chunking.md) — Splitting source documents into manageable pieces before indexing or feeding them to models.
- [clip](terms/clip.md) — Multimodal model that embeds images and text into a shared space using contrastive learning.
- [confusion matrix](terms/confusion-matrix.md) — Table that summarizes true/false positives and negatives to diagnose classification performance.
- [context window](terms/context-window.md) — Maximum number of tokens a model can consider at once during prompting or inference.
- [cross-validation](terms/cross-validation.md) — Evaluation technique that splits data into multiple folds to estimate model performance on unseen samples.
- [data lineage](terms/data-lineage.md) — Traceable record of how data moves, transforms, and is used across AI systems.
- [dataset card](terms/dataset-card.md) — Structured documentation describing a dataset’s purpose, composition, risks, and usage constraints.
- [decoding](terms/decoding.md) — Algorithms that turn model probability distributions into output tokens during generation.
- [diffusion model](terms/diffusion-model.md) — Generative model that iteratively denoises random noise to synthesize images, audio, or other data.
- [direct preference optimization](terms/direct-preference-optimization.md) — Alignment technique that fine-tunes models directly on preference data without training a separate reward model.
- [embedding](terms/embedding.md) — Dense numerical representation that captures semantic meaning of text, images, or other data.
- [evaluation harness](terms/evaluation-harness.md) — Automated pipeline that replays tasks, scores outputs, and reports regressions for AI systems.
- [f1 score](terms/f1-score.md) — Harmonic mean of precision and recall, balancing false positives and false negatives.
- [feature engineering](terms/feature-engineering.md) — Transforming raw data into model-ready features that improve signal, fairness, and maintainability.
- [fine-tuning](terms/fine-tuning.md) — Additional training that adapts a pretrained model to a specific task or domain.
- [function calling](terms/function-calling.md) — LLM capability that lets prompts invoke predefined functions and return structured arguments.
- [generalization](terms/generalization.md) — Model's ability to sustain performance on unseen data rather than memorising the training set.
- [gradient descent](terms/gradient-descent.md) — Iterative optimization algorithm that updates model parameters in the direction of the negative gradient to minimize a loss function.
- [greedy decoding](terms/greedy-decoding.md) — Strategy that selects the highest-probability token at each step, producing deterministic outputs.
- [hallucination](terms/hallucination.md) — When an AI model presents fabricated or unsupported information as fact.
- [instruction tuning](terms/instruction-tuning.md) — Supervised training that teaches models to follow natural-language instructions using curated examples.
- [knowledge distillation](terms/knowledge-distillation.md) — Technique that trains a smaller student model to mimic a larger teacher model’s behavior.
- [kv cache](terms/kv-cache.md) — Stored attention keys and values reused across decoding steps to speed sequential generation.
- [log probability](terms/log-probability.md) — Logarithm of a token’s probability, used to inspect model confidence and guide decoding tweaks.
- [loss function](terms/loss-function.md) — Mathematical rule that scores how far model predictions deviate from desired targets.
- [low-rank adaptation](terms/low-rank-adaptation.md) — Parameter-efficient fine-tuning that injects low-rank update matrices into transformer weights.
- [mixture of experts](terms/mixture-of-experts.md) — Neural architecture that routes tokens to specialized submodels to scale capacity efficiently.
- [overfitting](terms/overfitting.md) — When a model memorizes training data patterns so closely that it performs poorly on new samples.
- [precision](terms/precision.md) — Share of predicted positives that are actually correct for a given classifier.
- [preference dataset](terms/preference-dataset.md) — Labeled comparisons of model outputs that capture which responses humans prefer.
- [privacy budget](terms/privacy-budget.md) — Quantitative limit on how much privacy loss is allowed when applying differential privacy.
- [prompt engineering](terms/prompt-engineering.md) — Crafting and testing prompts to steer model behavior toward desired outcomes.
- [quantization](terms/quantization.md) — Technique that compresses model weights into lower-precision formats to shrink size and speed inference.
- [recall](terms/recall.md) — Share of actual positives a model successfully identifies.
- [regularization](terms/regularization.md) — Techniques that add penalties or constraints during training to reduce overfitting and improve generalisation.
- [reinforcement learning from human feedback](terms/reinforcement-learning-from-human-feedback.md) — Training approach that tunes a model using reward signals learned from human preference data.
- [repetition penalty](terms/repetition-penalty.md) — Decoding adjustment that down-weights tokens already generated to reduce loops and repeated phrases.
- [reranking](terms/reranking.md) — Step that refines retrieval results using a more precise but slower scoring model.
- [retrieval](terms/retrieval.md) — Process of selecting relevant documents or vectors from a corpus in response to a query.
- [retrieval-augmented generation](terms/retrieval-augmented-generation.md) — Workflow that grounds a generative model with retrieved context before producing output.
- [reward model](terms/reward-model.md) — Model trained on human preferences that scores AI responses for alignment or quality.
- [roc auc](terms/roc-auc.md) — Metric summarizing binary classifier performance by measuring area under the ROC curve.
- [self-consistency decoding](terms/self-consistency-decoding.md) — Decoding strategy that samples multiple reasoning paths and aggregates the most consistent answer.
- [synthetic data](terms/synthetic-data.md) — Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.
- [synthetic data evaluation](terms/synthetic-data-evaluation.md) — Process for measuring fidelity, utility, privacy, and bias of synthetic datasets before use.
- [system prompt](terms/system-prompt.md) — Foundational instruction that sets role, tone, and guardrails for an AI assistant before user input.
- [target variable](terms/target-variable.md) — Outcome the model is trained to predict, providing the signal for calculating loss.
- [temperature](terms/temperature.md) — Decoding parameter that controls how random or deterministic a model’s outputs are.
- [test set](terms/test-set.md) — Final evaluation split reserved for measuring real-world performance after all model tuning is finished.
- [token](terms/token.md) — Smallest unit of text a model processes after tokenization, such as a word fragment or character.
- [top-k sampling](terms/top-k-sampling.md) — Decoding method that samples from the k most probable next tokens to balance diversity and control.
- [top-p sampling](terms/top-p-sampling.md) — Decoding strategy that samples from the smallest set of tokens whose probabilities sum to p.
- [training data](terms/training-data.md) — Labeled examples the model learns from before it ever sees validation or test inputs.
- [validation set](terms/validation-set.md) — Dataset slice used to tune hyperparameters and compare experiments without touching the test set.
- [vector store](terms/vector-store.md) — Database optimized to store embeddings and execute similarity search over vectors.

## Policy & Risk
Ensure responsible AI controls align with governance frameworks.

**Action plan**
- Bookmark the [Glossary Search](search.md) filtered to this role and review the top 5 unfamiliar terms.
- Schedule a sync with partner roles listed under each term to clarify ownership and open questions.
- Capture insights in your runbook or onboarding guide so future teammates ramp faster.

### Guided learning path
1. Read algorithmic governance terms to map glossary content to internal controls.
2. Identify three technical concepts to discuss with engineering for upcoming reviews.
3. Draft guidance for disclosure or transparency using relevant glossary examples.

### Practice checklist
- Draft a review checklist referencing the top three governance terms surfaced in the backlog.
- Map required disclosures for the next launch memo using linked glossary examples.

### Focus areas
- Governance & Risk (40 terms)
- Operations & Monitoring (18 terms)
- Foundations (7 terms)
- LLM Core (6 terms)
- Agents & Tooling (2 terms)
- Optimization & Efficiency (1 term)
- Retrieval & RAG (1 term)

### Recommended terms
- [ai assurance](terms/ai-assurance.md) — Discipline that produces evidence, controls, and attestations showing an AI system meets agreed safety, compliance, and performance thresholds.
- [ai circuit breaker](terms/ai-circuit-breaker.md) — Automated control that halts model responses or tool access when risk thresholds are exceeded.
- [ai incident response](terms/ai-incident-response.md) — Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.
- [algorithmic audit](terms/algorithmic-audit.md) — Independent review of an AI system’s data, design, and outcomes to verify compliance, fairness, and risk controls.
- [algorithmic bias](terms/algorithmic-bias.md) — Systematic unfairness in model outputs that disadvantages certain groups or outcomes.
- [algorithmic impact assessment](terms/algorithmic-impact-assessment.md) — Structured review that documents how an AI system may affect people, processes, and compliance obligations.
- [alignment](terms/alignment.md) — Making sure AI systems optimize for human values, policies, and intended outcomes.
- [assurance case](terms/assurance-case.md) — Structured argument that proves an AI system meets safety and compliance expectations.
- [bias-variance tradeoff](terms/bias-variance-tradeoff.md) — Balance between underfitting and overfitting: low bias increases variance, while high bias lowers variance but misses patterns.
- [confusion matrix](terms/confusion-matrix.md) — Table that summarizes true/false positives and negatives to diagnose classification performance.
- [consent management](terms/consent-management.md) — Practices that capture, honor, and audit user permissions across AI features.
- [constitutional ai](terms/constitutional-ai.md) — Alignment approach where models critique and revise their own outputs against a written set of principles.
- [content moderation](terms/content-moderation.md) — Workflows and tools that review, filter, and act on user-generated content to enforce policy.
- [data lineage](terms/data-lineage.md) — Traceable record of how data moves, transforms, and is used across AI systems.
- [data minimization](terms/data-minimization.md) — Principle of collecting and retaining only the data necessary for a defined purpose.
- [data redaction](terms/data-redaction.md) — Removal or masking of sensitive fields before data is stored, shared, or used for model training.
- [data retention](terms/data-retention.md) — Policies defining how long data is stored, where it lives, and how it is deleted.
- [dataset card](terms/dataset-card.md) — Structured documentation describing a dataset’s purpose, composition, risks, and usage constraints.
- [differential privacy](terms/differential-privacy.md) — Mathematical framework that limits how much any single record influences published data or model outputs.
- [escalation policy](terms/escalation-policy.md) — Playbook that defines when and how AI systems route control to human reviewers.
- [evaluation](terms/evaluation.md) — Systematic measurement of model performance, safety, and reliability using defined tests.
- [fairness metrics](terms/fairness-metrics.md) — Quantitative measures that evaluate whether model performance is equitable across groups.
- [generative ai](terms/generative-ai.md) — Family of models that produce new content—text, images, code—rather than only making predictions.
- [guardrail policy](terms/guardrail-policy.md) — Documented rules and prompts that define allowed, blocked, and escalated behaviors for AI systems.
- [guardrails](terms/guardrails.md) — Controls that constrain model behavior to comply with safety, legal, or brand requirements.
- [hallucination](terms/hallucination.md) — When an AI model presents fabricated or unsupported information as fact.
- [human handoff](terms/human-handoff.md) — Moment when an AI workflow transfers control to a human for review or action.
- [impact mitigation plan](terms/impact-mitigation-plan.md) — Action plan that tracks risks, mitigations, owners, and timelines for an AI deployment.
- [incident taxonomy](terms/incident-taxonomy.md) — Standardized categories used to tag, analyze, and report AI incidents consistently.
- [instruction tuning](terms/instruction-tuning.md) — Supervised training that teaches models to follow natural-language instructions using curated examples.
- [loss function](terms/loss-function.md) — Mathematical rule that scores how far model predictions deviate from desired targets.
- [ml observability](terms/ml-observability.md) — Practices and tooling that surface model health through metrics, traces, and alerts across the lifecycle.
- [ml ops](terms/ml-ops.md) — Operational discipline that manages ML models from experimentation through deployment and monitoring.
- [model card](terms/model-card.md) — Standardized documentation describing a model’s purpose, data, performance, and limitations.
- [model drift](terms/model-drift.md) — Gradual mismatch between model assumptions and real-world data that degrades performance over time.
- [model governance](terms/model-governance.md) — Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.
- [model interpretability](terms/model-interpretability.md) — Ability to explain how a model arrives at its predictions in ways stakeholders understand.
- [precision](terms/precision.md) — Share of predicted positives that are actually correct for a given classifier.
- [preference dataset](terms/preference-dataset.md) — Labeled comparisons of model outputs that capture which responses humans prefer.
- [privacy](terms/privacy.md) — Principle of limiting data collection, use, and exposure to protect individuals’ information.
- [privacy budget](terms/privacy-budget.md) — Quantitative limit on how much privacy loss is allowed when applying differential privacy.
- [privacy impact assessment](terms/privacy-impact-assessment.md) — Structured review that evaluates how a system collects, uses, and safeguards personal data.
- [prompt registry](terms/prompt-registry.md) — Central system that versions, reviews, and distributes approved prompts for LLM-powered experiences.
- [recall](terms/recall.md) — Share of actual positives a model successfully identifies.
- [red teaming](terms/red-teaming.md) — Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.
- [reinforcement learning from human feedback](terms/reinforcement-learning-from-human-feedback.md) — Training approach that tunes a model using reward signals learned from human preference data.
- [responsible ai](terms/responsible-ai.md) — Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.
- [retrieval](terms/retrieval.md) — Process of selecting relevant documents or vectors from a corpus in response to a query.
- [reward model](terms/reward-model.md) — Model trained on human preferences that scores AI responses for alignment or quality.
- [risk register](terms/risk-register.md) — Central list of identified AI risks, their owners, mitigations, and review status.
- [safety classifier](terms/safety-classifier.md) — Model that detects policy-violating or risky content before or after generation.
- [safety evaluation](terms/safety-evaluation.md) — Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.
- [safety review board](terms/safety-review-board.md) — Cross-functional committee that approves high-risk AI launches and monitors mitigations.
- [safety spec](terms/safety-spec.md) — Document that codifies allowed, disallowed, and escalated behaviours for an AI system so teams can enforce safety and policy expectations.
- [self-critique loop](terms/self-critique-loop.md) — Pattern where a model reviews its own outputs, critiques them, and produces revisions before responding.
- [shadow deployment](terms/shadow-deployment.md) — Deploying an AI system alongside the existing workflow without user impact to collect telemetry.
- [synthetic data](terms/synthetic-data.md) — Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.
- [synthetic data evaluation](terms/synthetic-data-evaluation.md) — Process for measuring fidelity, utility, privacy, and bias of synthetic datasets before use.
- [transparency report](terms/transparency-report.md) — Periodic disclosure that details how an AI system operates, what data it handles, and how risks are mitigated.
- [voice cloning](terms/voice-cloning.md) — Technique that replicates a person’s voice using generative models trained on audio samples.

## Legal & Compliance
Evaluate regulatory exposure, contracts, and IP concerns.

**Action plan**
- Bookmark the [Glossary Search](search.md) filtered to this role and review the top 5 unfamiliar terms.
- Schedule a sync with partner roles listed under each term to clarify ownership and open questions.
- Capture insights in your runbook or onboarding guide so future teammates ramp faster.

### Guided learning path
1. Focus on Responsible AI and compliance-related terms to spot regulatory hooks.
2. Cross-reference privacy-focused entries with current policy language.
3. Capture open questions for the next risk or contract review cycle.

### Practice checklist
- Compare contractual language with glossary definitions for privacy and retention to spot gaps.
- Flag terms needing legal guidance through the intake form so questions are tracked.

### Focus areas
- Governance & Risk (31 terms)
- Operations & Monitoring (9 terms)
- Foundations (1 term)
- LLM Core (1 term)

### Recommended terms
- [ai assurance](terms/ai-assurance.md) — Discipline that produces evidence, controls, and attestations showing an AI system meets agreed safety, compliance, and performance thresholds.
- [ai incident response](terms/ai-incident-response.md) — Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.
- [algorithmic audit](terms/algorithmic-audit.md) — Independent review of an AI system’s data, design, and outcomes to verify compliance, fairness, and risk controls.
- [algorithmic bias](terms/algorithmic-bias.md) — Systematic unfairness in model outputs that disadvantages certain groups or outcomes.
- [algorithmic impact assessment](terms/algorithmic-impact-assessment.md) — Structured review that documents how an AI system may affect people, processes, and compliance obligations.
- [alignment](terms/alignment.md) — Making sure AI systems optimize for human values, policies, and intended outcomes.
- [assurance case](terms/assurance-case.md) — Structured argument that proves an AI system meets safety and compliance expectations.
- [consent management](terms/consent-management.md) — Practices that capture, honor, and audit user permissions across AI features.
- [data lineage](terms/data-lineage.md) — Traceable record of how data moves, transforms, and is used across AI systems.
- [data minimization](terms/data-minimization.md) — Principle of collecting and retaining only the data necessary for a defined purpose.
- [data redaction](terms/data-redaction.md) — Removal or masking of sensitive fields before data is stored, shared, or used for model training.
- [data retention](terms/data-retention.md) — Policies defining how long data is stored, where it lives, and how it is deleted.
- [dataset card](terms/dataset-card.md) — Structured documentation describing a dataset’s purpose, composition, risks, and usage constraints.
- [differential privacy](terms/differential-privacy.md) — Mathematical framework that limits how much any single record influences published data or model outputs.
- [evaluation](terms/evaluation.md) — Systematic measurement of model performance, safety, and reliability using defined tests.
- [fairness metrics](terms/fairness-metrics.md) — Quantitative measures that evaluate whether model performance is equitable across groups.
- [guardrails](terms/guardrails.md) — Controls that constrain model behavior to comply with safety, legal, or brand requirements.
- [hallucination](terms/hallucination.md) — When an AI model presents fabricated or unsupported information as fact.
- [impact mitigation plan](terms/impact-mitigation-plan.md) — Action plan that tracks risks, mitigations, owners, and timelines for an AI deployment.
- [model card](terms/model-card.md) — Standardized documentation describing a model’s purpose, data, performance, and limitations.
- [model drift](terms/model-drift.md) — Gradual mismatch between model assumptions and real-world data that degrades performance over time.
- [model governance](terms/model-governance.md) — Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.
- [model interpretability](terms/model-interpretability.md) — Ability to explain how a model arrives at its predictions in ways stakeholders understand.
- [privacy](terms/privacy.md) — Principle of limiting data collection, use, and exposure to protect individuals’ information.
- [privacy budget](terms/privacy-budget.md) — Quantitative limit on how much privacy loss is allowed when applying differential privacy.
- [privacy impact assessment](terms/privacy-impact-assessment.md) — Structured review that evaluates how a system collects, uses, and safeguards personal data.
- [prompt registry](terms/prompt-registry.md) — Central system that versions, reviews, and distributes approved prompts for LLM-powered experiences.
- [red teaming](terms/red-teaming.md) — Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.
- [responsible ai](terms/responsible-ai.md) — Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.
- [risk register](terms/risk-register.md) — Central list of identified AI risks, their owners, mitigations, and review status.
- [safety review board](terms/safety-review-board.md) — Cross-functional committee that approves high-risk AI launches and monitors mitigations.
- [transparency report](terms/transparency-report.md) — Periodic disclosure that details how an AI system operates, what data it handles, and how risks are mitigated.
- [voice cloning](terms/voice-cloning.md) — Technique that replicates a person’s voice using generative models trained on audio samples.

## Security & Trust
Safeguard data, access, and abuse prevention.

**Action plan**
- Bookmark the [Glossary Search](search.md) filtered to this role and review the top 5 unfamiliar terms.
- Schedule a sync with partner roles listed under each term to clarify ownership and open questions.
- Capture insights in your runbook or onboarding guide so future teammates ramp faster.

### Guided learning path
1. Study Operations & Monitoring entries for logging and detection requirements.
2. Review tool and agent terminology to assess abuse surface areas.
3. Coordinate with product/legal on incident response and disclosure expectations.

### Practice checklist
- Audit incident response and tool-use entries to confirm abuse-prevention controls are documented.
- Plan a tabletop exercise using the glossary's scenario examples and log outcomes.

### Focus areas
- Governance & Risk (21 terms)
- Operations & Monitoring (11 terms)
- LLM Core (2 terms)
- Agents & Tooling (1 term)
- Foundations (1 term)
- Retrieval & RAG (1 term)

### Recommended terms
- [ai assurance](terms/ai-assurance.md) — Discipline that produces evidence, controls, and attestations showing an AI system meets agreed safety, compliance, and performance thresholds.
- [ai circuit breaker](terms/ai-circuit-breaker.md) — Automated control that halts model responses or tool access when risk thresholds are exceeded.
- [ai incident response](terms/ai-incident-response.md) — Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.
- [algorithmic bias](terms/algorithmic-bias.md) — Systematic unfairness in model outputs that disadvantages certain groups or outcomes.
- [content moderation](terms/content-moderation.md) — Workflows and tools that review, filter, and act on user-generated content to enforce policy.
- [data minimization](terms/data-minimization.md) — Principle of collecting and retaining only the data necessary for a defined purpose.
- [data redaction](terms/data-redaction.md) — Removal or masking of sensitive fields before data is stored, shared, or used for model training.
- [data retention](terms/data-retention.md) — Policies defining how long data is stored, where it lives, and how it is deleted.
- [differential privacy](terms/differential-privacy.md) — Mathematical framework that limits how much any single record influences published data or model outputs.
- [escalation policy](terms/escalation-policy.md) — Playbook that defines when and how AI systems route control to human reviewers.
- [evaluation](terms/evaluation.md) — Systematic measurement of model performance, safety, and reliability using defined tests.
- [guardrail policy](terms/guardrail-policy.md) — Documented rules and prompts that define allowed, blocked, and escalated behaviors for AI systems.
- [incident taxonomy](terms/incident-taxonomy.md) — Standardized categories used to tag, analyze, and report AI incidents consistently.
- [jailbreak prompt](terms/jailbreak-prompt.md) — Crafted input that persuades a model to ignore safety instructions and produce disallowed responses.
- [ml observability](terms/ml-observability.md) — Practices and tooling that surface model health through metrics, traces, and alerts across the lifecycle.
- [ml ops](terms/ml-ops.md) — Operational discipline that manages ML models from experimentation through deployment and monitoring.
- [model drift](terms/model-drift.md) — Gradual mismatch between model assumptions and real-world data that degrades performance over time.
- [planner executor pattern](terms/planner-executor-pattern.md) — Agent design where one component plans multi-step work and another executes steps while feeding results back for re-planning.
- [privacy impact assessment](terms/privacy-impact-assessment.md) — Structured review that evaluates how a system collects, uses, and safeguards personal data.
- [prompt injection](terms/prompt-injection.md) — Attack that inserts malicious instructions into model inputs to override original prompts or policies.
- [red teaming](terms/red-teaming.md) — Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.
- [retrieval](terms/retrieval.md) — Process of selecting relevant documents or vectors from a corpus in response to a query.
- [robust prompting](terms/robust-prompting.md) — Prompt design techniques that harden models against injections, ambiguity, and unsafe outputs.
- [safety classifier](terms/safety-classifier.md) — Model that detects policy-violating or risky content before or after generation.
- [safety review board](terms/safety-review-board.md) — Cross-functional committee that approves high-risk AI launches and monitors mitigations.
- [safety spec](terms/safety-spec.md) — Document that codifies allowed, disallowed, and escalated behaviours for an AI system so teams can enforce safety and policy expectations.
- [shadow deployment](terms/shadow-deployment.md) — Deploying an AI system alongside the existing workflow without user impact to collect telemetry.
- [synthetic data](terms/synthetic-data.md) — Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.
- [voice cloning](terms/voice-cloning.md) — Technique that replicates a person’s voice using generative models trained on audio samples.

## Communications & Enablement
Craft messaging, disclosure, and stakeholder education.

**Action plan**
- Bookmark the [Glossary Search](search.md) filtered to this role and review the top 5 unfamiliar terms.
- Schedule a sync with partner roles listed under each term to clarify ownership and open questions.
- Capture insights in your runbook or onboarding guide so future teammates ramp faster.

### Guided learning path
1. Scan definitions tagged for Governance & Risk to prep stakeholder messaging.
2. Collect relatable examples from the glossary to use in enablement materials.
3. Draft a narrative that links technical terms to user-facing value and risk mitigations.

### Practice checklist
- Draft an FAQ using glossary language to keep messaging consistent across teams.
- Tag enablement tickets with relevant glossary links so stakeholders can self-serve context.

### Focus areas
- Governance & Risk (19 terms)
- Operations & Monitoring (7 terms)
- Foundations (4 terms)
- LLM Core (3 terms)
- Retrieval & RAG (2 terms)
- Agents & Tooling (1 term)

### Recommended terms
- [ai incident response](terms/ai-incident-response.md) — Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.
- [algorithmic audit](terms/algorithmic-audit.md) — Independent review of an AI system’s data, design, and outcomes to verify compliance, fairness, and risk controls.
- [algorithmic bias](terms/algorithmic-bias.md) — Systematic unfairness in model outputs that disadvantages certain groups or outcomes.
- [algorithmic impact assessment](terms/algorithmic-impact-assessment.md) — Structured review that documents how an AI system may affect people, processes, and compliance obligations.
- [alignment](terms/alignment.md) — Making sure AI systems optimize for human values, policies, and intended outcomes.
- [clip](terms/clip.md) — Multimodal model that embeds images and text into a shared space using contrastive learning.
- [content moderation](terms/content-moderation.md) — Workflows and tools that review, filter, and act on user-generated content to enforce policy.
- [diffusion model](terms/diffusion-model.md) — Generative model that iteratively denoises random noise to synthesize images, audio, or other data.
- [evaluation](terms/evaluation.md) — Systematic measurement of model performance, safety, and reliability using defined tests.
- [generative ai](terms/generative-ai.md) — Family of models that produce new content—text, images, code—rather than only making predictions.
- [guardrails](terms/guardrails.md) — Controls that constrain model behavior to comply with safety, legal, or brand requirements.
- [hallucination](terms/hallucination.md) — When an AI model presents fabricated or unsupported information as fact.
- [human handoff](terms/human-handoff.md) — Moment when an AI workflow transfers control to a human for review or action.
- [model card](terms/model-card.md) — Standardized documentation describing a model’s purpose, data, performance, and limitations.
- [model drift](terms/model-drift.md) — Gradual mismatch between model assumptions and real-world data that degrades performance over time.
- [model governance](terms/model-governance.md) — Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.
- [privacy](terms/privacy.md) — Principle of limiting data collection, use, and exposure to protect individuals’ information.
- [red teaming](terms/red-teaming.md) — Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.
- [responsible ai](terms/responsible-ai.md) — Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.
- [retrieval](terms/retrieval.md) — Process of selecting relevant documents or vectors from a corpus in response to a query.
- [safety evaluation](terms/safety-evaluation.md) — Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.
- [safety spec](terms/safety-spec.md) — Document that codifies allowed, disallowed, and escalated behaviours for an AI system so teams can enforce safety and policy expectations.
- [transparency report](terms/transparency-report.md) — Periodic disclosure that details how an AI system operates, what data it handles, and how risks are mitigated.
- [voice cloning](terms/voice-cloning.md) — Technique that replicates a person’s voice using generative models trained on audio samples.
